<!--yaml
name: cross_entropy_of_probability_distributions
needs:
    - logarithm
    - outcome_probabilities
-->

§ Definition ⦉
¶ ⦊
  ‖ Consider two distributions ⦉

  ‖ on the same finite set. ⦉

  ‖ The ❬cross entropy❭ of the first distribution ❬relative❭ to
    the second distribution is the expectation of the negative
    logarithm of the first distribution under the second
    distribution. ⦉
⦉

§§ Notation ⦉
¶ ⦊
  ‖ Suppose $p: A → 𝗥$ and $q: A → 𝗥$ are distributions on
    the finite set $A$. ⦉

  ‖ We denote the cross entropy of $p$ relative to $q$ by
    $H(q, p)$; in symbols,
    ◇ ⦊
      ‖ H(q, p) := -∑_{a ∈ A} q(a) \log p(a) ⦉
    ⦉⦉
⦉

<tex>
  ‖ \blankpage ⦉
</tex>
