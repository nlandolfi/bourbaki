<!--yaml
name: cross_entropy_of_probability_distributions
needs:
    - logarithm
    - outcome_probabilities
-->

Â§ Definition â¦‰
Â¶ â¦Š
  â€– Consider two distributions â¦‰

  â€– on the same finite set. â¦‰

  â€– The â¬cross entropyâ­ of the first distribution â¬relativeâ­ to
    the second distribution is the expectation of the negative
    logarithm of the first distribution under the second
    distribution. â¦‰
â¦‰

Â§Â§ Notation â¦‰
Â¶ â¦Š
  â€– Suppose $p: A â†’ ğ—¥$ and $q: A â†’ ğ—¥$ are distributions on
    the finite set $A$. â¦‰

  â€– We denote the cross entropy of $p$ relative to $q$ by
    $H(q, p)$; in symbols,
    â—‡ â¦Š
      â€– H(q, p) := -âˆ‘_{a âˆˆ A} q(a) \log p(a) â¦‰
    â¦‰â¦‰
â¦‰

<tex>
  â€– \blankpage â¦‰
</tex>
