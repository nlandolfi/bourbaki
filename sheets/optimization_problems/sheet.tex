%!name:optimization_problems
%!need:optimizers
%!need:real_functions
%!need:extended_real_numbers
%!need:greatest_lower_bounds
%!need:n-dimensional_space
%%!need:family_unions_and_intersections

\ssection{Why}

We are frequently interested in finding minimizers of real functions.
  \ifhmode\unskip\fi\footnote{
Future editions will modify and expand.
  }

\ssection{Definition}

An \t{optimization problem} is a pair $(\CX, f: \CX \to \R)$ in which $\CX$ is a nonempty set called the \t{constraint set} and $f$ is called the \t{objective} (or \t{cost function}).

If $\CX$ is finite we call the optimization problem \t{discrete}.
If $\CX \subset \R^d$ we call the optimization problem \t{continuous}.

We refer to all elements of the constraint set as \t{feasible}.
We refer to an element $x \in \CX$ of the constraint set as \t{optimal} if $f(x) = \inf_{z \in \CX}f(z)$.
We also refer to optimal elements as \t{solutions} of the optimization problem.

It is common for $f$ and $\CX$ to depend on some other, known, given objects.
In this case, these objects are often called \t{parameters} or \t{problem data}.

\ssubsection{Notation}

We often write optimization problems as
  \[
\begin{aligned}
\text{minimize}\quad & f(x) \\
\text{subject to}\quad & x \in \CX.
\end{aligned}
  \]
In this case we call $x$ the \t{decision variable}.

\ssubsection{Extended reals}

It is common to let $f: \CX \to \Rbar$, and allow there to exist $x \in \CX$ for which $f(x) = \infty$.
This is a trick to embed further constraints in the objective.

\ssubsection{Maximization}

If we have some function $g: \CX \to \Rbar$ that we wish to maximize, we can always convert it to an optimization problem by defining $f: \CX \to \Rbar$ by $f(x) = -g(x)$.
In this case $g$ is often called a \t{reward} (\t{utility}, \t{profit}).

\ssection{Solvers}

Let $\CP = \set{(X_a, f_a: X_a \to \Rbar)}_{a \in A}$ be a family of optimization problems.
A \t{solver} (or \t{solution method}, \t{solution algorithm}) for $\CP$ is a function $S: A \to \CS$ such that $S_a$ is a solution of the problem $(X_a, f_a)$.

Loosely speaking, the difficulty of \say{solving} the optimization problem $(\CX, f)$ depends on the properties of $\CX$ and $f$ and the problem \say{size}.
For example, when $\CX \subset \R^d$ the difficulty is related to the \say{dimension} $d$ of $x \in \CX$.

