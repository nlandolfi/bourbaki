¶ ⦊
  ‖ ❲%!name:optimization_problems❳ ⦉

  ‖ ❲%!need:optimizers❳ ⦉

  ‖ ❲%!need:real_functions❳ ⦉

  ‖ ❲%!need:extended_real_numbers❳ ⦉

  ‖ ❲%!need:greatest_lower_bounds❳ ⦉

  ‖ ❲%!need:n-dimensional_space❳ ⦉

  ‖ ❲%%!need:family_unions_and_intersections❳ ⦉
⦉

¶ ⦊
  ‖ \ssection{Why} ⦉
⦉

¶ ⦊
  ‖ We are frequently interested in finding minimizers of real
    functions.
    † ⦊
      ‖ Future editions will modify and expand. ⦉
    ⦉⦉
⦉

¶ ⦊
  ‖ \ssection{Definition} ⦉
⦉

¶ ⦊
  ‖ An ❬optimization problem❭ is a pair $(\CX, f: \CX → 𝗥)$ in
    which $\CX$ is a nonempty set called the ❬constraint set❭
    and $f$ is called the ❬objective❭ (or ❬cost function❭). ⦉
⦉

¶ ⦊
  ‖ If $\CX$ is finite we call the optimization problem
    ❬discrete❭. ⦉

  ‖ If $\CX ⊂ 𝗥^d$ we call the optimization problem ❬continuous❭. ⦉
⦉

¶ ⦊
  ‖ We refer to all elements of the constraint set as
    ❬feasible❭. ⦉

  ‖ We refer to an element $x ∈ \CX$ of the constraint set as
    ❬optimal❭ if $f(x) = ∈f_{z ∈ \CX}f(z)$. ⦉

  ‖ We also refer to optimal elements as ❬solutions❭ of the
    optimization problem. ⦉
⦉

¶ ⦊
  ‖ It is common for $f$ and $\CX$ to depend on some other,
    known, given objects. ⦉

  ‖ In this case, these objects are often called ❬parameters❭ or
    ❬problem data❭. ⦉
⦉

¶ ⦊
  ‖ \ssubsection{Notation} ⦉
⦉

¶ ⦊
  ‖ We often write optimization problems as
    ◇ ⦊
      ‖ \begin{aligned} ⦉

      ‖ \text{minimize}\quad ❲&❳ f(x) ᜶ ⦉

      ‖ \text{subject to}\quad ❲&❳ x ∈ \CX. ⦉

      ‖ \end{aligned} ⦉
    ⦉⦉

  ‖ In this case we call $x$ the ❬decision variable❭. ⦉
⦉

¶ ⦊
  ‖ \ssubsection{Extended reals} ⦉
⦉

¶ ⦊
  ‖ It is common to let $f: \CX → \Rbar$, and allow there to
    exist $x ∈ \CX$ for which $f(x) = ∞$. ⦉

  ‖ This is a trick to embed further constraints in the
    objective. ⦉
⦉

¶ ⦊
  ‖ \ssubsection{Maximization} ⦉
⦉

¶ ⦊
  ‖ If we have some function $g: \CX → \Rbar$ that we wish to
    maximize, we can always convert it to an optimization problem
    by defining $f: \CX → \Rbar$ by $f(x) = -g(x)$. ⦉

  ‖ In this case $g$ is often called a ❬reward❭ (❬utility❭,
    ❬profit❭). ⦉
⦉

¶ ⦊
  ‖ \ssection{Solvers} ⦉
⦉

¶ ⦊
  ‖ Let $\CP = \set{(X_a, f_a: X_a → \Rbar)}_{a ∈ A}$ be a
    family of optimization problems. ⦉

  ‖ A ❬solver❭ (or ❬solution method❭, ❬solution algorithm❭) for
    $\CP$ is a function $S: A → \CS$ such that $S_a$ is a
    solution of the problem $(X_a, f_a)$. ⦉
⦉

¶ ⦊
  ‖ Loosely speaking, the difficulty of “solving} the optimization
    problem $(\CX, f)$ depends on the properties of $\CX$ and
    $f$ and the problem \say{size}. ⦉

  ‖ For example, when $\CX ⊂ 𝗥^d$ the difficulty is related to
    the \say{dimension” $d$ of $x ∈ \CX$. ⦉

  ‖ ⦉
⦉
