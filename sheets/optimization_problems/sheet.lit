Â¶ â¦Š
  â€– â²%!name:optimization_problemsâ³ â¦‰

  â€– â²%!need:optimizersâ³ â¦‰

  â€– â²%!need:real_functionsâ³ â¦‰

  â€– â²%!need:extended_real_numbersâ³ â¦‰

  â€– â²%!need:greatest_lower_boundsâ³ â¦‰

  â€– â²%!need:n-dimensional_spaceâ³ â¦‰

  â€– â²%%!need:family_unions_and_intersectionsâ³ â¦‰
â¦‰

Â¶ â¦Š
  â€– \ssection{Why} â¦‰
â¦‰

Â¶ â¦Š
  â€– We are frequently interested in finding minimizers of real
    functions.
    â€  â¦Š
      â€– Future editions will modify and expand. â¦‰
    â¦‰â¦‰
â¦‰

Â¶ â¦Š
  â€– \ssection{Definition} â¦‰
â¦‰

Â¶ â¦Š
  â€– An â¬optimization problemâ­ is a pair $(\CX, f: \CX â†’ ğ—¥)$ in
    which $\CX$ is a nonempty set called the â¬constraint setâ­
    and $f$ is called the â¬objectiveâ­ (or â¬cost functionâ­). â¦‰
â¦‰

Â¶ â¦Š
  â€– If $\CX$ is finite we call the optimization problem
    â¬discreteâ­. â¦‰

  â€– If $\CX âŠ‚ ğ—¥^d$ we call the optimization problem â¬continuousâ­. â¦‰
â¦‰

Â¶ â¦Š
  â€– We refer to all elements of the constraint set as
    â¬feasibleâ­. â¦‰

  â€– We refer to an element $x âˆˆ \CX$ of the constraint set as
    â¬optimalâ­ if $f(x) = âˆˆf_{z âˆˆ \CX}f(z)$. â¦‰

  â€– We also refer to optimal elements as â¬solutionsâ­ of the
    optimization problem. â¦‰
â¦‰

Â¶ â¦Š
  â€– It is common for $f$ and $\CX$ to depend on some other,
    known, given objects. â¦‰

  â€– In this case, these objects are often called â¬parametersâ­ or
    â¬problem dataâ­. â¦‰
â¦‰

Â¶ â¦Š
  â€– \ssubsection{Notation} â¦‰
â¦‰

Â¶ â¦Š
  â€– We often write optimization problems as
    â—‡ â¦Š
      â€– \begin{aligned} â¦‰

      â€– \text{minimize}\quad â²&â³ f(x) áœ¶ â¦‰

      â€– \text{subject to}\quad â²&â³ x âˆˆ \CX. â¦‰

      â€– \end{aligned} â¦‰
    â¦‰â¦‰

  â€– In this case we call $x$ the â¬decision variableâ­. â¦‰
â¦‰

Â¶ â¦Š
  â€– \ssubsection{Extended reals} â¦‰
â¦‰

Â¶ â¦Š
  â€– It is common to let $f: \CX â†’ \Rbar$, and allow there to
    exist $x âˆˆ \CX$ for which $f(x) = âˆ$. â¦‰

  â€– This is a trick to embed further constraints in the
    objective. â¦‰
â¦‰

Â¶ â¦Š
  â€– \ssubsection{Maximization} â¦‰
â¦‰

Â¶ â¦Š
  â€– If we have some function $g: \CX â†’ \Rbar$ that we wish to
    maximize, we can always convert it to an optimization problem
    by defining $f: \CX â†’ \Rbar$ by $f(x) = -g(x)$. â¦‰

  â€– In this case $g$ is often called a â¬rewardâ­ (â¬utilityâ­,
    â¬profitâ­). â¦‰
â¦‰

Â¶ â¦Š
  â€– \ssection{Solvers} â¦‰
â¦‰

Â¶ â¦Š
  â€– Let $\CP = \set{(X_a, f_a: X_a â†’ \Rbar)}_{a âˆˆ A}$ be a
    family of optimization problems. â¦‰

  â€– A â¬solverâ­ (or â¬solution methodâ­, â¬solution algorithmâ­) for
    $\CP$ is a function $S: A â†’ \CS$ such that $S_a$ is a
    solution of the problem $(X_a, f_a)$. â¦‰
â¦‰

Â¶ â¦Š
  â€– Loosely speaking, the difficulty of â€œsolving} the optimization
    problem $(\CX, f)$ depends on the properties of $\CX$ and
    $f$ and the problem \say{size}. â¦‰

  â€– For example, when $\CX âŠ‚ ğ—¥^d$ the difficulty is related to
    the \say{dimensionâ€ $d$ of $x âˆˆ \CX$. â¦‰

  â€– â¦‰
â¦‰
