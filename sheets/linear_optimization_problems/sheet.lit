<!--yaml
name: linear_optimization_problems
needs:
    - optimization_problems
    - real_polyhedra
    - inner_product_linear_functional_representations
refs:
    - combinkert/3
-->

Â§ Definition â¦‰
Â¶ â¦Š
  â€– An optimization problem $(ğ’³, f)$ is called â¬linearâ­ (a
    â¬linear optimization problemâ­) if $ğ’³ âŠ‚ ğ—¥^n$ is a polyhedron
    and $f: ğ—¥^n â†’ \Rbar$ is a linear function. â¦‰
â¦‰

Â§Â§ Problem data â¦‰
Â¶ â¦Š
  â€– Recall that $f$ is linear means there exists $c âˆˆ ğ—¥^n$
    such that
    â—‡ â¦Š
      â€– f(x) = c^âŠ¤x \quad \text{for all } x âˆˆ ğ—¥^n â¦‰
    â¦‰â¦‰

  â€– Also, $ğ’³$ polyhedral means there exists $A âˆˆ ğ—¥^{m Ã— n}$
    and $b âˆˆ ğ—¥^{d}$ such that
    â—‡ â¦Š
      â€– ğ’³ = \Set{x âˆˆ ğ—¥^n}{Ax â‰¤ b} â¦‰
    â¦‰â¦‰

  â€– For this reason, the â¬problem dataâ­ $(A, b, c)$ is
    sufficient to specify a linear optimization problem. â¦‰

  â€– Recall that $Ax â‰¤ b$ means element-wise inequality (i.e.,
    that the inequality holds in each component). â¦‰
â¦‰

Â§Â§ Task â¦‰
Â¶ â¦Š
  â€– Given data $A âˆˆ ğ—¥^{m Ã— n}$, $b âˆˆ ğ—¥^n$, $c âˆˆ ğ—¥^n$, we
    want to find $x âˆˆ ğ—¥^d$ to
    â—‡ â¦Š
      â€– \begin{aligned} â¦‰

      â€– \text{minimize} &\quad c^âŠ¤x áœ¶ â¦‰

      â€– \text{subject to} &\quad Ax â‰¤ b â¦‰

      â€– \end{aligned} â¦‰
    â¦‰â¦‰

  â€– We either want $x^â˜… âˆˆ ğ—¥^n$ so that $Ax^â˜… â‰¤ b$ and $c^âŠ¤x^â˜…
    â‰¤ c^âŠ¤ x$ for all $x âˆˆ ğ—¥^n$, or we want to know that
    $\Set{x}{Ax â‰¤ b} = âˆ…$, or we want to know that for all $Î±
    âˆˆ ğ—¥$, there is an $x âˆˆ ğ—¥^n$ satisfying $Ax â‰¤ b$ and $c^âŠ¤x
    â‰¤ Î±$. â¦‰

  â€– This problem is regularly called â¬linear programmingâ­ (a
    â¬linear programâ­). â¦‰

  â€– Many authors define this problem with the goal as
    maximization: of course, minimizing $c^âŠ¤x$ is equivalent to
    (has the same set of optimal solutions) maximizing $-c^âŠ¤x$. â¦‰
â¦‰

Â§Â§ Notation â¦‰
Â¶ â¦Š
  â€– In the context of linear optimization, $c^âŠ¤x$ is often
    abbreviated $cx$. â¦‰

  â€– As usual, $x âˆˆ ğ—¥^n$ is called â¬feasibleâ­ (a â¬feasible
    solutionâ­) if $Ax â‰¤ b$. â¦‰

  â€– $x^â˜… âˆˆ ğ—¥^n$ is called â¬optimalâ­ (an â¬optimal solutionâ­,
    â¬optimum solutionâ­) if $c^âŠ¤x^â˜… â‰¤ c^âŠ¤x$ for all $x âˆˆ ğ—¥^n$. â¦‰

  â€– We sometimes denote the rows of $a$ by $\bar{a}_i^âŠ¤ âˆˆ ğ—¥^n$
    for $i = 1, â€¦, m$, i.e.,
    â—‡ â¦Š
      â€– A = \bmat{- & \bar{a}_1^âŠ¤ & - áœ¶ &â‹®& áœ¶ - & \bar{a}_m^âŠ¤
        & - } âˆˆ ğ—¥^{m Ã— n} â¦‰
    â¦‰
    and refer to the inequality $\bar{a}_i^âŠ¤ x â‰¤ b_i$ as an
    â¬inequality constraintâ­ for $i = 1, â€¦, m$. â¦‰

  â€– The set or the expression $Ax â‰¤ b$ are both sometimes
    called the â¬inequality constraintsâ­ of the problem. â¦‰

  â€– For this reason, we can say in English that linear
    optimization deals with maximizing or minimizing a linear
    objective function of â€¹finitelyâ€º many variables subject to
    finitely many linear inequalities. â¦‰
â¦‰

Â¶ â¦Š
  â€– The problem $(A, b, c)$ is â¬infeasibleâ­ if the polyhedron
    â—‡ â¦Š
      â€– P = \Set{x âˆˆ ğ—¥^n}{Ax â‰¤ b} â¦‰
    â¦‰
    is empty. â¦‰

  â€– In symbols, if $P = âˆ…$. â¦‰

  â€– The problem is â¬unboundedâ­ if for all $Î± âˆˆ ğ—¥$, there
    exists $x âˆˆ P$ with $c^âŠ¤x < Î±$. â¦‰
â¦‰

Â¶ â¦Š
  â€– Hereâ€™s an infeasible instance. Define
    â—‡ â¦Š
      â€– A = \bmat{1 áœ¶ -1} \quad b = \bmat{-1 áœ¶ -1} \quad c =
        \bmat{1} â¦‰
    â¦‰â¦‰

  â€– We want $x âˆˆ ğ—¥^1$ so that $x â‰¤ -1$ and $x â‰¥ 1$. â¦‰

  â€– There is no such $x$. â¦‰
â¦‰

Â¶ â¦Š
  â€– Hereâ€™s an unbounded instance: drop the second inequality
    constraint. â¦‰

  â€– Then we are interested in finding $x_1$ to minimize $x_1$
    subject to $x_1 â‰¤ -1$. â¦‰

  â€– Given $Î± âˆˆ ğ—¥$, if $Î± > 0$ pick $x_1 = -1$, else pick
    $2Î±$. â¦‰

  â€– This problem is unbounded. â¦‰
â¦‰

Â¶ â¦Š
  â€– Hereâ€™s a simple example with an optimal solution. â¦‰

  â€– Modify $b = \bmat{1 & 0}^âŠ¤$. â¦‰

  â€– Now we want to find $x_1$ so that
    â—‡ â¦Š
      â€– x_1 â‰¤ 1 \quad \text{ and } x_1 â‰¥ 0 â¦‰
    â¦‰
    and we minimize $x_1$ â¦‰

  â€– Clearly $x_1 = 0$ is an optimal solution. â¦‰

  â€– Indeed, it is the unique optimal solution in this case. â¦‰
â¦‰
