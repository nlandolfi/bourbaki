\input{../../sheet.tex}
\sbasic
\input{../probability_distributions/macros.tex}
\input{../logarithm/macros.tex}
\input{../real_summation/macros.tex}
\input{../probability_outcomes/macros.tex}
\input{../real_numbers/macros.tex}
\input{../natural_summation/macros.tex}
\input{../probability/macros.tex}
\input{../cardinality/macros.tex}
\input{../integer_numbers/macros.tex}
\input{../arithmetic/macros.tex}
\input{../direct_products/macros.tex}
\input{../common_sense/macros.tex}
\input{../natural_numbers/macros.tex}
\input{../solving_equations/macros.tex}
\input{../operations/macros.tex}
\input{../subsets/macros.tex}
\input{../functions/macros.tex}
\input{../equations/macros.tex}
\input{../sets/macros.tex}
\input{../relations/macros.tex}
\input{../objects/macros.tex}
\input{../ordered_pairs/macros.tex}
\input{./macros.tex}
\sstart
\stitle{Entropy}

\ssection{Why}

\ssection{Definition}

The entropy of a distribution
is the
expectation of the negative
logarithm of the distribution
under the distribution.

\ssubsection{Notation}

Let $R$ denote the set of real numbers.
Let $A$ be a finite set.
Let $p:A \to R$ be a distribution.
The entropy of $p$ is
\[
  -\sum_{a \in A} p(a) \log(p(a)).
\]
We denote the entropy of $p$ by
$H(p)$.
\strats
