%!name:entropy
%!need:probability_distributions
%!need:logarithm

\ssection{Why}\footnote{This will be included in a future edition.}

\ssection{Definition}

The entropy of a distribution
is the
expectation of the negative
logarithm of the distribution
under the distribution.
It is sometimes called the \t{discrete entropy} to distinguish it with another related topic.\footnote{Future editions may not forward reference differential entropy.}


\ssubsection{Notation}

Let $A$ be a finite set.
Let $p:A \to \R$ be a distribution.
The entropy of $p$ is
\[
  -\sum_{a \in A} p(a) \log(p(a)).
\]
We denote the entropy of $p$ by
$H(p)$.

\ssection{Properties}

Let $x: \Omega \to V$ be a discrete random variable.

\begin{enumerate}
\item $H(x) \geq 0$
\item $H(f(x)) \leq H(x)$
\item Let $g$ invertible, then $H(g(x)) \leq H(x)$
\end{enumerate}


\blankpage
