%!name:empirical_risk_minimizers
%!need:probabilistic_data-generation_models
%!refs:shai_shalev-schwartz2014understanding

\ssection{Why}

In the statistical learning framework, since the algorithm only has access to the training set, it is natural to select a hypothesis which does well.

\ssection{Definition}

Let $S$ be a training dataset (see \sheetref{inductors}{Inductors}) in $\CX \times \CY$.
The \t{training error} of a classifier $h: \CX \to \CY$ is
\[
  (\nicefrac{1}{m})\num{\Set*{i \in \upto{m}}{h(x_i) \neq y_i}}.
\]
Other terminology includes \t{empirical error} and \t{empirical risk}.
For these reasons, the learning paradigm of selecting a predictor $h$ to minimizer the empirical risk is called \t{empirical risk minimization} or \t{ERM}. 


\blankpage
