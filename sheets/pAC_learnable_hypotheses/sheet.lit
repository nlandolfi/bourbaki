<!--
!name:pAC_learnable_hypotheses
!need:probably_approximately_correct_inductors
!need:hypothesis_classes
!refs:shai_shalev-schwartz2014understanding/3
-->

Â§ Why â¦‰
Â¶ â¦Š
  â€– We want to talk about an algorithm which can learn from a
    hypothesis class, regardless of the underlying distribution. â¦‰
â¦‰

â€–   Â§ Definition â¦‰â¦‰
Â¶ â¦Š
  â€– Let $(X, ğ’³)$ and $(Y, ğ’´)$ be measurable input and output
    spaces. â¦‰
â¦‰

Â¶ â¦Š
  â€– A hypothesis class $â„‹$ of measurable functions from $X$ to
    $Y$ is â¬probably approximately correct learnableâ­ (or â¬PAC
    learnableâ­) if â¦‰

  â€– (a) there exists an inductor $ğ’œ: (X Ã— Y)^n â†’ â„‹$, so that
    (b) for every underlying measure $Î¼$ and labeling function
    $f: X â†’ Y$ (c) for every $Ïµ, Î´ âˆˆ (0, 1)$ (d) there
    exists $m_0 âˆˆ ğ—¡$ so that (e) for all $m â‰¥ m_0$
    â—‡ â¦Š
      â€– Î¼^m\left[ â¦‰

      â€– Î¼\left[ â¦‰

      â€– ğ’œ((x_i, f(x_i))_{i = 1}^{n})(Î¾) â‰  f(Î¾) â¦‰

      â€– \right] â¦‰

      â€– â‰¤ Ïµ \right] â‰¥ 1-Î´ â¦‰
    â¦‰
    where $x âˆˆ X^m$ and $Î¾ âˆˆ X$. â¦‰

  â€– In this case we say that the inductor (or learning
    algorithm) $ğ’œ$ â¬PAC learnsâ­ $â„‹$. â¦‰
â¦‰

Â¶ â¦Š
  â€– We interpret this as follows: â€œno matter the underlying
    distribution and correct labeling function, if someone
    specifies an accuracy and confidence we can tell them the
    number of samples they need so that the inductor outputs a
    hypothesis which is probably approximately correct.â€â¦‰
â¦‰

Â¶ â¦Š
  â€– Some authors require that the hypothesis class be realizable
    with respect to the underlying distribution and correct
    labeling function. â¦‰

  â€– This is natural, because if the hypothesis class includes
    the correct labeling function $f$, then it is realizable. â¦‰

  â€– In this case they refer to the above definition as the
    â¬agnostic PAC modelâ­. â¦‰

  â€– We emphasize here that there this definition includes the
    notion of realizability. â¦‰

  â€– In other words. â¦‰

  â€– We emphasize again that this definition contains to
    â€œapproximation parameters.â€ â¦‰

  â€– The accuracy parameter $Ïµ$ corresponds to the â€œapproximately
    correctâ€ piece and the confidence parameters $Î´$ corresponds
    to the â€œprobablyâ€ piece. â¦‰
â¦‰

Â§Â§ Sample complexity â¦‰
Â¶ â¦Š
  â€– Note that the existence of an $m_0$ above for each $Ïµ$ and
    $Î´$ is equivalent to requiring that there exists $m_0: (0,
    1)^2 â†’ \N$ so that for all $m â‰¥ m_0(Ïµ,Î´)$ the condition in
    the above equation holds. â¦‰

  â€– There may exist multiple such $m_0$, so we define
    $\tilde{m}: (0, 1)^2 â†’ ğ—¡$ so that $\tilde{m}(Ïµ,Î´)$ is the
    smallest integer so that the above equation holds. â¦‰

  â€– We call $\tilde{m}$ the â¬sample complexityâ­. â¦‰

  â€– Clearly it is a function of $Ïµ$ and $Î´$. â¦‰

  â€– It also depends on the hypothesis class $â„‹$ and the
    learning algorithm $ğ’œ$. â¦‰
â¦‰

<tex>
  â€– \blankpage â¦‰
</tex>
