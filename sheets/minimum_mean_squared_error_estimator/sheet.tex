%!name:minimum_mean_squared_error_estimator
%!need:estimators
%!need:expectation
%!need:norms

\ssection{Why}

What if the best estimator for a real-value random variable if we consider the squared loss.

\ssection{Definition}

Let $(\Omega, \CA, \PM)$ be a probability space and $x: \Omega \to \R$ a random variable.

A \t{minimum mean squared error estimator} or \t{MMSE estimator} or \t{least square estimator} is a value $\xi \in \R$ which minimizes $\E (x - \xi)^2$.

\begin{proposition}
  There is a unique MMSE estimator and it is given by $\E(x)$.
\end{proposition}

\ssection{Vector Case}

Let $(\Omega, \CA, \PM)$ be a probability space and $y: \Omega \to \R^n$ a random variable.\footnote{Future editions might collapse this into the previous case.}

A \t{minimum mean squared error estimator} or \t{MMSE estimator} or \t{least square estimator} is a value $\xi \in \R$ which minimizes $\E \norm{x - \xi}^2$.

\begin{proposition}
  There is a unique MMSE estimator and it is given by $\E(y)$.
\end{proposition}

\ssection{Case with observation}
Let $x:\Omega\to\R^n$ and $y: \Omega \to \R^m$.
A \t{minimum mean squared error estimator} or \t{MMSE estimator} or \t{least square estimator} for $x$ given $y$ is an estimator $f: \R^m \to \R^n$ which minimizes $\E \norm{f(x) - y}^2$.


\blankpage
