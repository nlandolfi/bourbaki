<!--yaml
name: independent_events
needs:
    - conditional_event_probabilities
-->

¬ß Why ‚¶â
¬∂ ‚¶ä
  ‚Äñ We want to talk about how knowledge of one aspect of an
    outcome can give us knowledge about another aspect. ‚¶â
‚¶â

¬ß¬ß Dependent events ‚¶â
¬∂ ‚¶ä
  ‚Äñ Suppose $Œ©$ is a finite set of outcomes with event
    probability function $P$. ‚¶â

  ‚Äñ Two uncertain events $A$ and $B$ with $P(A), P(B) > 0$ are
    ‚ù¨dependent‚ù≠ under $P$ if
    ‚óá ‚¶ä
      ‚Äñ P(A | B) ‚â† P(A) \quad \text{ or } \quad P(B | A) ‚â†
        P(B) ‚¶â
    ‚¶â‚¶â

  ‚Äñ In other words, if conditioning on one will change the
    probability of the other. ‚¶â

  ‚Äñ We can rewrite the condition as,
    ‚óá ‚¶ä
      ‚Äñ \frac{P(A ‚à© B)}{P(B)} ‚â† P(A) \quad \text{ or } \quad
        \frac{P(A ‚à© B)}{P(A)} ‚â† P(B) ‚¶â
    ‚¶â‚¶â

  ‚Äñ We see that in either case, $P(A ‚à© B) ‚â† P(A)P(B)$. ‚¶â
‚¶â

¬ß Definition ‚¶â
¬∂ ‚¶ä
  ‚Äñ Two events $A$ and $B$ are ‚ù¨independent‚ù≠ under $P$ if
    ‚óá ‚¶ä
      ‚Äñ P(A ‚à© B) = P(A)P(B). ‚¶â
    ‚¶â‚¶â

  ‚Äñ In other words, they are independent if the probability of
    their intersection is the product of their respective
    probabilities. ‚¶â

  ‚Äñ This definition clearly shows that independence (and
    dependence) is a ‚Äπsymmetric‚Ä∫ relation on the set of events. ‚¶â

  ‚Äñ Moreover, the expression $P(A ‚à© B) = P(A)P(B)$ is
    well-defined even when $P(A)$ or $P(B)$ is 0. ‚¶â
‚¶â

¬∂ ‚¶ä
  ‚Äñ As we have seen, in the case that $P(B) ‚â† 0$, $P(A ‚à© B)
    = P(A)P(B)$ is equivalent to $P(A \mid B) = P(A)$. ‚¶â

  ‚Äñ Roughly speaking, we interpret this second expression as
    encoding the fact that the occurence of event $B$ does not
    change the probability‚Äîintuitively, the ‚Äúcredibility‚Äù‚Äîof the
    event $A$. ‚¶â
‚¶â

¬ß Examples ‚¶â
¬∂ ‚¶ä
  ‚Äñ ‚ÄπTwo coin tosses.‚Ä∫ ‚¶â

  ‚Äñ As usual, model flipping a coin twice with the sample space
    $Œ© = \set{0,1}^2$. ‚¶â

  ‚Äñ Define $A = \set{(1,0), (1, 1)}$, the event that the first
    tops is heads, and $B = \set{(0,1), (1,1)}$, the event that
    the second toss turns up head. ‚¶â

  ‚Äñ Then $A ‚à© B$ is $\set{(1,1)}$, the event both tosses turn
    up heads. ‚¶â

  ‚Äñ Suppose we put a distribution on $Œ©$ as usual with $p(œâ) =
    1/4$ for all $œâ ‚àà Œ©$. ‚¶â

  ‚Äñ Then $P(A) 1/2$, $P(B) = 1/2$, and $P(A ‚à© B) = 1/4$. ‚¶â

  ‚Äñ Thus:
    ‚óá ‚¶ä
      ‚Äñ P(A ‚à© B) = P(A)P(B) ‚¶â
    ‚¶â
    and so the events are independent events under the
    distribution. ‚¶â
‚¶â

¬∂ ‚¶ä
  ‚Äñ ‚ÄπThree tosses.‚Ä∫ ‚¶â

  ‚Äñ As usual, model flipping a coin three times with the sample
    space $Œ© = \set{0,1}^3$ and define $p: Œ© ‚Üí [0,1]$ by $p(œâ)
    = 1/8$ for all $œâ ‚àà Œ©$. ‚¶â

  ‚Äñ Let $A$ be the event $\set{(1, 1, 0), (1,1,1)}$, the event
    that the first two tosses turn up heads, and $B = \set{(1,
    0, 0), (0, 0, 0)}$, the event the second two tosses are
    tails. ‚¶â

  ‚Äñ Then $P(A) = P(B) = 2/8 = 1/4$, but $A ‚à© B = ‚àÖ$. ‚¶â

  ‚Äñ So $P(A)P(B) = 1/16 ‚â† 0 P(A ‚à© B)$. ‚¶â

  ‚Äñ These are dependent events under the model. ‚¶â
‚¶â

¬∂ ‚¶ä
  ‚Äñ ‚ÄπRolling two dice.‚Ä∫ ‚¶â

  ‚Äñ As usual, model rolling two dice with the sample space $Œ©
    = \Set{(œâ_1, œâ_2)}{ œâ_i ‚àà \set{1, ‚Ä¶, 6}}$. ‚¶â

  ‚Äñ Define a distribution $p: Œ© ‚Üí ùó•$ by $p(œâ) = 1/36$. ‚¶â

  ‚Äñ Two events are $A = \Set{œâ ‚àà Œ©}{ œâ_1 + œâ_2 > 5}$, ‚Äúthe
    sum is greater than 5‚Äù, and $B = \Set{œâ ‚àà Œ©}{œâ_1 > 3}$,
    ‚Äúthe number of pips on the first die is greater than 3‚Äù. ‚¶â

  ‚Äñ Then $P(A) = 26/36$. ‚¶â

  ‚Äñ Also, $P(A | B) = 17/16$. ‚¶â

  ‚Äñ So, these events are dependent. ‚¶â

  ‚Äñ Roughly speaking, we say that knowing $B$ tells us something
    about $A$. ‚¶â

  ‚Äñ In this case, we say that it ‚Äúmakes $A$ more probable.‚Äù ‚¶â

  ‚Äñ In the language used to describe the events, we say that
    knowledge that the number of pips on the first die is
    greater than three makes its more probable that the sum of
    the number of pips on each die is greater than 5. ‚¶â
‚¶â

¬ß¬ß Basic implications ‚¶â
¬∂ ‚¶ä
  ‚Äñ Suppose $P$ is a probability measure on a finite sample
    space $Œ©$. ‚¶â

  ‚Äñ It happens that $A$ and $B$‚Äôs independence under $P$ is
    sufficient for the independence of $A^c$ and $B$, $A^c$ and
    $B^c$ and $A$ and $B^c$. ‚¶â

  ‚Äñ Here $A^c$ and $B^c$ denote the relative complement of $A$
    and $B$ in $Œ©$, respectively. ‚¶â
‚¶â

¬∂ ‚¶ä
  ‚Äñ There are a few ways to see these‚Äîhere‚Äôs one
    ‚óá ‚¶ä
      ‚Äñ \begin{aligned} ‚¶â

      ‚Äñ P(A^c ‚à© B) ‚¶â

      ‚Äñ &= P(B) - P(A ‚à© B)·ú∂ ‚¶â

      ‚Äñ &= P(B) - P(A)P(B)·ú∂ ‚¶â

      ‚Äñ &= (1- P(A))P(B) = P(A^c)P(B). ‚¶â

      ‚Äñ \end{aligned} ‚¶â
    ‚¶â‚¶â

  ‚Äñ The first equality here holds since $P(B) = P(A ‚à© B) +
    P(A^c ‚à© B)$. ‚¶â
‚¶â

¬∂ ‚¶ä
  ‚Äñ Here are alternative routes, using the more explicit notation
    of relative complements. ‚¶â

  ‚Äñ To this, we note that since $P(¬∑|B)$ is a probability
    measure, and the events $A$ and $\relcomplement{A}{Œ©}$
    partition $Œ©$, we have
    ‚óá ‚¶ä
      ‚Äñ P(A | B) + P(\relcomplement{A}{Œ©} | B) = 1. ‚¶â
    ‚¶â‚¶â

  ‚Äñ From which we deduce $P(\relcomplement{A}{Œ©} | B) = 1 -
    P(A) = P(\relcomplement{A}{Œ©})$. ‚¶â

  ‚Äñ Which is equivalent to $P(\relcomplement{A}{Œ©} ‚à© B) =
    P(\relcomplement{A}{Œ©})P(B)$. ‚¶â

  ‚Äñ In other words, $B$ and $\relcomplement{A}{Œ©}$ are independent
    events. ‚¶â

  ‚Äñ Similarly, $A$ and $\relcomplement{B}{Œ©}$ are independent
    events. ‚¶â

  ‚Äñ Since $(Œ© - A) ‚à© (Œ© - B) = Œ© - (A ‚à™ B)$, we have
    ‚óá ‚¶ä
      ‚Äñ P(A ‚à™ B) + P(\relcomplement{A}{Œ©} ‚à© \relcomplement{B}{Œ©})
        = 1. ‚¶â
    ‚¶â
    Since $P(A ‚à™ B) = P(A) + P(B) - P(A ‚à© B)$, we obtain
    ‚óá ‚¶ä
      ‚Äñ P(\relcomplement{A}{Œ©} ‚à© \relcomplement{B}{Œ©}) = 1 - P(A)
        - P(B) - P(A)P(B). ‚¶â
    ‚¶â‚¶â

  ‚Äñ We can express the right hand side as $(1 - P(A))(1 -
    P(B))$ or $P(\relcomplement{A}{Œ©})P(\relcomplement{B}{Œ©})$. ‚¶â

  ‚Äñ In other words, $\relcomplement{A}{Œ©}$ and
    $\relcomplement{B}{Œ©}$ are independent. ‚¶â
‚¶â
