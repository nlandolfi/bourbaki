%!name:probability_vectors
%!need:probability_distributions
%!need:real_vectors
%!need:real_inner_product
%!need:outcome_variable_expectation

\section*{Why}

We can identify probability distributions with vectors.

\section*{Definition}

Let $p: \Omega  \to \R $ be a probability distribution on a finite set $\Omega  = \set{\omega _1, \dots , \omega _n}$.
We can associate $p$ with the vector $x \in \R ^n$ defined by $x_i = p(\omega _i)$ for $i = 1, \dots , n$.
We call this vector $y$ the \t{probability vector} associated with $p$.
The conditions on $p$ mean that (1) $\ip{1, y} = 1$ and (2) $y \geq 0$ (i.e., $y_i \geq 0$ for all $i = 1, \dots , n$.).

Conversely, suppose $z \in \R $ satifies (1) and (2).
Then $q: \Omega  \to \R $ defined by $q(\omega _i) = z_i$ for $i = 1, \dots , n$ is a probability distribution.
For this reason, we call $z$ satisfying the conditions a \t{distribution vector}.
Notice that implicit in this correspondence is a numbering $\omega : \set{1, \dots , n} \to \Omega $ of the set of outcomes $\Omega $.

\section*{Expectation}

Suppose $\rho  \in \R ^n$ is a distribution vector corresponding to $p: \Omega  \to \R $ its corresponding distribution.
Let $x: \Omega  \to \R $ and (similar to $\rho $) define $\xi \in \R ^n$ by $\xi _i = x(\omega _i)$ for $i = 1, \dots , n$.
Then $\E x = \ip{\rho , \xi }$.

\subsection*{Example}

For $\rho  = (-1, -1, 1, 1, 2)$ and $\xi  = (0.1, 0.15, 0.1, 0.25, 0.4)$
  \[
\E x = \ip{\rho , \xi } -1 -0.15 + 0.1 + 0.25 + 2(0.4) = 0.9.
  \]

\blankpage