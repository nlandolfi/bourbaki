%!name:cross-entropy
%!need:entropy

\ssection{Why}

\ssection{Cross-entropy}

Consider two distributions
on the same finite set.
The \ct{cross-entropy}{}
of the first distribution
\ct{relative}{} to the
second distribution
is the expectation of the
negative logarithm of the first distribution
under the second distribution.

\ssubsection{Notation}

Let $R$ denote the set of
real numbers. Let
$A$ be a finite set.
Let $p: A \to R$ and
$q: A \to R$ be distributions.
The cross-entropy of $p$ relative to $q$
is
\[
  -\sum_{a \in A} q(a) \log(p(a)).
\]
