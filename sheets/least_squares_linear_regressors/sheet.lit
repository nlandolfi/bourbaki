<!--yaml
name: least_squares_linear_regressors
needs:
    - matrix_transpose
    - real_matrix_inverses
    - loss_functions
    - norms
    - linear_predictors
    - regressors
-->

§ Why ⦉
¶ ⦊
  ‖ What is the best linear regressor if we choose according to
    a squared loss function. ⦉
⦉

§ Definition ⦉
¶ ⦊
  ‖ Let $X ∈ 𝗥^{n ×d}$ and $y ∈ 𝗥^d$. ⦉

  ‖ In other words, we have a paired dataset of records with
    inputs in $𝗥^d$ (the rows of $X$) and outputs in $𝗥$ (the
    elements of $y$). ⦉
⦉

¶ ⦊
  ‖ A ❬least squares linear predictor❭ or ❬linear least squares
    predictor❭ is a linear transformation $f: 𝗥^d → 𝗥$ (the
    field is $𝗥$) which minimizes ⦉

  ‖     ◇ ⦊
      ‖ \frac{1}{n} ∑_{i = 1}^{n} (f(x^i) - y_i)^2. ⦉
    ⦉

    ‖ over the dataset of pairs $(x^1, y_1), …, (x^n, y_n) ∈
      𝗥^d ×𝗥$ where $(x^i)^⊤$ is the $i$th row of $X$ for $i =
      1, …, n$. ⦉⦉
  ¶ ⦊
    ‖ The set of linear functions from $𝗥^d$ to $𝗥$ is in
      one-to-one correspondence with $𝗥^d$. ⦉

    ‖ So we want to find $θ ∈ 𝗥^d$ to minimize ⦉

    ‖       ◇ ⦊
        ‖ \frac{1}{n} \norm{Xθ - y}^2. ⦉
      ⦉⦉
  ⦉
⦉

§ Solution ⦉
<statement type='proposition'>
  ‖ There exists a unique linear least squares predictor and its
    parameters are given by $(X^⊤X)^{-1}X^⊤y$.
    † ⦊
      ‖ Future editions will include an account. ⦉
    ⦉⦉
</statement>
<tex>
  ‖ \blankpage ⦉
</tex>