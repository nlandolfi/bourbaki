<!--yaml
name: least_squares_linear_regressors
needs:
    - matrix_transpose
    - real_matrix_inverses
    - loss_functions
    - norms
    - linear_predictors
    - regressors
-->

Â§ Why â¦‰
Â¶ â¦Š
  â€– What is the best linear regressor if we choose according to
    a squared loss function. â¦‰
â¦‰

Â§ Definition â¦‰
Â¶ â¦Š
  â€– Let $X âˆˆ ğ—¥^{n Ã—d}$ and $y âˆˆ ğ—¥^d$. â¦‰

  â€– In other words, we have a paired dataset of records with
    inputs in $ğ—¥^d$ (the rows of $X$) and outputs in $ğ—¥$ (the
    elements of $y$). â¦‰
â¦‰

Â¶ â¦Š
  â€– A â¬least squares linear predictorâ­ or â¬linear least squares
    predictorâ­ is a linear transformation $f: ğ—¥^d â†’ ğ—¥$ (the
    field is $ğ—¥$) which minimizes â¦‰

  â€–     â—‡ â¦Š
      â€– \frac{1}{n} âˆ‘_{i = 1}^{n} (f(x^i) - y_i)^2. â¦‰
    â¦‰

    â€– over the dataset of pairs $(x^1, y_1), â€¦, (x^n, y_n) âˆˆ
      ğ—¥^d Ã—ğ—¥$ where $(x^i)^âŠ¤$ is the $i$th row of $X$ for $i =
      1, â€¦, n$. â¦‰â¦‰
  Â¶ â¦Š
    â€– The set of linear functions from $ğ—¥^d$ to $ğ—¥$ is in
      one-to-one correspondence with $ğ—¥^d$. â¦‰

    â€– So we want to find $Î¸ âˆˆ ğ—¥^d$ to minimize â¦‰

    â€–       â—‡ â¦Š
        â€– \frac{1}{n} \norm{XÎ¸ - y}^2. â¦‰
      â¦‰â¦‰
  â¦‰
â¦‰

Â§ Solution â¦‰
<statement type='proposition'>
  â€– There exists a unique linear least squares predictor and its
    parameters are given by $(X^âŠ¤X)^{-1}X^âŠ¤y$.
    â€  â¦Š
      â€– Future editions will include an account. â¦‰
    â¦‰â¦‰
</statement>
<tex>
  â€– \blankpage â¦‰
</tex>