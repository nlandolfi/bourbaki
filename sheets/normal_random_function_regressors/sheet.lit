<!--yaml
name: normal_random_function_regressors
needs:
    - normal_random_function_predictive_densities
    - loss_functions
-->

Â§ Why â¦‰
Â¶ â¦Š
  â€– We use a loss function and a predictive density to
    construct a regressor on the domain of the random process. â¦‰
â¦‰

Â§ Setup â¦‰
Â¶ â¦Š
  â€– Let $â„“: ğ—¥ Ã—ğ—¥ â†’ ğ—¥$ be a loss function. â¦‰

  â€– We choose a predictor to minimize the expected loss under
    the predictive density. â¦‰
â¦‰

Â§Â§ Squared error case â¦‰
Â¶ â¦Š
  â€– Consider $â„“(Î±, Î²) = (Î± - Î²)^2$. â¦‰

  â€– The â¬minimum squared error normal random function predictorâ­
    or â¬minimum squared error gaussian process predictorâ­ for
    dataset $(a^1, Î³_1), â€¦, (a^n, Î³_n)$ in $A Ã—ğ—¥$ is the
    predictor which minimizes the squared error loss. â¦‰

  â€– Since the predictive density is normal, the minimizer is the
    conditional mean. â¦‰
â¦‰

Â§Â§ Absolute error case â¦‰
Â¶ â¦Š
  â€– Consider $â„“(Î±, Î²) = \abs{Î± - Î²}$. â¦‰

  â€– The â¬minimum absolute deviation normal random function
    predictorâ­ or â¬minimum absolute deviation gaussian process
    predictorâ­ for dataset $(a^1, Î³_1), â€¦, (a^n, Î³_n)$ in $A Ã—ğ—¥$
    is the predictor which minimizes the absolute deviation loss. â¦‰

  â€– For any density, the solution is the median. â¦‰

  â€– Since the predictive density is normal, and so symmetric,
    the median is the conditional mean. â¦‰

  â€– In other words, the minimum absolute deviation normal random
    function predictor coinicides with the minimium squared error
    normal random function predictor. â¦‰
â¦‰

Â§ Definition â¦‰
Â¶ â¦Š
  â€– For this reason, the â¬normal random function predictorâ­ or
    â¬gaussian process predictorâ­ for dataset $(a^1, Î³_1), â€¦, (a^n,
    Î³_n)$ in $A Ã—ğ—¥$ is $h: A â†’ ğ—¥$ defined by
    â—‡ â¦Š
      â€– h(x) = m(x) + \pmat{k(x,a^1) â‹¯ k(x, a^n)}\invp{Î£_{a} +
        Î£_{e}}(Î³ - m_{a}). â¦‰
    â¦‰â¦‰

  â€– In other words, the regressor which assigns to each point
    its conditional mean. â¦‰
â¦‰

Â¶ â¦Š
  â€– Notice that $h$ is an affine function of $Î³$. â¦‰

  â€– If the mean function $m â‰¡ 0$ then $h$ is linear in $Î³$. â¦‰

  â€– This is sometimes called a â¬linear estimatorâ­.
    â€  â¦Š
      â€– We avoid the other terminology we have seen usedâ€”linear
        predictorâ€”because the predictor $h$ is not linear in its
        input $x$. â¦‰
    â¦‰â¦‰

  â€– Alternatively, notice (in the zero mean setting) that $h$ is
    a linear combination of $n$ kernel function $k(x, a^i)$ for
    $i = 1, â€¦, n$. â¦‰

  â€– Specifically, $h$ is a linear combination of â¦‰
â¦‰

Â¶ â¦Š
  â€– The process of using a normal random function predictor is
    often called â¬Gaussian process regressionâ­ or (especially in
    spatial statistics) â¬krigingâ­. â¦‰

  â€– The upside is that a gaussian process predicor interpolates
    the data, is smooth, and the so-called variance increases
    with the distance from the data.
    â€  â¦Š
      â€– This last piece is true for certain covariance kernels
        and will be clarified in future editions. â¦‰
    â¦‰â¦‰
â¦‰