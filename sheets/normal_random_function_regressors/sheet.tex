%!name:normal_random_function_regressors
%!need:normal_random_functions
%!need:affine_functions

\ssection{Why}

We use a normal random function model to make a regressor.

\ssection{Definition}

Let $F: \Omega \to (A \to \R)$ be a normal random function with mean function $m: A \to \R$ and covariance function $k: A \times A \to \R$ over the probability space $(\Omega, \CA, \PM)$.
Let the family of random variables (or stochastic process) of $F$ be $f: A \to (\Omega \to \R)$.

Let $e$ be a normal random vector with mean zero and covariance $\Sigma_{e}$.
We call $e$ the \t{error vector} or \t{noise vector}.
Let $a^1, \dots, a^n \in A$.
We sometimes call the sequence $a^1, \dots, a^n$ the \t{design}.
Define $y: \Omega \to \R^d$ by
\[
  y_i = f(a^i) + e_i
\]
We call $y$ the \t{observation vector} or \t{observation random vector}.

Let $b^1, \dots, b^m \in A$.
Define $z: \Omega \to \R^d$ by $z_i = f(b^i)$ for $i = 1, \dots, n$.
So $z_i$ is the random variable corresponding to the family at index $b^i \in A$.
Then $(y, z)$ is normal.
We call the conditional density of $z$ given $y$ the \t{predictive density} for $b$ given $a$.

\begin{proposition}
  Define $m_{a} \in \R^{n}$ by $\tranpose{(m(a^1),\cdots,m(a^n))}$ and define $m_{b}$ by $\tranpose{(m(b^1), \cdots, m(b^m))}$.\footnote{Future editions will fix the re-use of the symbol $m$.}
  Define $\Sigma_a \in \R^{n \times n}$ by
  \[
    \pmat{
      k(a^1, a^1) & \cdots & k(a^1, a^n) \\
      \vdots & \ddots & \vdots \\
      k(a^n, a^1) & \cdots & k(a^n, a^n) \\
    }
  \]
  and define $\Sigma_{ba} \in \R^{m \times n}$ by
  \[
    \pmat{
      k(b^1, a^1) & \cdots & k(b^1, a^n) \\
      \vdots & \ddots & \vdots \\
      k(b^m, a^1) & \cdots & k(b^m, a^n) \\
    }.
  \]
  The predictive density $g_{z \mid y}(\cdot, \gamma): \R^m \to \R$ of $b \in A$ for design $a^1, \dots, a^n$ is normal with mean.
  \[
    m_b  + K_{ba}\invp{K_{a} + \Sigma_{e}}\parens{\gamma - m_a}
  \]
  and covariance
  \[
    \Sigma_{b} - \Sigma_{ba}\invp{\Sigma_{a} + \Sigma_{e}}\Sigma_{ab}.
  \]
\end{proposition}

\ssection{Regressor}

Ultimately, we want a regressor $h: A \to \R$.
We maximize the the predictive density.
The \t{normal random function predictor} or \t{gaussian process predictor} for dataset $(a^1, \gamma_1), \dots, (a^n, \gamma_n)$ in $A \times \R$ is $h: A \to \R$ defined by
\[
  h(x) = m(x) + \pmat{k(x,a^1) \cdots k(x, a^n)}\invp{\Sigma_{a} + \Sigma_{e}}\parens{\gamma - m_{a}}.
\]
Notice that $h$ is an affine function of $\gamma$.
If the mean function $m \equiv 0$ then $h$ is linear in $\gamma$.
This is sometimes called a \t{linear estimator}.\footnote{We avoid the other terminology we have seen used---linear predictor---because the predictor $h$ is not linear in its input $x$.}
Alternatively, notice (in the zero mean setting) that $h$ is a linear combination of $n$ kernel function $k(x, a^i)$ for $i = 1, \dots, n$.
Specifically, $h$ is a linear combination of

The process of using a normal random function predictor is often called \t{Gaussian process regression}.
The upside is that a gaussian process predicor interpolates the data, is smooth, and the so-called variance increases with the distance from the data.\footnote{This last piece is true for certain covariance kernels and will be clarified in future editions.}
