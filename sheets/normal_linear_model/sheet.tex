%!name:normal_linear_model
%!need:probabilistic_linear_model
%!need:normal_conditionals
%!need:maximum_conditional_estimates

\ssection{Why}

We consider the probabilistic linear model in which all random variables are normal.

\ssection{Definition}

A \t{normal linear model} is a probabilistic linear model in which the parameter and noise vectors have normal (Gaussian) densities.
The model is also called the \t{Gaussian linear model} or the \t{linear model with Gaussian noise}.

Let $(x: \Omega \to \R^d, A \in \R^{n \times d}, e: \Omega \to \R^n)$ be a probabilistic linear model over the probability space $(\Omega, \CA, \PM)$ in which $x$ and $e$ have normal densities.
Recall that a probabilistic linear model has observation vector $y: \Omega \to \R^n$ defined by
\[
  y = Ax + e.
\]

\ssection{Conditional density of $x$ on $y$}

Since $x$ and $e$ are normal and independent, $y$ is normal.\footnote{Future editions will include an account.}
Moreover, the random vector $(x, y)$ is normal with covariance
\[
  \pmat{
    \Sigma_x & \Sigma_{x}\transpose{A} \\
      A \Sigma_{x} & A\Sigma_{x}\transpose{A} + \Sigma_{e}
  }.
\]
So the conditional density (see \sheetref{normal_conditionals}{Normal Conditionals}) of $g_{x \mid y}(\cdot, \gamma)$ is normal with mean
  \[
    \Sigma_{x}\transpose{A}\invp{A\Sigma_{x}\transpose{A} + \Sigma_e}\gamma
  \]
  and covariance
  \[
    \Sigma_{x} - \Sigma_{x}\transpose{A}\invp{A\Sigma_{x}\transpose{A} + \Sigma_e}A\Sigma_{x}.
  \]
This density is sometimes called the posterior for the parameters given the observations.
So the parameter posterior of the normal linear model is normal.

We can write the conditional mean as
\[
    \invp{\inv{\Sigma_{x}} + \transpose{A}\inv{\Sigma_{e}}A}\transpose{A}\inv{\Sigma_{e}}
\]
and the conditional covariance as
  \[
     \invp{\inv{\Sigma_x} + \transpose{A}\inv{\Sigma_{e}}A}.\footnote{A proof will appear in future editions. Use the matrix inversion lemma or facts about inverses.}
  \]
Very frequently we use these forms when $d < n$.
In other words, in the case that we have fewer unknowns than measurements.
In that case $\Sigma_{x}$ is smaller than $A\Sigma_{x}\transpose{A}$.

\ssubsection{Maximum conditional estimate of $x$}

\begin{proposition}
  The maximum conditional estimate of $x: \Omega \to \R^d$ given observed value $\gamma \in \R^n$ of $y: \Omega \to \R^n$ is the conditional mean
\[
  \Sigma_{x}\transpose{A}\invp{A\Sigma_{x}\transpose{A} + \Sigma_{e}}\gamma.
\]
\end{proposition}
