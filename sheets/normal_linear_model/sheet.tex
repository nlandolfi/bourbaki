%!name:normal_linear_model
%!need:probabilistic_linear_model
%!need:normal_conditionals

\ssection{Why}

We consider the probabilistic linear model in which all random variables involved are normal.

\ssection{Definition}

The \t{normal linear model} is a linear model in which the signal and noise have normal (Gaussian) densities.
For this reason, the model is often called the \t{Gaussian linear model} or the \t{linear model with Gaussian noise}.

Let $(\Omega, \CA, \PM)$ be a probability space.
Let $x: \Omega \to \R^d$ and $e: \Omega \to \R^d$ be independent normal random vectors with zero mean and covariances $\Sigma_{x}$ and $\Sigma_e$.
Let $y = Ax + e$.
%$y^i(\omega) = f(\omega)(x^i) = \theta(\omega)^\top x^i$, $\forall i = 1, \dots, n$.
We have $n$ precepts in $\R^d$.
So let $a^1, \dots, a^n \in \R^d$ with data matrix $A \in \R^{n \times d}$.

\ssection{Maximum conditional estimate of $x$}

Since $x$ and $e$ are gaussian, $y$ is gaussian.
So the random vector $(x, y)$ has normal density with mean zero and covariance
\[
  \pmat{
    \Sigma_x & \Sigma_{x}\transpose{A} \\
      A \Sigma_{x} & A\Sigma_{x}\transpose{A} + \Sigma_{e}
  }.
\]

This is also called \t{bayesian linear regression} or the \t{bayesian analysis of the linear model}.
The word bayesian is in reference to treating the quantity of interest---$x$---as a random variable.

\begin{proposition}
  The maximum conditional estimate of $x: \Omega \to \R^d$ given observed value $\gamma \in \R^n$ of $y: \Omega \to \R^n$ is the conditional mean $\Sigma_{xy}\inv{\Sigma_{yy}}\gamma$.
\end{proposition}
Recall that the maximum conditional estimate also maximizes the joint density.

\ssubsection{Uncorrelated noise}

Suppose that $\Sigma_e = \sigma^2I$.


\begin{proposition}
  A solution to maximize $g(\alpha, \gamma)$ with respect to $\alpha$ is $\alpha = -\inv{\Sigma}\Sigma\transpose{X}\gamma$.
\end{proposition}
\begin{proposition}
  $g_{\theta \mid y}(\alpha, \gamma)$ is normal with mean
  \[
    \tilde{\mu}(\gamma) = \Sigma \transpose{X}\inversep{X\Sigma\transpose{X}}\gamma
  \] and covariance \[\tilde{\Sigma} = \Sigma - \Sigma\transpose{X}\inversep{X\Sigma\transpose{X}}X\Sigma.\]% (\sheetref{normal_conditionals}{Normal Conditionals}).
\end{proposition}
\begin{proposition}
  A solution to maximize $g_{\theta | y}(\alpha, \gamma)$ w.r.t. $\alpha$ is
  \[
    \tilde{\Sigma}\inv{\tilde{\Sigma}}\tilde{\mu}(\gamma).
  \]
\end{proposition}


But, of course, $y$ also has a density.
Denote the density of $y$ by $g: \R^n \to \R$.
In other words, $g \geq 0$ and $\int g = 1$.

\begin{proposition}
  \[
    \log g(\gamma) = -\nicefrac{1}{2}(\transpose{\gamma}\inv{\left(X\Sigma\transpose{X}\right)}\gamma) - \frac{d}{2}\log 2\pi - \frac{1}{2}\log\det\left(X\Sigma\transpose{X}\right)
  \]
\end{proposition}


\ssection{Test}


This expression makes clear that $y$ is has a normal density with mean $X\E(x)$ and covariance $X\E(x)X^{\top}$.

Let $w: \Omega \to \R^d$ be a random vector with mean $0$ and covariance $\eta I$.
Let $x^1, \dots, x^n \in \R^d$
Define $y^i: \Omega \to \R$ by $y_i(\omega) = w(\omega)^\top x^i$ for $i = 1, \dots, d$.

\ssection{Noise setup}

Let $e: \Omega \to \R^n$ be a normal random vector with mean $0$ and covariance $\sigma I$.
Define $\tilde{y}:\Omega\to\R^n$ by $\tilde{y} = y(\omega) + e(\omega)$.

\begin{proposition}
  $\tilde{y}$ is a normal random vector with mean zero and covariance $X \Sigma \transpose{X} + \sigma I$.
\end{proposition}

\blankpage
