<!--yaml
name: outcome_probabilities
needs:
    - real_summation
    - uncertain_outcomes
    - set_numbers
    - real_functions
-->

§ Why ⦉
¶ ⦊
  ‖ A set of outcomes may be finite or infinite. ⦉

  ‖ For now, we consider finite samples spaces. ⦉

  ‖ To talk about the uncertain outcomes, we assign credibility
    to each outcome according to our intuition of proportion.
    † ⦊
      ‖ Future editions may drop the dependence on real numbers,
        and use intuition of repeated trials to introduce
        ‹rational› probability distributions. ⦉
    ⦉⦉
⦉

§ Definition ⦉
¶ ⦊
  ‖ Suppose $Ω$ is a finite set. ⦉

  ‖ A $p: Ω → 𝗥$ ‹on› $Ω$ is a ❬probability distribution❭ ‹on›
    $Ω$ if it is nonnegative (i.e., $p(ω) ≥ 0$ for every $ω ∈
    Ω$) and
    ◇ ⦊
      ‖ \textstyle ⦉

      ‖ ∑_{ω ∈ Ω} p(ω) = 1 ⦉
    ⦉⦉

  ‖ The ❬probability❭ ‹of› an outcome $ω ∈ Ω$ ‹under› the
    distribution $p$ is the result $p(ω)$. ⦉
⦉

§§ Interpretations ⦉
¶ ⦊
  ‖ There are two usual meanings of the word “probability”. ⦉

  ‖ The first, is its intuitive interpretation as frequency—the
    fraction of times that an outcome $ω$ will occur if we are
    able to repeat the scenario producing the outcomes many times. ⦉

  ‖ This is the so-called ❬frequentist❭ viewpoint. ⦉
⦉

¶ ⦊
  ‖ The trouble is that some scenarios are not “repeatable”
    (e.g., whether it will rain or not ‹tomorrow›). ⦉

  ‖ Thus, it is sometimes natural to think of probabilities as
    ❬beliefs❭ or ❬degrees of belief❭ which are updated according
    to particular rules.
    † ⦊
      ‖ Future editions may elaborate on the justification for
        these rules, according to Keynes and Jaynes. ⦉
    ⦉⦉

  ‖ This is the so-called ❬Bayesian viewpoint❭. ⦉
⦉

¶ ⦊
  ‖ This second interpretation matches the English etymology: the
    word probabiliy has its roots in the English word probable,
    which has the Middle English sense “worthy of belief”. ⦉

  ‖ The probability of an outcome models how worthy of belief
    it is, relative to other outcomes. ⦉

  ‖ In the case of flipping a coin, or rolling a die, we may
    assert that all outcomes are equally worthy of belief. ⦉
⦉

¶ ⦊
  ‖ If a first outcome has a larger probability than a second
    outcome, we call the first ❬more probable❭ than the second. ⦉

  ‖ Similarly, we call the second outcome ❬less probable❭ than
    the first outcome. ⦉

  ‖ Some authors use the term ❬more likely❭ and ❬less likely❭. ⦉
⦉

§ Examples ⦉
¶ ⦊
  ‖ ‹Probabilities for flipping a coin.› ⦉

  ‖ Suppose we model flipping a coin,␣
    <a href='/sheets/uncertain_outcomes.html#Examples_of_modeling'>
      ‖ as before ⦉
    </a>
    , with the sample space $\set{0,1}$. ⦉

  ‖ We may model both heads and tails as equally worthy of
    belief. ⦉

  ‖ Thus we would like to pick two nonnegative numbers $p(1)$
    and $p(2)$ so that they are non-negative and $p(1) + p(2) =
    1$. ⦉

  ‖ Consequently, we model outcome $p(0) = p(1) = ½$. ⦉

  ‖ We often refer to this particular model as a ❬fair coin❭. ⦉

  ‖ Neither heads nor tails is ‹more› or ‹less› probable. ⦉
⦉

¶ ⦊
  ‖ ‹Probabilities for rolling a die.› ⦉

  ‖ Suppose we model rolling a die,␣
    <a href='/sheets/uncertain_outcomes.html#Examples_of_modeling'>
      ‖ as before ⦉
    </a>
    , with the sample space $\set{1, 2, 3, 4, 5, 6}$. ⦉

  ‖ We may model each side of the die as equally likely to
    face up. ⦉

  ‖ Thus we want numbers $p(1), p(2), p(3), p(4), p(5), p(6)$
    so that
    ◇ ⦊
      ‖ p(1) = p(2) = p(3) = p(4) = p(5) = p(6) ⦉
    ⦉⦉

  ‖ Consequently, we choose $p(ω) = 1/6$ for each $ω ∈ Ω$. ⦉

  ‖ A Bayesian interpretation is that, prior to the roll, each
    outcome is ‹equally credible›. ⦉

  ‖ A frequentist interpretation is that, after rolling the die
    several (hundreds, say) of times, the ratio of times the die
    lands with six pips facing up to the number of rolls is a
    rational number close to $1/6$—and likewise with the other
    number of pips. ⦉

  ‖ We often speak of this particular model as ❬fair die❭. ⦉

  ‖ As with the fair coin, no outcome is ‹more› or ‹less›
    probable than any other. ⦉
⦉

§§ Other terminology ⦉

‖ Other terminology for probability distribution includes
  ❬distribution❭, ❬probability mass function❭, ❬pmf❭, ❬proportion
  distribution❭, and ❬probabilities❭. ⦉

§§ Simple consequences on the range of a distribution ⦉
<statement type='proposition'>
  ‖ Suppose $p: Ω → 𝗥$ is a distribution. ⦉

  ‖ Then $p(ω) ≤ 1$ for all $ω ∈ Ω$. ⦉
  <proof>
    ‖ Let $ω ∈ Ω$. ⦉

    ‖ We claim
      ◇ ⦊
        ‖ \textstyle ⦉

        ‖ p(ω) ≤ ∑_{t ∈ Ω} p(t) = 1 ⦉
      ⦉⦉

    ‖ This holds because $p(t) ≥ 0$ for every $t ∈ Ω$. ⦉
  </proof>
</statement>
¶ ⦊
  ‖ Consequently, the␣
    <a href='/sheets/relations.html#Domain_and_range'>
      ‖ range ⦉
    </a>
    ␣of $p$ is contained in $[0,1]$. ⦉

  ‖ For this reason, we often introduce a distribution on the
    finite sample space $Ω$ with the notation $p: Ω → [0,1]$,
    to remind ourselves that $\range(p) ⊂ [0,1]$. ⦉
⦉
