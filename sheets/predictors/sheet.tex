%!name:predictors
%!need:lists

\ssection{Why}
We discuss inferring (or learning) functions from examples.

\ssection{Definitions}
A \t{predictor} $f: \CU \to \CV$ is a function from $\CU$ to $\CV$.
An \t{inducer} is a function from finite datasets in $\CU \times \CV$ to predictors from $\CU$ to $\CV$.
A \t{learner} is a function family of inducers, indexed by $n$, each defined for datasets of size $n$.
We call the elements of $\CU$ \t{inputs} and the elements of $\CV$ \t{outputs}.

\ssubsection{Predictors}
An \t{function inducer} is an inducer from datasets functions, in which case we call the elements of $\CU$ \t{inputs} and the elements of $\CV$ \t{outputs}.
We also refer to a function inputs to outputs as a \t{predictor} and call the result of an input under a predictor a \t{prediction}.
Predictors map inputs to ouputs, and (functional) inducers map datasets to predictors.

\ssection{Relation inducers}
We need only consider the case of functional inducers, since we can associate a relation $R$ on $\CU \times \CV$ with a function function $f: \CU \times \CU \to \set{0, 1}$ defined by $f(u, v) = 1$ if $(u, v) \in R$.
Henceforth, by \t{inducer} we mean a \textit{functional} inducer.

\ssubsection{Notation}

Let $D$ be a dataset of size $n$ in $\CU \cross \CV$.
Let $g: \CU \to \CV$, a predictor, which makes prediction $g(u)$ on input $u \in \CU$.
Let $G_n: (\CU \cross \CV)^n \to (\CU \times \CV)$ be an inductor.
Then $G_n(D)$ is the predictor which the inductor associates with dataset $D$.
And $\set{G_n: (\CU \times \CV)^n \to \powerset{(\CU \times \CV)}}_{n \in \N}$ is a family of inductors.

\ssection{Consistent and complete datasets}

Let $D = ((u_i, v_i))_{i = 1}^{n}$ be a dataset and $R \subset X \times Y$ a relation.
$D$ is \t{consistent with $R$} if each $(u_i, v_i) \in R$.
$D$ is \t{consistent} if there exists a relation with which it is consistent.
A dataset is always consistent (take $R = \CU \times \CV$).
$D$ is \t{functionally consistent} if it is consistent with a function; in this case, $x_i = x_j \implies y_i = y_j$.
$D$ is \t{functionally complete} if $\union_i\set{x_i} = X$.
In this case, the dataset includes every element of the relation.

\ssubsection{Other terminology}

Other terms for the inputs include \t{independent variables}, \t{explanatory variables}, \t{precepts}, \t{covariates}, \t{patterns}, \t{instances}, or \t{observations}.
Other terms for the outputs include \t{dependent variables}, \t{explained variables}, \t{postcepts}, \t{targets}, \t{outcomes}, \t{labels} or \t{observational outcomes}.
An input, output pair is sometimes called a \t{record pair}.

Other terms for a functional inductor include \t{learning algorithm}, \t{learner}, \t{supervised learning algorithm}.
Other terms for a predictor include \t{input-output} mapping, \t{prediction rule}, \t{hypothesis}, \t{concept}, or \t{classifier}.
