<!--
!name:predictors
!need:lists
-->

§ Why ⦉
¶ ⦊
  ‖ We discuss inferring (or learning) functions from examples. ⦉
⦉

§ Definitions ⦉
¶ ⦊
  ‖ Suppose $𝒰$ and $𝒱$ are two sets. ⦉

  ‖ A ❬predictor❭ from $𝒰$ to $𝒱$ is a function $f: 𝒰 → 𝒱$. ⦉

  ‖ We call $𝒰$ the ❬inputs❭, $𝒱$ the ❬outputs❭, and $f(u)$ the
    ❬prediction❭ of $f$ on $u ∈ 𝒰$. ⦉
⦉

¶ ⦊
  ‖ An ❬inductor❭ is a function from datasets in $𝒰 × 𝒱$ to
    predictors from $𝒰$ to $𝒱$. ⦉

  ‖ A ❬learner❭ (or ❬learning algorithm❭) is a family of
    inductors whose index set is $𝗡$, and whose ‹$n$th› term is
    is an inductor for a dataset of size $n$. ⦉
⦉

§§ Notation ⦉
¶ ⦊
  ‖ Let $D$ be a dataset of size $n$ in $𝒰 × 𝒱$. ⦉

  ‖ Let $g: 𝒰 → 𝒱$, a predictor, which makes prediction $g(u)$
    on input $u ∈ 𝒰$. ⦉

  ‖ Let $G_n: (𝒰 × 𝒱)^n → (𝒰 × 𝒱)$ be an inductor, so that
    $G_n(D)$ is the predictor which the inductor associates with
    dataset $D$. ⦉

  ‖ Then $\set{G_n: (𝒰 × 𝒱)^n → 𝒱^𝒰}_{n ∈ 𝗡}$ is a learner. ⦉
⦉

§§ Relations ⦉
¶ ⦊
  ‖     <a href='/sheets/functions.html'>
      ‖ Functions ⦉
    </a>
    ␣are␣
    <a href='/sheets/relations.html'>
      ‖ relations ⦉
    </a>
    , so we might ask if ‹inferring› relations may be a more
    general and difficult problem than inferring functions. ⦉

  ‖ The following consideration shows that this is ‹not› the
    case. ⦉
⦉

¶ ⦊
  ‖ A ❬relation inductor❭ is a function from finite datasets in
    $𝒰 × 𝒱$ to ‹relations› on $𝒰 × 𝒱$. ⦉

  ‖ Suppose $R$ is a relation between $𝒰$ and $𝒱$. ⦉

  ‖ Suppose the function $f: 𝒰 × 𝒱 → \set{0, 1}$ is such that
    ◇ ⦊
      ‖ f(u,v) = 1 \quad \text{ if and only if } \quad (u, v)
        ∈ R ⦉
    ⦉⦉

  ‖ Given $f$ we can find $R$, and given $R$ we can find $f$. ⦉

  ‖ Thus, instead of learning the ‹relation› $R$ we can think
    of learning the ‹function› $f$. ⦉

  ‖ In other words, if we have an inductor for $f$, we have a
    ‹relation› inductor for $R$. ⦉
⦉

§ Consistent and complete datasets ⦉
¶ ⦊
  ‖ What can a dataset tell us? ⦉
⦉

¶ ⦊
  ‖ Suppose $D = ((u_i, v_i))_{i = 1}^{n}$ be a dataset and $R
    ⊂ X × Y$ a relation. ⦉

  ‖ $D$ is ❬consistent with❭ $R$ if $(u_i, v_i) ∈ R$ for all
    $i = 1, …, n$. ⦉

  ‖ $D$ is ❬consistent❭ if there exists a relation with which
    it is consistent. ⦉

  ‖ A dataset is always consistent (take $R = 𝒰 × 𝒱$). ⦉

  ‖ $D$ is ❬functionally consistent❭ if it is consistent with a
    function; in this case, $x_i = x_j ⇒ y_i = y_j$. ⦉

  ‖ $D$ is ❬functionally complete❭ if $∪_i\set{x_i} = X$. ⦉

  ‖ In this case, the dataset includes every element of the
    relation. ⦉
⦉

§§ Other terminology ⦉
¶ ⦊
  ‖ Other terms for the inputs include ❬independent variables❭,
    ❬explanatory variables❭, ❬precepts❭, ❬covariates❭, ❬patterns❭,
    ❬instances❭, or ❬observations❭. ⦉

  ‖ Other terms for the outputs include ❬dependent variables❭,
    ❬explained variables❭, ❬postcepts❭, ❬targets❭, ❬outcomes❭,
    ❬labels❭ or ❬observational outcomes❭. ⦉

  ‖ An input-output pair is sometimes called a ❬record pair❭. ⦉
⦉

¶ ⦊
  ‖ Other terms for a learner include ❬supervised learning
    algorithm❭. ⦉

  ‖ Other terms for a predictor include ❬input-output mapping❭,
    ❬prediction rule❭, ❬hypothesis❭, ❬concept❭, and ❬classifier❭. ⦉
⦉
