<!--
!name:predictors
!need:lists
-->

Â§ Why â¦‰
Â¶ â¦Š
  â€– We discuss inferring (or learning) functions from examples. â¦‰
â¦‰

Â§ Definitions â¦‰
Â¶ â¦Š
  â€– Suppose $ğ’°$ and $ğ’±$ are two sets. â¦‰

  â€– A â¬predictorâ­ from $ğ’°$ to $ğ’±$ is a function $f: ğ’° â†’ ğ’±$. â¦‰

  â€– We call $ğ’°$ the â¬inputsâ­, $ğ’±$ the â¬outputsâ­, and $f(u)$ the
    â¬predictionâ­ of $f$ on $u âˆˆ ğ’°$. â¦‰
â¦‰

Â¶ â¦Š
  â€– An â¬inductorâ­ is a function from datasets in $ğ’° Ã— ğ’±$ to
    predictors from $ğ’°$ to $ğ’±$. â¦‰

  â€– A â¬learnerâ­ (or â¬learning algorithmâ­) is a family of
    inductors whose index set is $ğ—¡$, and whose â€¹$n$thâ€º term is
    is an inductor for a dataset of size $n$. â¦‰
â¦‰

Â§Â§ Notation â¦‰
Â¶ â¦Š
  â€– Let $D$ be a dataset of size $n$ in $ğ’° Ã— ğ’±$. â¦‰

  â€– Let $g: ğ’° â†’ ğ’±$, a predictor, which makes prediction $g(u)$
    on input $u âˆˆ ğ’°$. â¦‰

  â€– Let $G_n: (ğ’° Ã— ğ’±)^n â†’ (ğ’° Ã— ğ’±)$ be an inductor, so that
    $G_n(D)$ is the predictor which the inductor associates with
    dataset $D$. â¦‰

  â€– Then $\set{G_n: (ğ’° Ã— ğ’±)^n â†’ ğ’±^ğ’°}_{n âˆˆ ğ—¡}$ is a learner. â¦‰
â¦‰

Â§Â§ Relations â¦‰
Â¶ â¦Š
  â€–     <a href='/sheets/functions.html'>
      â€– Functions â¦‰
    </a>
    â£areâ£
    <a href='/sheets/relations.html'>
      â€– relations â¦‰
    </a>
    , so we might ask if â€¹inferringâ€º relations may be a more
    general and difficult problem than inferring functions. â¦‰

  â€– The following consideration shows that this is â€¹notâ€º the
    case. â¦‰
â¦‰

Â¶ â¦Š
  â€– A â¬relation inductorâ­ is a function from finite datasets in
    $ğ’° Ã— ğ’±$ to â€¹relationsâ€º on $ğ’° Ã— ğ’±$. â¦‰

  â€– Suppose $R$ is a relation between $ğ’°$ and $ğ’±$. â¦‰

  â€– Suppose the function $f: ğ’° Ã— ğ’± â†’ \set{0, 1}$ is such that
    â—‡ â¦Š
      â€– f(u,v) = 1 \quad \text{ if and only if } \quad (u, v)
        âˆˆ R â¦‰
    â¦‰â¦‰

  â€– Given $f$ we can find $R$, and given $R$ we can find $f$. â¦‰

  â€– Thus, instead of learning the â€¹relationâ€º $R$ we can think
    of learning the â€¹functionâ€º $f$. â¦‰

  â€– In other words, if we have an inductor for $f$, we have a
    â€¹relationâ€º inductor for $R$. â¦‰
â¦‰

Â§ Consistent and complete datasets â¦‰
Â¶ â¦Š
  â€– What can a dataset tell us? â¦‰
â¦‰

Â¶ â¦Š
  â€– Suppose $D = ((u_i, v_i))_{i = 1}^{n}$ be a dataset and $R
    âŠ‚ X Ã— Y$ a relation. â¦‰

  â€– $D$ is â¬consistent withâ­ $R$ if $(u_i, v_i) âˆˆ R$ for all
    $i = 1, â€¦, n$. â¦‰

  â€– $D$ is â¬consistentâ­ if there exists a relation with which
    it is consistent. â¦‰

  â€– A dataset is always consistent (take $R = ğ’° Ã— ğ’±$). â¦‰

  â€– $D$ is â¬functionally consistentâ­ if it is consistent with a
    function; in this case, $x_i = x_j â‡’ y_i = y_j$. â¦‰

  â€– $D$ is â¬functionally completeâ­ if $âˆª_i\set{x_i} = X$. â¦‰

  â€– In this case, the dataset includes every element of the
    relation. â¦‰
â¦‰

Â§Â§ Other terminology â¦‰
Â¶ â¦Š
  â€– Other terms for the inputs include â¬independent variablesâ­,
    â¬explanatory variablesâ­, â¬preceptsâ­, â¬covariatesâ­, â¬patternsâ­,
    â¬instancesâ­, or â¬observationsâ­. â¦‰

  â€– Other terms for the outputs include â¬dependent variablesâ­,
    â¬explained variablesâ­, â¬postceptsâ­, â¬targetsâ­, â¬outcomesâ­,
    â¬labelsâ­ or â¬observational outcomesâ­. â¦‰

  â€– An input-output pair is sometimes called a â¬record pairâ­. â¦‰
â¦‰

Â¶ â¦Š
  â€– Other terms for a learner include â¬supervised learning
    algorithmâ­. â¦‰

  â€– Other terms for a predictor include â¬input-output mappingâ­,
    â¬prediction ruleâ­, â¬hypothesisâ­, â¬conceptâ­, and â¬classifierâ­. â¦‰
â¦‰
