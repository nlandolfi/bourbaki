\input{../../sheet.tex}
\sbasic
\input{../dataset/macros.tex}
\input{../direct_products/macros.tex}
\input{../natural_families/macros.tex}
\input{../natural_order/macros.tex}
\input{../families/macros.tex}
\input{../natural_numbers/macros.tex}
\input{../functions/macros.tex}
\input{../set_specification/macros.tex}
\input{../subsets/macros.tex}
\input{../relations/macros.tex}
\input{../set_extension/macros.tex}
\input{../sets/macros.tex}
\input{../ordered_pairs/macros.tex}
\input{../identity/macros.tex}
\input{../objects/macros.tex}
\input{./macros.tex}
\sstart
\stitle{Inductors}

\ssection{Why}

We want to talk about learning
associations between perceptions
in time or space.

\ssection{Definition}

An \ct{inductor}{} is a
function mapping a dataset
of records in a cartesian product
of two sets to a function between
the two sets.
We call the first set the
\ct{precepts}{} and the second
set the
\ct{postcepts}{}.
We call a function from the precepts
to the postcepts a \ct{predictor}{}.
We call the result of a precept
under a predictor a \ct{prediction}{}.
An inductor maps datasets to predictors.

\ssubsection{Notation}

No new notation, just the concepts in
old notation.  Let $A$ and $B$ be two non-empty sets.

Let $n$ be a natural number
and let $r \in (A \cross B)^n$.
Then $r$ is a dataset and $r_1 \in A \cross B$
is a record.

Let $g: A \to B$. Then $g$
is a predictor. For $a \in A$,
$g(a)$ is the prediction of
$g$ on $a$.

Let $f: (A \cross B)^n \to (A \to B)$.
Then $f$ is an inductor.

\strats
