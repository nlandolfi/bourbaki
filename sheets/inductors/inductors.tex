\input{../../sheet.tex}
\sbasic
\input{../datasets/macros.tex}
\input{../direct_products/macros.tex}
\input{../natural_families/macros.tex}
\input{../natural_order/macros.tex}
\input{../families/macros.tex}
\input{../natural_numbers/macros.tex}
\input{../functions/macros.tex}
\input{../set_specification/macros.tex}
\input{../relations/macros.tex}
\input{../sentences/macros.tex}
\input{../set_inclusion/macros.tex}
\input{../ordered_pairs/macros.tex}
\input{../sets/macros.tex}
\input{../set_equality/macros.tex}
\input{../objects/macros.tex}
\input{../identity/macros.tex}
\input{./macros.tex}
\sstart
\stitle{Inductors}

\ssection{Why}

We want to talk about learning
associations between perceptions
in time or space.

\ssection{Definition}

An \t{inductor} is a function mapping a dataset of records in a cartesian product of two sets to a function between the two sets.
We call the first set the \t{precepts} and the second set the \t{postcepts}.
We call a function from the precepts to the postcepts a \t{predictor}.
We call the result of a precept under a predictor a \t{prediction}.
An inductor maps datasets to predictors.

\ssubsection{Notation}

We introduce no new notation, but rather express the new concepts in the old notation. Let $A$ and $B$ be non-empty sets.  Let $D$ be a dataset in $A \cross B$.
Let $g: A \to B$, a predictor, which makes prediction $g(a)$ on precept $a \in A$.
Let $f: (A \cross B)^n \to (A \to B)$, an inductor.
Then $f(D)$ is the predictor which the inductor associates with data $D$.
\strats
