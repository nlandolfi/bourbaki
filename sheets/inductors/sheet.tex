%!name:inductors
%!need:datasets

\ssection{Why}

We want to talk about learning
associations between objects
in time or space.

\ssection{Definition}

Let $A$ and $B$ be sets.
An \t{inductor} is a function mapping a dataset of records in $A \times B$ to a function from $A$ to $B$.
We call the elements of $A$ the \t{precepts} and the elements of $B$ the \t{postcepts}.

We call a function from the precepts to the postcepts a \t{predictor}.
We call the result of a precept under a predictor a \t{prediction}.
An inductor maps datasets to predictors.
A predictor maps precepts to postcepts.

\ssubsection{Notation}

Let $D$ be a dataset of size $n$ in $A \cross B$.
Let $g: A \to B$, a predictor, which makes prediction $g(a)$ on precept $a \in A$.
Let $f: (A \cross B)^n \to (A \to B)$, an inductor.
Then $f(D)$ is the predictor which the inductor associates with dataset $D$.

\ssubsection{Other terminology}

Many authorities call the precepts the \t{independent variables}, \t{inputs}, \t{covariates}, or \t{observations}.
Similarly, some call the postcepts the \t{dependent variables}, \t{outputs}, \t{targets}, or \t{outcomes}.
Some call a predictor an \t{input-output} mapping.

\ssubsection{Supervised learning}

In these sheets, we discuss mathematical objects.
There is, therefore, no notion of time.
Recall that we use only present-tense verbs \say{is} and \say{belongs} (see \sheetref{statements}{Statements}), represented, of course, by $=$ and $\in$.

In spite of this, we acknowledge that many authors discuss \say{problems,} \say{fields,} and \say{areas.}
Many authors refer to application of an inductor to a dataset (that of associating to a dataset a predictor) the task or problem of \t{supervised learning}.
They refer to supervised learning as the problem of constructing an input-output mapping from empirical data.

\blankpage
