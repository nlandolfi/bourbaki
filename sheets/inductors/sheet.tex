%!name:inductors
%!need:datasets

\ssection{Why}
We discuss learning (or inferring) relations from examples.

\ssection{Definition}

Let $X$ and $Y$ be sets.
A \t{relation inductor} (for a dataset of size $n$ in $X \times Y$) is a function mapping a dataset in $(X \times Y)^n$ to a relation between $X$ and $Y$.
We frequently use the term inductor to refer to a family of inductors, indexed by $n \in \N$.

An inductor is \t{functional} if it produces functions.
In this case, we call the elements of $X$ the \t{inputs} and the elements of $Y$ the \t{outputs}.
We call a function from inputs to outputs a \t{predictor} and call the result of an input under a predictor a \t{prediction}.
Using this language, a functional inductor maps datasets to predictors.
A predictor maps inputs to outputs.

To every relation between $X$ and $Y$ corresponds a characteristic function on $X \times Y$ and vice versa.
For this reason, henceforth by \t{inductor} we mean a functional inductor.
A relational inductor on a dataset in $(X \times Y)^n$ can be modeled by a functional inductor on a dataset in $((X \times Y) \times \set{0, 1})^n$.

\ssubsection{Notation}

Let $D$ be a dataset of size $n$ in $X \cross Y$.
Let $g: X \to Y$, a predictor, which makes prediction $g(x)$ on input $x \in X$.
Let $G_n: (X \cross Y)^n \to (X \times Y)$ be an inductor.
Then $G_n(D)$ is the predictor which the inductor associates with dataset $D$.
And $\set{G_n: (X \times Y)^n \to \powerset{(X \times Y)}}_{n \in \N}$ is a family of inductors.

\ssection{Consistent and complete datasets}

Let $D = ((x_i, y_i))_{i =1}^{n}$ be a dataset and $R \subset X \times Y$ a relation.
$D$ is \t{consistent with $R$} if each $(x_i, y_i) \in R$.
$D$ is \t{consistent} if there exists a relation with which it is consistent.
A dataset is always consistent (take $R = X \times Y$).
$D$ is \t{functionally consistent} if it is consistent with a function; in this case, $x_i = x_j \implies y_i = y_j$.
$D$ is \t{functionally complete} if $\union_i\set{x_i} = X$.
In this case, the dataset includes every element of the relation.

\ssubsection{Other terminology}

Other terms for the inputs include \t{independent variables}, \t{explanatory variables}, \t{precepts}, \t{covariates}, \t{patterns}, \t{instances}, or \t{observations}.
Other terms for the outputs include \t{dependent variables}, \t{explained variables}, \t{postcepts}, \t{targets}, \t{outcomes}, \t{labels} or \t{observational outcomes}.

Other terms for a predictor include \t{input-output} mapping, \t{prediction rule}, \t{hypothesis}, \t{concept}, or \t{classifier}.
Since a predictor can be used to \t{guess} the output of an input, some authors call an inductor (or family of inductors) a \t{learner} or \t{learning algorithm} or \t{supervised learning algorithm} and refer to the argument as the \t{training dataset}.
Often the word \say{supervised} is included, as in \t{supervised learning}.
The language intends to indicate that inputs are given along with outputs, and these outputs \say{provide supervision to the algorithm.}
