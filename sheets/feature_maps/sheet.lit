<!--yaml
name: feature_maps
needs:
    - least_squares_linear_regressors
-->

Â§ Why â¦‰
Â¶ â¦Š
  â€– Linear predictors are simple and we know how to select the
    parameters. â¦‰

  â€– The main downside is that there may not be a linear
    relationship between inputs and outputs. â¦‰
â¦‰

Â§ Definition â¦‰
Â¶ â¦Š
  â€– A â¬feature mapâ­ (or â¬regression functionâ­) for outputs $A$
    is a mapping $Ï†: A â†’ ğ—¥^d$. â¦‰

  â€– In this setting, we call $a âˆˆ A$ the â¬raw input recordâ­
    and we call $Ï†(a)$ an â¬embeddingâ­, â¬feature embeddingâ­ or
    â¬feature vectorâ­. â¦‰

  â€– We call the components of a feature vector the â¬featuresâ­. â¦‰

  â€– We call $Ï†(A)$ the â¬regression rangeâ­. â¦‰
â¦‰

Â¶ â¦Š
  â€– A feature map is â¬faithfulâ­ if, whenever records $a_i$ and
    $a_j$ are in some sense â€œsimilarâ€ in the set $A$, the
    embeddings $Ï†(a_i)$ and $Ï†(a_j)$ are close in the vector
    space $ğ—¥^d$. â¦‰
â¦‰

Â¶ â¦Š
  â€– Since it is common for raw input records $a âˆˆ A$ to
    consist of many fields, it is regular to have several
    feature maps $Ï†_i$ which operate component-wise on the fields
    of $a$. â¦‰

  â€– These are sometimes called â¬basis functionsâ­, by analogy with
    real function approximators (seeâ£
    <a href='/sheets/real_function_approximators.html'>
      â€– Real Function Approximators â¦‰
    </a>
    ). â¦‰

  â€– We concatenate these field feature maps and commonly add a
    constant feature $1$. â¦‰

  â€– Since $ğ—¥^d$ is a vector space, it is common to refer to
    it in this case as the â¬feature spaceâ­. â¦‰
â¦‰

Â¶ â¦Š
  â€– Given a dataset $a = (a^1, â€¦, a^n)$ in $A$ and a feature
    map $Ï†: A â†’ ğ—¥^d$, the â¬embedded datasetâ­ of $a$ with
    respect to $Ï†$ is the dataset $(Ï†(a^1), â€¦, Ï†(a^n)$ in $ğ—¥^d$. â¦‰
â¦‰

Â§Â§ Featurized consistency: a route around $X â‰  ğ—¥^d$ â¦‰
Â¶ â¦Š
  â€– Recall that a dataset is parametrically consistent with the
    family $\set{h_{Î¸}: X â†’ Y}_{Î¸}$ if there exists $Î¸^â˜…$ so
    that the dataset is consistent with $Î¸^{â˜…}$. â¦‰

  â€– We saw how to pick $Î¸$ if we use a linear model with a
    squared loss (seeâ£
    <a href='/sheets/least_squares_linear_regressors.html'>
      â€– Least Squares Linear Regressors â¦‰
    </a>
    ). â¦‰
â¦‰

Â¶ â¦Š
  â€– Let $ğ’¢ = \set{g_{Î¸}: ğ—¥^d â†’ ğ—¥}_{Î¸}$. â¦‰

  â€– A dataset is â¬featurized parametrically consistentâ­ with
    respect to the family $ğ’¢$ and the feature map $Ï†: X â†’ ğ—¥^d$
    if it is parametrically consistent with respect to $ğ’¢ âˆ˜ Ï† =
    \Set*{g âˆ˜ Ï†}{g âˆˆ ğ’¢}$. â¦‰
â¦‰

Â¶ â¦Š
  â€– The interpretation is that we have transformed the problem
    of selecting a predictor on an arbitrary space $X$ to the
    problem of selecting a predictor on the space $ğ—¥^d$. â¦‰

  â€– In so doing, we can continue to use simple predictors, such
    as those that are linear and minimize the squared error on
    the dataset.
    â€  â¦Š
      â€– Future editions are likely to modify this section. â¦‰
    â¦‰â¦‰
â¦‰

Â¶ â¦Š
  â€– In other words, we have â€œshifted emphasisâ€ from the model
    function $h: X â†’ ğ—¥$ to the â¬regression functionâ­ from $ğ—¥^d
    â†’ ğ—¥$. â¦‰

  â€– If we know the features and the input $x$, then we know
    the â¬regression vectorâ­ $Ï†(x)$. â¦‰

  â€– The â¬regression rangeâ­ is the set $\Set*{Ï†(x)}{x âˆˆ X}$. â¦‰

  â€– In this case linearity pertains to the parameters $Î¸ âˆˆ ğ—¥^d$
    instead of the inputs (or experimental conditions) $x âˆˆ X$. â¦‰
â¦‰