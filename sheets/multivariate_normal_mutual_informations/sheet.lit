<!--
!name:multivariate_normal_mutual_informations
!need:multivariate_normal_entropy
!need:differential_mutual_information
!need:normal_correlation
-->

§ Why ⦉
¶ ⦊
  ‖ What is the differential mutual information between two
    components of a multivariate normal? ⦉
⦉

§ Result ⦉
<statement type='proposition'>
  ‖ Let $g ∼ \normal{μ}{Σ}$. ⦉

  ‖ Then the mutual information between ⦉

  ‖ component $i$ and component $j$ is
    ◇ ⦊
      ‖ -\frac{1}{2}\ln(1 - ρ_{ij}^2) ⦉
    ⦉
    where $ρ_{ij}$ is the correlation between components $i$ and
    $j$. ⦉
</statement>
<tex>
  ‖ \blankpage ⦉
</tex>
