%!name:relative_entropy
%!need:cross_entropy
%!need:discrete_entropy
%!need:similarity_functions

\ssection{Why}

\ssection{Definition}

Consider two distributions on the same finite set.
The \t{entropy} of the first distribution \t{relative} to the second distribution is the difference of the cross entropy of the first distribution relative to the second and the entropy of the second distribution.
We call it the \t{relative entropy} of the first distribution with the second distribution.
People also call the relative entropy the \t{Kullback-Leibler divergence} or \t{KL divergence}.

\ssubsection{Notation}

Let $A$ be a non-empty finite set.
Let $p: A \to \R$ and $q: A \to \R$ be distributions.
Let $H(q, p)$ denote the cross entropy of $p$ relative to $q$ and let $H(q)$ denote the entropy of $q$.
The entropy of $p$ relative to $q$ is $$H(q, p) - H(p).$$
Herein, we denote the entropy of $p$ relative to $q$ by $d(q, p)$.

\ssection{A similarity function}

The relative entropy is a similarity function between distributions.

\begin{prop}

Let $q$ and $p$ be distributions
on the same set.
Then $d(q, p) \geq 0$ with equality if and only if $p = q$.

\end{prop}

So, $d$ has a few of the properties of a metric.
However, $d$ is not a metric; for example, it is not symmetric.

\begin{prop}
  There exist distributions $p: A \to \R$ and $q: A \to \R$ (with $A$ a non-empty finite set) such that $$d(q, p) \neq d(p, q).$$
\end{prop}

\ssubsection{Optimization Perspective}

If we want to find a distribution $p: A \to \R$ to
\[
  \text{minimize} \quad d(q, p),
\]
then $p = q$ is a solution.
