%!name:relative_entropy
%!need:cross_entropy
%!need:entropy

\ssection{Why}

\ssection{Definition}

Consider two distributions
on the same finite set.
The \ct{relative entropy}{}
of the first distribution
\ct{relative}{} to the
second distribution
is the difference of
the cross entropy of the first
distribution relative to the second
and the entropy of the second distribution.

\ssubsection{Notation}

Let $R$ denote the set of
real numbers. Let
$A$ be a finite set.
Let $p: A \to R$ and
$q: A \to R$ be distributions.
Let $H(q, p)$ denote the
cross entropy of $p$ relative
to $q$ and let $H(q)$ denote
the entropy of $q$.
The relatie entropy of $p$ relative to $q$
is
\[
  H(q, p) - H(p).
\]
