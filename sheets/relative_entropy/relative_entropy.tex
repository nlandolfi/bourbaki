\input{../../sheet.tex}
\sbasic
\input{../cross_entropy/macros.tex}
\input{../entropy/macros.tex}
\input{../logarithm/macros.tex}
\input{../distributions/macros.tex}
\input{../real_numbers/macros.tex}
\input{../summation/macros.tex}
\input{../integer_numbers/macros.tex}
\input{../solving_equations/macros.tex}
\input{../arithmetic/macros.tex}
\input{../equations/macros.tex}
\input{../natural_numbers/macros.tex}
\input{../operations/macros.tex}
\input{../objects/macros.tex}
\input{../functions/macros.tex}
\input{../relations/macros.tex}
\input{../ordered_pairs/macros.tex}
\input{../sets/macros.tex}
\input{./macros.tex}
\sstart
\stitle{Relative Entropy}

\ssection{Why}

\ssection{Definition}

Consider two distributions
on the same finite set.
The \ct{relative entropy}{}
of the first distribution
\ct{relative}{} to the
second distribution
is the difference of
the cross entropy of the first
distribution relative to the second
and the entropy of the second distribution.

\ssubsection{Notation}

Let $R$ denote the set of
real numbers. Let
$A$ be a finite set.
Let $p: A \to R$ and
$q: A \to R$ be distributions.
Let $H(q, p)$ denote the
cross entropy of $p$ relative
to $q$ and let $H(q)$ denote
the entropy of $q$.
The relatie entropy of $p$ relative to $q$
is
\[
  H(q, p) - H(p).
\]
\strats
