\input{../../sheet.tex}
\sbasic
\input{../set_equality/macros.tex}
\input{../sets/macros.tex}
\input{../objects/macros.tex}
\input{../sentences/macros.tex}
\input{../set_inclusion/macros.tex}
\input{../ordered_pairs/macros.tex}
\input{../identity/macros.tex}
\input{../set_specification/macros.tex}
\input{../relations/macros.tex}
\input{../equations/macros.tex}
\input{../families/macros.tex}
\input{../algebras/macros.tex}
\input{../operations/macros.tex}
\input{../functions/macros.tex}
\input{../solving_equations/macros.tex}
\input{../family_operations/macros.tex}
\input{../arithmetic/macros.tex}
\input{../common_sense/macros.tex}
\input{../natural_numbers/macros.tex}
\input{../injective_functions/macros.tex}
\input{../integer_numbers/macros.tex}
\input{../length_common_notions/macros.tex}
\input{../natural_summation/macros.tex}
\input{../probability/macros.tex}
\input{../cardinality/macros.tex}
\input{../rational_numbers/macros.tex}
\input{../distance/macros.tex}
\input{../real_summation/macros.tex}
\input{../probability_outcomes/macros.tex}
\input{../intervals/macros.tex}
\input{../real_numbers/macros.tex}
\input{../metrics/macros.tex}
\input{../probability_distributions/macros.tex}
\input{../logarithm/macros.tex}
\input{../similarity_functions/macros.tex}
\input{../entropy/macros.tex}
\input{../cross_entropy/macros.tex}
\input{./macros.tex}
\sstart
\stitle{Relative Entropy}

\ssection{Why}

\ssection{Definition}

Consider two distributions on the same finite set.
The \t{entropy} of the first distribution \t{relative}{} to the second distribution is the difference of the cross entropy of the first distribution relative to the second and the entropy of the second distribution.
We call it the \t{relative entropy} of the first distribution with the second distribution.
People also call the relative entropy the \t{Kullback-Liebler divergence}.

\ssubsection{Notation}

Let $A$ be a non-empty finite set.
Let $p: A \to \R$ and $q: A \to \R$ be distributions.
Let $H(q, p)$ denote the cross entropy of $p$ relative to $q$ and let $H(q)$ denote the entropy of $q$.
The entropy of $p$ relative to $q$ is $$H(q, p) - H(p).$$
Herein, we denote the entropy of $p$ relative to $q$ by $d(q, p)$.

\ssection{A similarity function}

The relative entropy is a similarity function between distributions.

\begin{prop}

Let $q$ and $p$ be distributions
on the same set.
Then $d(q, p) \geq 0$ with equality if and only if $p = q$.

\end{prop}

So, $d$ has a few of the properties of a metric.
However, $d$ is not a metric; for example, it is not symmetric.

\begin{prop}
  There exist distributions $p: A \to \R$ and $q: A \to \R$ (with $A$ a non-empty finite set) such that $$d(q, p) \neq d(p, q).$$
\end{prop}

\ssubsection{Optimization Perspective}

if we want to find a distribution $p$
to
\[
  \text{minimize} \quad d(q, p)
\]
then $p = q$ is a solution.
\strats
