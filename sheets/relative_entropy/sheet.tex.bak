%!name:relative_entropy
%!need:cross_entropy
%!need:entropy

\ssection{Why}

\ssection{Definition}

Consider two distributions
on the same finite set.
The \ct{relative entropy}{}
of the first distribution
\ct{relative}{} to the
second distribution
is the difference of
the cross entropy of the first
distribution relative to the second
and the entropy of the second distribution.

\ssubsection{Notation}

Let $R$ denote the set of
real numbers. Let
$A$ be a finite set.
Let $p: A \to R$ and
$q: A \to R$ be distributions.
Let $H(q, p)$ denote the
cross entropy of $p$ relative
to $q$ and let $H(q)$ denote
the entropy of $q$.
The relative entropy of
$p$ relative to $q$ is
\[
  H(q, p) - H(p).
\]
We denote the entropy of
$p$ relative to $q$
entropy in this sheet by $d(q, p)$.

\ssection{Distance Perspective}

\begin{prop}
$d(q, p) \geq 0$ for all $q, p$ and $d(q, p) = 0$
if and only if $p = q$.
\end{prop}

\ssection{Assymmetry}

\begin{prop}
  $d(q, p) \neq d(p, q)$
\end{prop}

\ssection{Optimization Perspective}

if we want to find a distribution $p$
to
\[
  \text{minimize} d(q, p)
\]
then $p = q$ is a solution.
