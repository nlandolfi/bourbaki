<!--
!name:relative_entropy
!need:cross_entropy
!need:discrete_entropy
!need:similarity_functions
-->

Â§ Definition â¦‰
Â¶ â¦Š
  â€– Consider two distributions on the same finite set. â¦‰

  â€– The â¬entropyâ­ of the first distribution â¬relativeâ­ to the
    second distribution is the difference of the cross entropy of
    the first distribution relative to the second and the entropy
    of the second distribution. â¦‰

  â€– We call it the â¬relative entropyâ­ of the first distribution
    with the second distribution. â¦‰

  â€– People also call the relative entropy the â¬Kullback-Leibler
    divergenceâ­ or â¬KL divergenceâ­. â¦‰
â¦‰

Â§Â§ Notation â¦‰
Â¶ â¦Š
  â€– Let $A$ be a non-empty finite set. â¦‰

  â€– Let $p: A â†’ ğ—¥$ and $q: A â†’ ğ—¥$ be distributions. â¦‰

  â€– Let $H(q, p)$ denote the cross entropy of $p$ relative to
    $q$ and let $H(q)$ denote the entropy of $q$. â¦‰

  â€– The entropy of $p$ relative to $q$ is $$H(q, p) - H(q).$$ â¦‰

  â€– Herein, we denote the entropy of $p$ relative to $q$ by
    $d(q, p)$. â¦‰
â¦‰

Â§ A similarity function â¦‰
Â¶ â¦Š
  â€– The relative entropy is a similarity function between
    distributions. â¦‰
â¦‰

<statement type='proposition'>
  â€– Let $q$ and $p$ be distributions â¦‰

  â€– on the same set. â¦‰

  â€– Then $d(q, p) â‰¥ 0$ with equality if and only if $p = q$. â¦‰
</statement>
Â¶ â¦Š
  â€– So, $d$ has a few of the properties of a metric. â¦‰

  â€– However, $d$ is not a metric; for example, it is not
    symmetric. â¦‰
â¦‰

<statement type='proposition'>
  â€– There exist distributions $p: A â†’ ğ—¥$ and $q: A â†’ ğ—¥$ (with
    $A$ a non-empty finite set) such that $$d(q, p) â‰  d(p, q).$$ â¦‰
</statement>

Â§Â§ Optimization perspective â¦‰
Â¶ â¦Š
  â€– A solution to finding a distribution $p: A â†’ ğ—¥$ to
    â—‡ â¦Š
      â€– \text{minimize} \quad d(q, p), â¦‰
    â¦‰

    â€– is $p^â˜… = q$. â¦‰â¦‰
â¦‰
