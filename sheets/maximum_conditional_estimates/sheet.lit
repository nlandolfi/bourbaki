<!--yaml
name: maximum_conditional_estimates
needs:
    - random_vectors
    - estimators
    - conditional_densities
-->

§ Why ⦉
¶ ⦊
  ‖ We want to estimate a random vector $x: Ω → 𝗥^d$ from a
    random vector $y: Ω → 𝗥^n$. ⦉
⦉

§ Definition ⦉
¶ ⦊
  ‖ Denote by $g: 𝗥^d ×𝗥^n → 𝗥$ the joint density for $(x,
    y)$.
    † ⦊
      ‖ Future editions will comment on the existence of such a
        density. ⦉
    ⦉⦉

  ‖ Denote the conditional density for $x$ given $y$ by $g_{x
    |y}: 𝗥^d ×𝗥^n → 𝗥$. ⦉

  ‖ In this setting, $g_{x |y}$ is called the ❬posterior
    density❭, $g_{x}$ is called the ❬prior density❭, and $g_{y
    |x}$ is called the ❬likelihood density❭ and $g_{y}$ is called
    the ❬marginal likelihood density❭. ⦉
⦉

¶ ⦊
  ‖ As usual (and assuming $g_{y} > 0$), the posterior is
    related to the likelihood, prior and marginal likelihood by
    ◇ ⦊
      ‖ g_{x |y} ≡ \frac{g_x g_{y |x}}{g_{y}}. ⦉
    ⦉⦉
⦉

¶ ⦊
  ‖ A ❬maximum conditional estimate❭ for $x: Ω → 𝗥^n$ given
    that $y$ has taken the value $γ ∈ 𝗥^n$ is a maximizer $ξ
    ∈ 𝗥^d$ of $g_{x |y}(ξ, γ)$. ⦉

  ‖ It is also called the ❬maximum a posteriori estimate❭ or
    ❬MAP estimate❭. ⦉

  ‖ The maximum conditional estimate is natural, in part, because
    it also maximizes the joint density, since $g(ξ, γ) = g_y(γ)
    g_{x |y}(ξ, γ)$ for all $ξ ∈ 𝗥^d$ and $γ ∈ 𝗥^n$. ⦉
⦉

<tex>
  ‖ \blankpage ⦉
</tex>