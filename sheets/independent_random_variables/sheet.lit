Â¶ â¦Š
  â€– â²%!name:independent_random_variablesâ³ â¦‰

  â€– â²%!need:random_variablesâ³ â¦‰

  â€– â²%!need:independent_sigma_algebrasâ³ â¦‰
â¦‰

Â¶ â¦Š
  â€– \ssection{Why} â¦‰
â¦‰

Â¶ â¦Š
  â€– What does it mean for two random variables to be
    independent? â¦‰

  â€– What are the events associated with a random variable?
    â€  â¦Š
      â€– Future editions will modify this. â¦‰
    â¦‰â¦‰
â¦‰

Â¶ â¦Š
  â€– \ssection{Definition} â¦‰
â¦‰

Â¶ â¦Š
  â€– Two random variables are â¬independentâ­ if the sigma algebras
    generated by the random variables are independent. â¦‰

  â€– In general, a family of random variables are â¬independentâ­
    if the sigma algebras generated by the random variables are
    independent. â¦‰
â¦‰

Â¶ â¦Š
  â€– \ssubsection{Notation} â¦‰
â¦‰

Â¶ â¦Š
  â€– Let $(X, ğ’œ, Î¼)$ be a probability space and $(Y, \SB)$ be
    a measurable space. â¦‰

  â€– Let $f_1,f_2: X â†’ Y$ be random variables. â¦‰

  â€– If the random variables are independent we write $f_1 âŸ‚
    f_2$. â¦‰
â¦‰

Â¶ â¦Š
  â€– \ssubsection{Results} â¦‰
â¦‰

Â¶ â¦Š
  â€– \begin{prop} â¦‰

  â€– Let $f_1, â€¦, f_n$ be independent real-valued random variables
    defined on a probability space $(X, ğ’œ, Î¼)$. â¦‰
â¦‰

Â¶ â¦Š
  â€– Let $B_1, â€¦, B_n$ be Borel sets of real numbers and let
    $A_i = f_i^{-1}(B_i)$. â¦‰

  â€– Let $A = âˆ©_{i = 1}^{n} f_i^{-1}(B_i)$. â¦‰

  â€– Then
    â—‡ â¦Š
      â€– Î¼(A) = âˆ_{i = 1}^{n} Î¼(A_i) â¦‰
    â¦‰â¦‰

  â€– \begin{proof} â¦‰

  â€– Since $f_i$ are independent, so are the sigma algebras they
    generate. â¦‰

  â€– $A_i$ are in each of these sigma algebras, so by definition
    of independence the measure of the intersection is the
    product of the measures. â¦‰

  â€– \end{proof} â¦‰

  â€– \end{prop} â¦‰
â¦‰
