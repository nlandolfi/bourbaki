
%!name:neural_networks
%!need:regressors
%!need:real_matrix-vector_products
%!refs:∕sanjay_lall∕introduction_to_machine_learning∕neural_networks

\section*{Why}
\footnote{Future editions will include. Future editions may change the name of this sheet to \t{computation networks}, or may add a prerequisite sheet on computation graphs.}
\section*{Definition}

A sequence of functions $(g_1, \dots , g_\ell )$ is \t{composable} if $g_i$ is composible with $g_{i-1}$ for $i = 2, \dots , \ell $.
In this case we write $g_\ell  \circ g_{\ell -1} \circ \cdots \circ g_2 \circ g_1$. For example, we write $g_3 \circ g_2 \circ g_1$ for $(g_1, g_2, g_3)$.

A \t{neural network} (or \t{feedforward neural network}) from $\R ^n$ to $\R ^m$ is a sequence of composable functions $(g_1, \dots , g_{\ell })$, $\dom g_1 = \R ^n$, $\ran g_\ell  \subset \R ^m$, satisfying
\[
g_i(\xi ) = h_i(A_i \xi  + b_i)
\]
for some conforming matrices $A_i$, vectors $b_i$ and functions $h_i$.

The $i$th \t{layer} of the neural network is the $i$th function $g_i$.
The $i$th \t{activation} of the neural network is the function $h_i$.
A \t{neural network} is called \t{deep} if its number of layers is larger than 3.

We call the composition of the layers of the neural network the \t{network predictor} (or just \t{predictor}).
We also call it \t{the function} of the network.\footnote{Many authorities refer to a neural network as a function. Strictly speaking that is true for us, as well, since a sequence is a function. But the meaning of the common use is in reference to the \t{network predictor}.}

A \t{multi-layer perceptron} (\t{MLP}) is a neural network with 2 layers (1 \t{hidden layer}) and for which $A_i$ and $b_i$ have unrestricted nonzero entries.

%% \ssubsection{Notation}
%%
%% Let $g^1: \R^d \to \R^k$, $g^2: \R^k \to \R^k$, $g^3: \R^k \to \R^m$.
%% Then $(g^1, g^2, g^3)$ is a neural network.
%% Notice that the codomain of $g^1$ ($\R^k$
%%
%% \ssubsection{Activation functions}
%%
%% An activation function $h: \R \to \R$ is nonlinear.

\blankpage