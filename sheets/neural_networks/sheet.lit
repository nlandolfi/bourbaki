<!--yaml
name: neural_networks
needs:
    - regressors
    - real_matrix-vector_products
refs:
    - ∕sanjay_lall∕introduction_to_machine_learning∕neural_networks
-->

§ Why ⦉
† ⦊
  ‖ Future editions will include. Future editions may change the
    name of this sheet to ❬computation networks❭, or may add a
    prerequisite sheet on computation graphs. ⦉
⦉

§ Definition ⦉
¶ ⦊
  ‖ A sequence of functions $(g_1, …, g_ℓ)$ is ❬composable❭ if
    $g_i$ is composable with $g_{i-1}$ for $i = 2, …, ℓ$. ⦉

  ‖ In this case we write $g_ℓ ∘ g_{ℓ-1} ∘ ⋯ ∘ g_2 ∘ g_1$.
    For example, we write $g_3 ∘ g_2 ∘ g_1$ for $(g_1, g_2,
    g_3)$. ⦉
⦉

¶ ⦊
  ‖ A ❬neural network❭ (or ❬feedforward neural network❭) from
    $𝗥^n$ to $𝗥^m$ is a sequence of composable functions $(g_1,
    …, g_{ℓ})$, $\dom g_1 = 𝗥^n$, $\ran g_ℓ ⊂ 𝗥^m$, satisfying
    ◇ ⦊
      ‖ g_i(ξ) = h_i(A_i ξ + b_i) ⦉
    ⦉
    for some conforming matrices $A_i$, vectors $b_i$ and
    functions $h_i$. ⦉
⦉

¶ ⦊
  ‖ The $i$th ❬layer❭ of the neural network is the $i$th
    function $g_i$. ⦉

  ‖ The $i$th ❬activation❭ of the neural network is the function
    $h_i$. ⦉

  ‖ A ❬neural network❭ is called ❬deep❭ if its number of layers
    is larger than 3. ⦉
⦉

¶ ⦊
  ‖ We call the composition of the layers of the neural network
    the ❬network predictor❭ (or just ❬predictor❭). ⦉

  ‖ We also call it ❬the function❭ of the network.
    † ⦊
      ‖ Many authorities refer to a neural network as a
        function. Strictly speaking that is true for us, as well,
        since a sequence is a function. But the meaning of the
        common use is in reference to the ❬network predictor❭. ⦉
    ⦉⦉
⦉

¶ ⦊
  ‖ A ❬multi-layer perceptron❭ (❬MLP❭) is a neural network with
    2 layers (1 ❬hidden layer❭) and for which $A_i$ and $b_i$
    have unrestricted nonzero entries. ⦉
⦉

<!--
% \ssubsection{Notation}
%
% Let $g^1: \R^d \to \R^k$, $g^2: \R^k \to \R^k$, $g^3: \R^k \to \R^m$.
% Then $(g^1, g^2, g^3)$ is a neural network.
% Notice that the codomain of $g^1$ ($\R^k$
%
% \ssubsection{Activation functions}
%
% An activation function $h: \R \to \R$ is nonlinear.
-->
<tex>
  ‖ \blankpage ⦉
</tex>
