<!--yaml
name: neural_networks
needs:
    - regressors
    - real_matrix-vector_products
refs:
    - âˆ•sanjay_lallâˆ•introduction_to_machine_learningâˆ•neural_networks
-->

Â§ Why â¦‰
â€  â¦Š
  â€– Future editions will include. Future editions may change the
    name of this sheet to â¬computation networksâ­, or may add a
    prerequisite sheet on computation graphs. â¦‰
â¦‰

Â§ Definition â¦‰
Â¶ â¦Š
  â€– A sequence of functions $(g_1, â€¦, g_â„“)$ is â¬composableâ­ if
    $g_i$ is composable with $g_{i-1}$ for $i = 2, â€¦, â„“$. â¦‰

  â€– In this case we write $g_â„“ âˆ˜ g_{â„“-1} âˆ˜ â‹¯ âˆ˜ g_2 âˆ˜ g_1$.
    For example, we write $g_3 âˆ˜ g_2 âˆ˜ g_1$ for $(g_1, g_2,
    g_3)$. â¦‰
â¦‰

Â¶ â¦Š
  â€– A â¬neural networkâ­ (or â¬feedforward neural networkâ­) from
    $ğ—¥^n$ to $ğ—¥^m$ is a sequence of composable functions $(g_1,
    â€¦, g_{â„“})$, $\dom g_1 = ğ—¥^n$, $\ran g_â„“ âŠ‚ ğ—¥^m$, satisfying
    â—‡ â¦Š
      â€– g_i(Î¾) = h_i(A_i Î¾ + b_i) â¦‰
    â¦‰
    for some conforming matrices $A_i$, vectors $b_i$ and
    functions $h_i$. â¦‰
â¦‰

Â¶ â¦Š
  â€– The $i$th â¬layerâ­ of the neural network is the $i$th
    function $g_i$. â¦‰

  â€– The $i$th â¬activationâ­ of the neural network is the function
    $h_i$. â¦‰

  â€– A â¬neural networkâ­ is called â¬deepâ­ if its number of layers
    is larger than 3. â¦‰
â¦‰

Â¶ â¦Š
  â€– We call the composition of the layers of the neural network
    the â¬network predictorâ­ (or just â¬predictorâ­). â¦‰

  â€– We also call it â¬the functionâ­ of the network.
    â€  â¦Š
      â€– Many authorities refer to a neural network as a
        function. Strictly speaking that is true for us, as well,
        since a sequence is a function. But the meaning of the
        common use is in reference to the â¬network predictorâ­. â¦‰
    â¦‰â¦‰
â¦‰

Â¶ â¦Š
  â€– A â¬multi-layer perceptronâ­ (â¬MLPâ­) is a neural network with
    2 layers (1 â¬hidden layerâ­) and for which $A_i$ and $b_i$
    have unrestricted nonzero entries. â¦‰
â¦‰

<!--
% \ssubsection{Notation}
%
% Let $g^1: \R^d \to \R^k$, $g^2: \R^k \to \R^k$, $g^3: \R^k \to \R^m$.
% Then $(g^1, g^2, g^3)$ is a neural network.
% Notice that the codomain of $g^1$ ($\R^k$
%
% \ssubsection{Activation functions}
%
% An activation function $h: \R \to \R$ is nonlinear.
-->
<tex>
  â€– \blankpage â¦‰
</tex>
