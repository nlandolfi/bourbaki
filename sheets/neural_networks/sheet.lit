¶ ⦊
  ‖ ❲%!name:neural_networks❳ ⦉

  ‖ ❲%!need:regressors❳ ⦉

  ‖ ❲% %!need:inductors❳ ⦉

  ‖ ❲%!need:real_matrix-vector_products❳ ⦉

  ‖ ❲%!refs:∕sanjay_lall∕introduction_to_machine_learning∕neural_networks❳ ⦉
⦉

¶ ⦊
  ‖ \ssection{Why}
    † ⦊
      ‖ Future editions will include. Future editions may change
        the name of this sheet to ❬computation networks❭, or may
        add prerequisite sheet on computation graphs. ⦉
    ⦉⦉
⦉

¶ ⦊
  ‖ \ssection{Definition} ⦉
⦉

¶ ⦊
  ‖ A sequence of functions $(g_1, …, g_ℓ)$ is ❬composable❭ if
    $g_i$ is composible with $g_{i-1}$ for $i = 2, …, ℓ$. ⦉

  ‖ In this case we write $g_ℓ \composed g_{ℓ-1} \composed ⋯
    \composed g_2 \composed g_1$. For example, we write $g_3
    \composed g_2 \composed g_1$ for $(g_1, g_2, g_3)$. ⦉
⦉

¶ ⦊
  ‖ A ❬neural network❭ (or ❬feedforward neural network❭) from
    $𝗥^n$ to $𝗥^m$ is a sequence of composable functions $(g_1,
    …, g_{ℓ})$, $\dom g_1 = 𝗥^n$, $\ran g_ℓ ⊂ 𝗥^m$, satisfying
    ◇ ⦊
      ‖ g_i(ξ) = h_i(A_i ξ + b_i) ⦉
    ⦉
    for some conforming matrices $A_i$, vectors $b_i$ and
    functions $h_i$. ⦉
⦉

¶ ⦊
  ‖ The $i$th ❬layer❭ of the neural network is the $i$th
    function $g_i$. ⦉

  ‖ The $i$th ❬activation❭ of the neural network is the function
    $h_i$. ⦉

  ‖ A ❬neural network❭ is called ❬deep❭ if its number of layers
    is larger than 3. ⦉
⦉

¶ ⦊
  ‖ We call the composition of the layers of the neural network
    the ❬network predictor❭ (or just ❬predictor❭). ⦉

  ‖ We also call it ❬the function❭ of the network.
    † ⦊
      ‖ Many authorities refer to a neural network as a
        function. Strictly speaking that is true for us, as well,
        since a sequence is a function. But the meaning of the
        common use is in reference to the ❬network predictor❭. ⦉
    ⦉⦉
⦉

¶ ⦊
  ‖ A ❬multi-layer perceptron❭ (❬MLP❭) is a neural network with
    2 layers (1 ❬hidden layer❭) and for which $A_i$ and $b_i$
    have unrestricted nonzero entries. ⦉
⦉

¶ ⦊
  ‖ ⦉

  ‖ ❲% \ssubsection{Notation}❳ ⦉

  ‖ ❲%❳ ⦉

  ‖ ❲% Let $g^1: 𝗥^d → 𝗥^k$, $g^2: 𝗥^k → 𝗥^k$, $g^3: 𝗥^k → 𝗥^m$.❳ ⦉

  ‖ ❲% Then $(g^1, g^2, g^3)$ is a neural network.❳ ⦉

  ‖ ❲% Notice that the codomain of $g^1$ ($𝗥^k$❳ ⦉

  ‖ ❲%❳ ⦉

  ‖ ❲% \ssubsection{Activation functions}❳ ⦉

  ‖ ❲%❳ ⦉

  ‖ ❲% An activation function $h: 𝗥 → 𝗥$ is nonlinear.❳ ⦉
⦉

¶ ⦊
  ‖ \blankpage ⦉
⦉
