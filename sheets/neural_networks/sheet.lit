Â¶ â¦Š
  â€– â²%!name:neural_networksâ³ â¦‰

  â€– â²%!need:regressorsâ³ â¦‰

  â€– â²% %!need:inductorsâ³ â¦‰

  â€– â²%!need:real_matrix-vector_productsâ³ â¦‰

  â€– â²%!refs:âˆ•sanjay_lallâˆ•introduction_to_machine_learningâˆ•neural_networksâ³ â¦‰
â¦‰

Â¶ â¦Š
  â€– \ssection{Why}
    â€  â¦Š
      â€– Future editions will include. Future editions may change
        the name of this sheet to â¬computation networksâ­, or may
        add prerequisite sheet on computation graphs. â¦‰
    â¦‰â¦‰
â¦‰

Â¶ â¦Š
  â€– \ssection{Definition} â¦‰
â¦‰

Â¶ â¦Š
  â€– A sequence of functions $(g_1, â€¦, g_â„“)$ is â¬composableâ­ if
    $g_i$ is composible with $g_{i-1}$ for $i = 2, â€¦, â„“$. â¦‰

  â€– In this case we write $g_â„“ \composed g_{â„“-1} \composed â‹¯
    \composed g_2 \composed g_1$. For example, we write $g_3
    \composed g_2 \composed g_1$ for $(g_1, g_2, g_3)$. â¦‰
â¦‰

Â¶ â¦Š
  â€– A â¬neural networkâ­ (or â¬feedforward neural networkâ­) from
    $ğ—¥^n$ to $ğ—¥^m$ is a sequence of composable functions $(g_1,
    â€¦, g_{â„“})$, $\dom g_1 = ğ—¥^n$, $\ran g_â„“ âŠ‚ ğ—¥^m$, satisfying
    â—‡ â¦Š
      â€– g_i(Î¾) = h_i(A_i Î¾ + b_i) â¦‰
    â¦‰
    for some conforming matrices $A_i$, vectors $b_i$ and
    functions $h_i$. â¦‰
â¦‰

Â¶ â¦Š
  â€– The $i$th â¬layerâ­ of the neural network is the $i$th
    function $g_i$. â¦‰

  â€– The $i$th â¬activationâ­ of the neural network is the function
    $h_i$. â¦‰

  â€– A â¬neural networkâ­ is called â¬deepâ­ if its number of layers
    is larger than 3. â¦‰
â¦‰

Â¶ â¦Š
  â€– We call the composition of the layers of the neural network
    the â¬network predictorâ­ (or just â¬predictorâ­). â¦‰

  â€– We also call it â¬the functionâ­ of the network.
    â€  â¦Š
      â€– Many authorities refer to a neural network as a
        function. Strictly speaking that is true for us, as well,
        since a sequence is a function. But the meaning of the
        common use is in reference to the â¬network predictorâ­. â¦‰
    â¦‰â¦‰
â¦‰

Â¶ â¦Š
  â€– A â¬multi-layer perceptronâ­ (â¬MLPâ­) is a neural network with
    2 layers (1 â¬hidden layerâ­) and for which $A_i$ and $b_i$
    have unrestricted nonzero entries. â¦‰
â¦‰

Â¶ â¦Š
  â€– â¦‰

  â€– â²% \ssubsection{Notation}â³ â¦‰

  â€– â²%â³ â¦‰

  â€– â²% Let $g^1: ğ—¥^d â†’ ğ—¥^k$, $g^2: ğ—¥^k â†’ ğ—¥^k$, $g^3: ğ—¥^k â†’ ğ—¥^m$.â³ â¦‰

  â€– â²% Then $(g^1, g^2, g^3)$ is a neural network.â³ â¦‰

  â€– â²% Notice that the codomain of $g^1$ ($ğ—¥^k$â³ â¦‰

  â€– â²%â³ â¦‰

  â€– â²% \ssubsection{Activation functions}â³ â¦‰

  â€– â²%â³ â¦‰

  â€– â²% An activation function $h: ğ—¥ â†’ ğ—¥$ is nonlinear.â³ â¦‰
â¦‰

Â¶ â¦Š
  â€– \blankpage â¦‰
â¦‰
