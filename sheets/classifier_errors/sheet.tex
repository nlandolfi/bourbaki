
\section*{Why}

How should we compare two classifiers?

\section*{Definition}

Let $u_1, \dots , u_n$ in $\mathcal{U} $ be a dataset of inputs and $v_1, \dots , u_n$ in $\mathcal{V} $ be a dataset of labels.
Let $G: \mathcal{U}  \to \mathcal{V} $ be a classifier.

For each $i = 1, \dots , n$, the classifier associates a prediction $G(u_i)$ with input $u_i$.
The prediction $G(u_i)$ is \t{correct} if $G(u_i) = b_i$ and \t{incorrect} (\t{wrong}, an \t{error}) if $G(a_i) \neq b_i$.
The \t{error rate} is the proportion of the dataset for which the classifier's prediction is an incorrect.
In other words, the error rate is $(1/n)\num{\Set{i}{G(a_i) \neq b_i}}$.

Given two classifiers, it is natural to prefer the one with the lower error rate.
If these two classifiers are the results of different inductors applied to the same dataset, we can use a separate dataset to \t{validate} these.

\section*{Boolean case}

In the case that $B = \set{-1, 1}$, we call the class $-1$ \t{negative} and the class $+1$ \t{positive}.
Similarly, we call an example $(u_i, v_i)$ a \t{negative example} if $b_i = -1$ and a \t{positive example} if $b_i = 1$.

We call the result of $u_i$ under classifier $G$ a \t{true positive} if $v_i = 1$ and $G(u_i) = 1$, a \t{true negative} if $v_i = -1$ and $G(v_i) = -1$.
In these two cases, the classifier has predicted the label correctly.

We call the result of $u_i$ under classifier $G$ a \t{false positive} (or \t{type I error}, read ``type one error'') if $v_i = -1$ and $G(u_i) = 1$ and a \t{false negative} (or \t{type II error}, read ``type two error'') if $v_i = 1$ and $G(u_i) = -1$.
In these two cases the classifier has predicted the label incorrectly.
Notice that the words positive and negative reference the classifier's prediction, not the true label.

The \t{false positive rate} (\t{false negative rate}) of a classifier on a dataset is the proportion of dataset elements for which it predicts a \t{false positive} (\t{false negative}).
The \t{true positive rate} (or \t{sensitivity}, \t{recall}) of $G$ on a dataset is the proportion of positive examples which $G$ labels positive.

The \t{precision} (or \t{positive predictive value}) of $G$ is the proportion of all examples which the classifier labels $positive$ whose label is positive.
The \t{true negative rate} (or \t{specificity}, \t{selectivity}) is the proportion of negative examples which $G$ labels negative.
The \t{false alarm rate} is the proportion of negative examples which $G$ \textit{incorrectly} labels positive.

\blankpage