%!name:mutual_information
%!need:relative_entropy
%!need:marginal_distributions

\ssection{Why}

\ssection{Definition}

The mutual information of a joint distribution over two random variables is the entropy of the product of the marginal distributions relative to the joint distribution.

\ssubsection{Notation}

Let $A$ and $B$ be two non-empty sets.
Let $p: A \times B \to \R$ be a distribution.

\blankpage
