<!--yaml
name: mutual_information
needs:
    - relative_entropy
    - marginal_distributions
-->

§ Definition ⦉
¶ ⦊
  ‖ The mutual information of a joint distribution over two
    random variables is the entropy of the product of the
    marginal distributions relative to the joint distribution. ⦉
⦉

§§ Notation ⦉
¶ ⦊
  ‖ Let $A$ and $B$ be two non-empty sets. ⦉

  ‖ Let $p_{12}: A × B → 𝗥$ be a distribution with marginal
    distributions $p_1: A → 𝗥$ and $p_2: B → 𝗥$. ⦉

  ‖ The mutual information of $p$ is $d(p, p_1p_2)$ where $d$
    denotes the relative entropy. ⦉
⦉

<tex>
  ‖ \blankpage ⦉
</tex>