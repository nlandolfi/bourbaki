<!--yaml
name: mutual_information
needs:
    - relative_entropy
    - marginal_distributions
-->

Â§ Definition â¦‰
Â¶ â¦Š
  â€– The mutual information of a joint distribution over two
    random variables is the entropy of the product of the
    marginal distributions relative to the joint distribution. â¦‰
â¦‰

Â§Â§ Notation â¦‰
Â¶ â¦Š
  â€– Let $A$ and $B$ be two non-empty sets. â¦‰

  â€– Let $p_{12}: A Ã— B â†’ ğ—¥$ be a distribution with marginal
    distributions $p_1: A â†’ ğ—¥$ and $p_2: B â†’ ğ—¥$. â¦‰

  â€– The mutual information of $p$ is $d(p, p_1p_2)$ where $d$
    denotes the relative entropy. â¦‰
â¦‰

<tex>
  â€– \blankpage â¦‰
</tex>