%!name:featurized_probabilistic_linear_models
%!need:normal_linear_model_regressors
%!need:feature_maps

\ssection{Why}

It is natural to embed a dataset.

\ssection{Definition}

Let $(x: \Omega \to \R^d, A \in \R^{n \times d}, e: \Omega \to \R^n)$ be a probabilistic linear model over the probability space $(\Omega, \CA, \PM)$.
Let $\phi: \R^d \to \R^{d'}$ be a feature map.

We call the sequence $(x, A, e, \phi)$ a \t{featurized probabilistic linear model} (also \t{embedded probabilistic linear model}).
We interpret the model as a random field $h: \Omega \to (\R^d \to \R)$ which is a linear function of the features
\[
  h_{\omega}(a) = \transpose{\phi(a)}x(\omega).
\]

Denote the data matrix of the embedded feature vectors by $\phi(A)$.
In other words, $\phi(A) \in \R^{n \times d'}$ is a matrix whose rows are feature vectors.
Then $(x, A, e, \phi)$ corresponds to the probabilistic linear model $(x, \phi(A), e)$.

\ssection{Normal case}

In the normal (Gaussian) case, the parameter posterior $g_{x \mid y}(\cdot, \gamma)$ is a normal density with mean
\[
  \Sigma_{x}\transpose{\phi(A)}\invp{\phi(A)\Sigma_{x}\transpose{\phi(A)} + \Sigma_{e}} \gamma
\]
and covariance
\[
  \invp{\inv{\Sigma_{x}} + \transpose{\phi(A)}\inv{\Sigma_{e}}\phi(A)}.
\]

The predictive density for $a \in \R^d$ is normal with mean
\[
  \transpose{\phi(a)}\Sigma_{x}\transpose{\phi(A)}\invp{\phi(A)\Sigma_{x}\transpose{\phi(A)} + \Sigma_{e}}\gamma.
\]
and covariance
\[
  \transpose{\phi_a}\Sigma_{x}\phi_a - \transpose{\phi_a}\Sigma_{x}\transpose{\phi(A)}\invp{\phi(A)\Sigma_{x}\transpose{\phi(A)} + \Sigma_e}\phi(A)\Sigma_{x}\phi_a.
\]

% \blankpage
