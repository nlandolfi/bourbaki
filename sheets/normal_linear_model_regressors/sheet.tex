%!name:normal_linear_model_regressors
%!need:normal_linear_model
%!need:regressors
%!need:interpolators

\ssection{Why}

We use a normal linear model to predict the function at inputs not included in the design.

\ssection{Definition}

Let $(x: \Omega \to \R^d, A \in \R^{n \times d}, e: \Omega \to \R^n)$ be a normal linear model over the probability space $(\Omega, \CA, \PM)$.


\ssubsection{Predictive density}
We are modeling $h_\omega: \R^d \to \R$ by $h_w(a) = \transpose{x(\omega)}a$.
The \t{predictive density} for a dataset $c^1, \dots, c^m \in \R^d$ is the conditional density of the random vector $(h_{(\cdot)}(c^1), \dots, h_{(\cdot)}(c^m))$ given $y$.


\begin{proposition}
  The predictive density for $c^1, \dots, c^m \in \R^d$ (with data matrix $C \in \R^{m \times d}$) is normal with mean
  \[
    g(a) = (C\Sigma_{x}\transpose{A})\invp{A\Sigma_{x}\transpose{A} + \Sigma_e}\gamma.
  \]
  and covariance
  \[
   % \Sigma_{zz} - \Sigma_{zy}\Sigma_{yy}^{-1}\Sigma_{yz} =
    C\Sigma_{x}\transpose{C} - C\Sigma_{x}\transpose{A}\invp{A\Sigma_{x}\transpose{A} + \Sigma_e}A\Sigma_{x}\transpose{C}.
  \]
  \begin{proof}
    Define (as usual) $y: \Omega \to \R^n$ and $z : \Omega \to \R^m$ by
  \[
  \begin{aligned}
    y &= Ax + e \\
    z &= Cx.
  \end{aligned}
  \]
    Recognize $(x, y, z)$ as jointly normal, and use \sheetref{normal_conditional}{Normal Conditionals}).
  \end{proof}
\end{proposition}

\ssubsection{Predictor}

The \t{normal linear model predictor} or \t{normal linear model regressor} for the normal linear model $(x, A, e)$ is the predictor which assigns to a new point $a \in \R^d$ the mean of the predictive density at $a$.
That is, the predictor $g: \R^d \to \R$ defined by
\[
  g(a) = \transpose{a}\Sigma_{x}\transpose{A}\invp{A\Sigma_{x}\transpose{A} + \Sigma_e}\gamma.
\]
In the above we have substituted $\transpose{a}$ for $C$.
In the case of normal random vectors this corresponds with the MAP esetimate and the MMSE estimate.\footnote{Future editions will have discussed this and include a reference to the sheet.}

Use of a normal linear model predictor is often referred to as \t{Gaussian process regression}.
The upside is that a gaussian process predicor interpolates the data, is smooth, and the so-called variance increases with the distance from the data.
This is also called \t{Bayesian linear regression}.

% \blankpage
