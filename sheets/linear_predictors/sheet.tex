
\section*{Why}

A simple class of predictors when the input and output sets are vector spaces is the class of linear predictors.

%% A simplification of finding a predictor from $\R^d$ to $\R$ is to only consider linear predictors.
%% The linear functions on $\R^d$ are in one-to-one correspondence with $\R^d$.
%% So these predictors are \say{simple} (and perhaps \say{interpretable}) and may also be flexible.
%%
%% In other words, if we suppose a dataset is parametrically consistent linear functions parameterized by $R^d$.
%%
%% Let $X = \R$ and $Y = \R$.
%% A
%% A real-valued function
%% Let $X = \R^d$ be a set of inputs and $Y = \R$ a set of outputs.
%% %For example, let $(x^1, y^1), \dots, (x^{10}, y^{10})$ be a dataset in which we have recorded the number of rainy days, and the number of sunshine days, and the crop yield of a field.
%% Such a model is simple to implement and interpretable, at the cost of flexibility.
%%

\section*{Definition}

A \t{linear predictor} (or \t{linear model} or \t{deterministic linear model}) is a predictor which is a linear function of its inputs.

Such a model is simple to implement and interpretable, at the cost of flexibility.

\section*{$\R ^d$ Example}

Let $X = \R ^d$ be a set of inputs and $Y = \R $ a set of outputs.
The linear functions on $\R ^d$ are in one-to-one correspondence with vectors in $\R ^d$.

A linear function $f: \R ^d \to \R $ over the vector space $(\R ^d, \R )$ has a set of parameters $w \in \R ^d$ so that
\[
f(x) = \sum_{i} w_i x_i = w^\top  x.
\]
The parameters of a linear predictor on $\R ^d$ are often called \t{weights}.

\blankpage