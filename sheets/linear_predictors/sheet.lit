<!--yaml
name: linear_predictors
needs:
    - input_designs
    - linear_transformations
    - matrix_transpose
    - real_function_approximators
refs:
    - pukelsheim2006optimal/01_experimental_designs_in_linear_models
-->

§ Why ⦉
¶ ⦊
  ‖ A simple class of predictors when the input and output sets
    are vector spaces is the class of linear predictors. ⦉
⦉

<!--
% A simplification of finding a predictor from $\R^d$ to $\R$ is to only consider linear predictors.
% The linear functions on $\R^d$ are in one-to-one correspondence with $\R^d$.
% So these predictors are \say{simple} (and perhaps \say{interpretable}) and may also be flexible.
%
% In other words, if we suppose a dataset is parametrically consistent linear functions parameterized by $R^d$.
%
% Let $X = \R$ and $Y = \R$.
% A
% A real-valued function
% Let $X = \R^d$ be a set of inputs and $Y = \R$ a set of outputs.
% %For example, let $(x^1, y^1), \dots, (x^{10}, y^{10})$ be a dataset in which we have recorded the number of rainy days, and the number of sunshine days, and the crop yield of a field.
% Such a model is simple to implement and interpretable, at the cost of flexibility.
%
-->

§ Definition ⦉
¶ ⦊
  ‖ A ❬linear predictor❭ (or ❬linear model❭ or ❬deterministic
    linear model❭) is a predictor which is a linear function of
    its inputs. ⦉
⦉

¶ ⦊
  ‖ Such a model is simple to implement and interpretable, at
    the cost of flexibility. ⦉
⦉

§ $𝗥^d$ Example ⦉
¶ ⦊
  ‖ Let $X = 𝗥^d$ be a set of inputs and $Y = 𝗥$ a set of
    outputs. ⦉

  ‖ The linear functions on $𝗥^d$ are in one-to-one
    correspondence with vectors in $𝗥^d$. ⦉
⦉

¶ ⦊
  ‖ A linear function $f: 𝗥^d → 𝗥$ over the vector space
    $(𝗥^d, 𝗥)$ has a set of parameters $w ∈ 𝗥^d$ so that
    ◇ ⦊
      ‖ f(x) = ∑_{i} w_i x_i = w^⊤ x. ⦉
    ⦉

    ‖ The parameters of a linear predictor on $𝗥^d$ are often
      called ❬weights❭. ⦉⦉
⦉

<tex>
  ‖ \blankpage ⦉
</tex>