<!--yaml
name: linear_predictors
needs:
    - input_designs
    - linear_transformations
    - matrix_transpose
    - real_function_approximators
refs:
    - pukelsheim2006optimal/01_experimental_designs_in_linear_models
-->

Â§ Why â¦‰
Â¶ â¦Š
  â€– A simple class of predictors when the input and output sets
    are vector spaces is the class of linear predictors. â¦‰
â¦‰

<!--
% A simplification of finding a predictor from $\R^d$ to $\R$ is to only consider linear predictors.
% The linear functions on $\R^d$ are in one-to-one correspondence with $\R^d$.
% So these predictors are \say{simple} (and perhaps \say{interpretable}) and may also be flexible.
%
% In other words, if we suppose a dataset is parametrically consistent linear functions parameterized by $R^d$.
%
% Let $X = \R$ and $Y = \R$.
% A
% A real-valued function
% Let $X = \R^d$ be a set of inputs and $Y = \R$ a set of outputs.
% %For example, let $(x^1, y^1), \dots, (x^{10}, y^{10})$ be a dataset in which we have recorded the number of rainy days, and the number of sunshine days, and the crop yield of a field.
% Such a model is simple to implement and interpretable, at the cost of flexibility.
%
-->

Â§ Definition â¦‰
Â¶ â¦Š
  â€– A â¬linear predictorâ­ (or â¬linear modelâ­ or â¬deterministic
    linear modelâ­) is a predictor which is a linear function of
    its inputs. â¦‰
â¦‰

Â¶ â¦Š
  â€– Such a model is simple to implement and interpretable, at
    the cost of flexibility. â¦‰
â¦‰

Â§ $ğ—¥^d$ Example â¦‰
Â¶ â¦Š
  â€– Let $X = ğ—¥^d$ be a set of inputs and $Y = ğ—¥$ a set of
    outputs. â¦‰

  â€– The linear functions on $ğ—¥^d$ are in one-to-one
    correspondence with vectors in $ğ—¥^d$. â¦‰
â¦‰

Â¶ â¦Š
  â€– A linear function $f: ğ—¥^d â†’ ğ—¥$ over the vector space
    $(ğ—¥^d, ğ—¥)$ has a set of parameters $w âˆˆ ğ—¥^d$ so that
    â—‡ â¦Š
      â€– f(x) = âˆ‘_{i} w_i x_i = w^âŠ¤ x. â¦‰
    â¦‰

    â€– The parameters of a linear predictor on $ğ—¥^d$ are often
      called â¬weightsâ­. â¦‰â¦‰
â¦‰

<tex>
  â€– \blankpage â¦‰
</tex>