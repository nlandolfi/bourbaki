<!--
!name:distortion_functions
!need:quantizations
!need:random_variables
!need:expectation
!need:differential_relative_entropy
!refs:cover/chapter_10
-->

§ Why ⦉
¶ ⦊
  ‖ We want to quantify the error of compressing a real-valued
    random variable. ⦉
⦉

§ Definition ⦉
¶ ⦊
  ‖ Let $𝒳$ be a finite set and $q: 𝗥 → 𝒳$ a quantization
    (see␣
    <a href='/sheets/quantizations.html'>
      ‖ Quantizations ⦉
    </a>
    ) of $𝗥$. ⦉

  ‖ Also, fix a probability space $(Ω, 𝒜, 𝗣)$ and a random
    variable $x: Ω → 𝗥$. ⦉
⦉

¶ ⦊
  ‖ The ❬compression❭ $\hat{x}: Ω → 𝒳$ of $x$ under $q$ is $q
    ∘ x$. ⦉

  ‖ A ❬distortion function❭ for $x$ and $\hat{x}$ is a function
    ◇ ⦊
      ‖ d: (Ω → 𝗥) × (Ω → 𝒳) → 𝗥. ⦉
    ⦉⦉

  ‖ Roughly speaking, a distortion function is meant to quantify
    the error in using this compression. ⦉
⦉

§§ Examples ⦉
¶ ⦊
  ‖ The ❬expected mean-squared-error distortion❭ $d_{\text{mse}}$
    between $x$ and $\hat{x}$ is
    ◇ ⦊
      ‖ d_{\text{mse}}(x, \hat{x}) = 𝗘[(x - \hat{x})^2] ⦉
    ⦉⦉

  ‖ The ❬Kulback-Liebler distortion❭ $d_{\text{kld}}$ defined by
    ◇ ⦊
      ‖ d_{\text{kld}}(x, \hat{x}) = 𝗘[d_{\text{kl}}(𝗣(y ∈ · |x,
        \hat{x}) |𝗣(y ∈ · |\hat{x}))] ⦉
    ⦉
    where $y$ is some random variable that depends on $x$.
    † ⦊
      ‖ Future editions will clarify this sentence. ⦉
    ⦉⦉
⦉

<tex>
  ‖ \blankpage ⦉
</tex>
