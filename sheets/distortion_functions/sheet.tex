%!name:distortion_functions
%!need:quantizations
%!need:random_variables
%!need:expectation
%!need:differential_relative_entropy


\ssection{Why}

We want to quantify the error of compressing a real-valued random variable.

\ssection{Definition}

Let $\CX$ be a finite set and $q: \R \to \CX$ a quantization (see \sheetref{quantizations}{Quantizations}) of $\R$.
Also, fix a probability space $(\Omega, \CA, \PM)$ and a random variable $x: \Omega \to \R$.


The \t{compression} $\hat{x}: \Omega \to \CX$ of $x$ under $q$ is $q \composed x$.
A \t{distortion function} for $x$ and $\hat{x}$ is a function
\[
  d: (\Omega \to \R) \cross (\Omega \to \CX) \to \R.
\]
Roughly speaking, a distortion function is meant to quantify the error in using this compression.

\ssubsection{Examples}

The \t{expected mean-squared-error distortion} $d_{\text{mse}}$ between $x$ and $\hat{x}$ is
\[
  d_{\text{mse}}(x, \hat{x}) = \E[(x - \hat{x})^2]
\]
The \t{Kulback-Liebler distortion} $d_{\text{kld}}$ defined by
\[
    d_{\text{kld}}(x, \hat{x}) = \E[d_{\text{kl}}(\PM(y \in \cdot \mid x, \hat{x}) \mid \PM(y \in \cdot \mid \hat{x}))]
\]
where $y$ is some random variable that depends on $x$.\footnote{Future editions will clarify this sentence.}


\blankpage
