<!--yaml
name: weighted_least_squares_linear_regressors
needs:
    - least_squares_linear_regressors
    - real_inner_products
-->

Â§ Why â¦‰
Â¶ â¦Š
  â€– What is the best linear regressor if we choose according to
    a weighted squared loss function. â¦‰
â¦‰

Â§ Definition â¦‰
Â¶ â¦Š
  â€– Suppose we have a paired dataset of $n$ records with inputs
    in $ğ—¥^d$ and outputs in $ğ—¥$. â¦‰

  â€– A â¬weighted least squares linear predictorâ­ for nonnegative
    weights $w âˆˆ ğ—¥^n$, $w â‰¥ 0$, is a linear transformation $f:
    ğ—¥^d â†’ ğ—¥$ (the field is $ğ—¥$) which minimizes
    â—‡ â¦Š
      â€– \frac{1}{n} âˆ‘_{i = 1}^{n} w_i(y_i - x^âŠ¤a^i)^2. â¦‰
    â¦‰â¦‰

  â€– Some authors refer to this process of selecting a linear
    predictor as the â¬weighted least-squares problemâ­. â¦‰
â¦‰

Â¶ â¦Š
  â€– Define $W âˆˆ ğ—¥^{n Ã—n}$ so that $W_{ii} = w_i$ and $W_{ij}
    = 0$ when $i â‰  j$. â¦‰

  â€– So, in particular, $W$ is a diagonal matrix. â¦‰

  â€– We want to find $x$ to minimize
    â—‡ â¦Š
      â€– \normm{W(Ax - y)} â¦‰
    â¦‰â¦‰
â¦‰

Â§ Solution â¦‰
<statement type='proposition'>
  Â¶ â¦Š
    â€– There exists a unique weighted least squares linear
      predictor and its parameters are given by
      â—‡ â¦Š
        â€– \inversep{\transpose{A}W\transpose{A}}\transpose{A}Wy. â¦‰
      â¦‰â¦‰
  â¦‰
</statement>
<tex>
  â€– \blankpage â¦‰
</tex>