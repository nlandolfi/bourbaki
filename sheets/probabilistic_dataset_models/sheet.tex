%!name:probabilistic_dataset_models
%!need:predictors
%!need:probability_measures

\ssection{Why}

Let $X = \set{a, b}$ and $Y = \set{0, 1}$. Define $f: X \to Y$ by $f \equiv 0$.

The dataset $(a, 0)$ is consistent with $f$.
So are the datasets $((a, 0), (a, 0))$ and $((a, 0), (a, 0), (a, 0))$.
Unfortunately, these datasets are \say{bad} in the sense that we do not see the value associated with $b$.
Each dataset is consistent with the (functional) relations $\set{(a, 0), (b, 0)}$ and $\set{(a, 0), (b, 1)}$.

In other words, a dataset may be incomplete.
In spite of this limitation, we want to discuss an inductor's performance on consistent datasets.
One route is to put a measure on the set of training sets and only consider high-measure subsets.

% We take two steps.
% First, we put a measure on the set of training sets and only consider high-measure subsets.
% Second, we consider inductors which perform well in some tolerance.

\ssection{Definition}

Let $(X, \CX)$ and $(Y, \CY)$ be measurable spaces and $R$ be a relation on $X \times Y$.
Let $\mu: \CX \times \CY \to [0, 1]$ be a probability measure so that $\CD = (X \times Y, \CX \times \CY, \mu)$ is a probability space.

If $\mu(B) = 0$ for all $B \subset \relcomplement{R}{X \times Y}$, then we call $\CD$ a \t{probabilistic dataset model} for $R$.
In other words, $\mu$ gives zero measure to any set of points not in the relation.
Equivalently, $\mu(R) = 1$; or we observe a pair in $R$ almost surely.

If $R$ is functional, then we call $\CD$ a \t{supervised probabilistic dataset model}.
In this case, since there is a functional relation between $X$ and $Y$, we call the marginal measure $\mu_X: \CX \to [0, 1]$ the \t{data-generating distribution} or \t{underlying distribution} since $\mu(A) = \mu_X(\Set*{x \in X}{(x, y) \in A)})$.
In this case we call the (functional) relation $R$ the \t{correct labeling function}.
Many authors refer to a supervised probabilistic data model as the \t{statistical learning (theory) framework}.

\ssection{Probable datasets}

For datasets of size $n$, we use the product measure $((X \times Y)^n, (\CX \times \CY)^n, \mu^n)$.
We interpret this measure as modeling independent and identically distributed elements of $R$.

For $\delta \in (0, 1)$, $\CS \subset (X \times Y)^n$ is \t{$1 - \delta$-probable}  if $\mu^n(S) \geq 1 - \delta$.
We call $\delta$ the \t{confidence parameter}.
If $\delta$ is small, we interpret $\CS$ as a set of \say{reasonable} datasets.
