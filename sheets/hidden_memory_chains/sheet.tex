%!name:hidden_memory_chains
%!need:memory_chains

\ssection{Why}\footnote{Future editions will include.}

\ssection{Definition}

Let $X$ and $Y$ be sets.
A \t{hidden memory chain} (\t{hidden markov chain}, \t{hidden memory model}, \t{hidden markov model}\footnote{This term is universal. We avoid it because of the Bourbaki project's policy on naming. The skeptical reader will note (as in \sheetref{memory_chains}{Memory Chains}) that our term and this term have the same initials.}, \t{HMM}) with \t{hiddens} (or \t{latents}) $Y$ and \t{observations} $Y$ of \t{length} $n$ is a joint distribution $p: X^n \times Y^n \to [0, 1]$ satisfying
\[
  p(x, y) = f(x)g(y_1, x_1)\prod_{i = 1}^{n} h(x_i, x_{i-1}) g(y_i, x_i),
\]
where $f: X \to [0, 1]$ is a distribution, and $g$ and $h$ are functions satisfying $g(\cdot, \xi)$ and $h(\cdot, \xi)$ are distributions on $Y$ and $X$, respectively, for all $\xi \in X$.
\begin{proposition}
  $p$ so defined is a distribution.
  The function $f$ is the distribition $p_{1}$.
  For all $i = 1, \dots, n$, $p_{n+1 \mid i} \equiv gg$.
  For all $i = 2, \dots, n$, $p_{i \mid i-1} \equiv h$.\footnote{Future editions will define everything needed in this proposition in the proposition statement, as ooposed to saying \say{$p$ so defined}.}
\end{proposition}

Clearly, $p_{1, \dots, n}$ is a memory chain (see \sheetref{memory_chains}{Memory Chains}).
For this reason, we continue to refer to $h$ as the \t{conditional distribution}.
We continute to refer to $f$ as the \t{initial distribution}.
We refer to $h$ as the \t{observation distribution}.

The word \say{hidden} refers to the situation in which we observe outcomes $y$, and we hypothesize that they were \say{generated by} unobserved outcomes $x$.


% \blankpage
