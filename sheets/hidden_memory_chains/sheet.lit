<!--yaml
name: hidden_memory_chains
needs:
    - memory_chains
-->

§ Definition ⦉
¶ ⦊
  ‖ Let $X$ and $Y$ be sets. ⦉

  ‖ A ❬hidden memory chain❭ (❬hidden markov chain❭, ❬hidden
    memory model❭, ❬hidden markov model❭,
    † ⦊
      ‖ This term is universal. ⦉

      ‖ We avoid it because of the Bourbaki project’s policy on
        naming. ⦉

      ‖ The skeptical reader will note (as in
        \sheetref{memory_chains} {Memory Chains}) that our term and
        this term have the same initials. ⦉
    ⦉
    ❬HMM❭) with ❬hiddens❭ (or ❬latents❭) $Y$ and ❬observations❭
    $Y$ of ❬length❭ $n$ is a joint distribution $p: X^n ×Y^n →
    [0, 1]$ satisfying
    ◇ ⦊
      ‖ p(x, y) = f(x)g(y_1, x_1)∏_{i = 1}^{n} h(x_i, x_{i-1})
        g(y_i, x_i), ⦉
    ⦉
    where $f: X → [0, 1]$ is a distribution, and $g$ and $h$
    are functions satisfying $g(·, ξ)$ and $h(·, ξ)$ are
    distributions on $Y$ and $X$, respectively, for all $ξ ∈ X$. ⦉
  <statement type='proposition'>
    ‖ $p$ so defined is a distribution. ⦉

    ‖ The function $f$ is the distribition $p_{1}$. ⦉

    ‖ For all $i = 1, …, n$, $p_{n+1 |i} ≡ gg$. ⦉

    ‖ For all $i = 2, …, n$, $p_{i |i-1} ≡ h$.
      † ⦊
        ‖ Future editions will define everything needed in this
          proposition in the proposition statement, as ooposed to
          saying “$p$ so defined” ⦉
      ⦉⦉
  </statement>
⦉

¶ ⦊
  ‖ Clearly, $p_{1, …, n}$ is a memory chain (see
    \sheetref{memory_chains}{Memory Chains}). ⦉

  ‖ For this reason, we continue to refer to $h$ as the
    ❬conditional distribution❭. ⦉

  ‖ We continute to refer to $f$ as the ❬initial distribution❭. ⦉

  ‖ We refer to $h$ as the ❬observation distribution❭. ⦉
⦉

¶ ⦊
  ‖ The word “hidden” refers to the situation in which we
    observe outcomes $y$, and we hypothesize that they were
    “generated by” unobserved outcomes $x$. ⦉
⦉

<tex>
  ‖ \blankpage ⦉
</tex>