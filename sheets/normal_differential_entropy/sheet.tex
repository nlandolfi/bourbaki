%!name:normal_differential_entropy
%!need:multivariate_normals

\ssection{Why}

What is the differential mutual information
between two components of a multivariate normal.

\ssection{Result}

\begin{prop}
Let $g \sim \normal{\mu}{\Sigma}$
Then the differential entropy of g is
\[
  \frac{1}{2}\ln\left((2\pi e)^d \det\Sigma\right)
\]
\end{prop}
