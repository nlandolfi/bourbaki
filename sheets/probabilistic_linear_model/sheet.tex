%!name:probabilistic_linear_model
%!need:multivariate_normals
%!need:matrix_transpose
%!need:probability_measures
%!need:linear_predictors
%!need:expectation
%!need:data_matrix
%!need:normal_conditionals
%!need:matrix_inverses
%!need:covariance
%!need:estimators

\ssection{Why}

We want an estimator for the parameters of a linear function, given observations of the function with additive noise.

\ssection{Definition}

Let $(\Omega, \CA, \PM)$ be a probability space.
Let $x: \Omega \to \R^d$.
Define $g: \Omega \to (\R^d \to \R)$ by $g(\omega)(a) = \transpose{a}x(\omega)$, for $a \in \R^d$.
In other words, for each outcome $\omega \in \Omega$, $g_\omega: \R^d \to \R$ is a linear function with parameters $x(\omega)$.
$g_\omega$ is the function of interest.


Let $a^1, \dots, a^n \in \R^d$ a dataset with data matrix $A \in \R^{n \times d}$.
Let $e: \Omega \to \R^n$ independent of $x$, and define $y: \Omega \to \R^n$ by
\[
  y = Ax + e.
\]
In other words, $y_i = \transpose{x}a^i + e_i$.

We call $(x, A, e)$ a \t{probabilistic linear model}.
Other terms include \t{linear model}, \t{linear regression model}, \t{bayesian linear regression}, and \t{bayesian analysis of the linear model}.\footnote{The word bayesian is in reference to treating the object of interest---$x$---as a random variable.}
We call $x$ the parameters, $A$ a \t{design}, $e$ the \t{error} or \t{noise} vector, and $y$ the \t{observation} vector.

One may want an estimator for the parameters $x$ in terms of $y$ or one may be modeling the function $g$ and want to predict $g(a)$ for $a \in A$ not in the dataset.

\ssection{Mean and variance}

\begin{proposition}
  $\E(y) = A\E(x) + \E(w)$\footnote{By linearity. Full account in future editions.}
\end{proposition}

\begin{proposition}
  $\cov((x, y)) = A\cov(x)\transpose{A} + \cov{e}$\footnote{Full account in future editions.}
\end{proposition}
