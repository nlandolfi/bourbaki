<!--yaml
name: differential_entropy
needs:
    - multivariate_real_densities
    - discrete_entropy
-->

Â§ Why â¦‰
Â¶ â¦Š
  â€– We want to extend our notion of entropy (see
    \sheetref{discrete_entropy}{Discrete Entropy}) to real-valued
    (continuous) random variables. â¦‰
â¦‰

Â§ Definition â¦‰
Â¶ â¦Š
  â€– The â¬differential entropyâ­ of a probability density function
    is the integral of the density against the negative log of
    the density. â¦‰

  â€– This definition made to be similar to the case of discrete
    entropy. â¦‰

  â€– If a real-valued random variable has a density, then we
    call the differential entropy of its density the â¬differential
    entropyâ­ of the random variable. â¦‰
â¦‰

Â§Â§ Notation â¦‰
Â¶ â¦Š
  â€– Let $f: ğ—¥^n â†’ ğ—¥$ be a probability density function. â¦‰

  â€– The differential entropy of $f$ is
    â—‡ â¦Š
      â€– - âˆ«f \log f â¦‰
    â¦‰

    â€– We denote the differential entropy of $f$ by $h(f)$. â¦‰â¦‰
â¦‰

Â§Â§ Example â¦‰
Â¶ â¦Š
  â€– Let $x: Î© â†’ ğ—¥$ be uniform on $[0, 1/2]$. â¦‰

  â€– Then $h(x) = \log1/2 < 0$. â¦‰
â¦‰

Â§Â§ Problems â¦‰
Â¶ â¦Š
  â€– We have $h(ax) = h(x) + \log\abs{a}$. â¦‰

  â€– In general $h(Ax) = h(x) + \log\abs{A}$. â¦‰
â¦‰

Â§Â§ Differences still meaningful â¦‰
Â¶ â¦Š
  â€– Even though the value of the differential entropy is not
    necessarily a good analogy to discrete entropy, differences
    still are. â¦‰

  â€– In particular, the following holds
    â—‡ â¦Š
      â€– I(X; Y) = H(Y) - H(Y |X) = H(X) = H(X |Y) â¦‰
    â¦‰â¦‰
â¦‰

<tex>
  â€– \blankpage â¦‰
</tex>