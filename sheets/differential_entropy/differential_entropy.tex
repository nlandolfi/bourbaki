\input{../../sheet.tex}
\sbasic
\input{../multivariate_real_densities/macros.tex}
\input{../entropy/macros.tex}
\input{../probability_densities/macros.tex}
\input{../n-dimensional_space/macros.tex}
\input{../probability_distributions/macros.tex}
\input{../logarithm/macros.tex}
\input{../probability_events/macros.tex}
\input{../space/macros.tex}
\input{../real_numbers/macros.tex}
\input{../summation/macros.tex}
\input{../probability_outcomes/macros.tex}
\input{../subsets/macros.tex}
\input{../integer_numbers/macros.tex}
\input{../probability/macros.tex}
\input{../sets/macros.tex}
\input{../cardinality/macros.tex}
\input{../solving_equations/macros.tex}
\input{../objects/macros.tex}
\input{../natural_numbers/macros.tex}
\input{../arithmetic/macros.tex}
\input{../equations/macros.tex}
\input{../functions/macros.tex}
\input{../operations/macros.tex}
\input{../relations/macros.tex}
\input{../ordered_pairs/macros.tex}
\input{./macros.tex}
\sstart
\stitle{Differential Entropy}

\ssection{Why}

We want a notion of entropy
for continuous random variables.

\ssection{Definition}

The \ct{relative entropy}{}
of a probability density function
is the integral of the density
against the negative log
of the density.

\ssubsection{Notation}

Let $R$ denote the set of
real numbers.
Let $f: R^n \to R$ be
a probability density function.
The differential entropy of $f$
is
\[
  - \int f \log f
\]
We denote the differential entropy
of $f$ by $h(f)$.
\strats
