\input{../../sheet.tex}
\sbasic
\input{../probability_density_function/macros.tex}
\input{../entropy/macros.tex}
\input{../probability_measures/macros.tex}
\input{../distributions/macros.tex}
\input{../logarithm/macros.tex}
\input{../measure_space/macros.tex}
\input{../real_numbers/macros.tex}
\input{../summation/macros.tex}
\input{../subset_algebras/macros.tex}
\input{../integer_numbers/macros.tex}
\input{../set_operations/macros.tex}
\input{../set_specification/macros.tex}
\input{../cardinality/macros.tex}
\input{../solving_equations/macros.tex}
\input{../algebras/macros.tex}
\input{../set_extension/macros.tex}
\input{../natural_numbers/macros.tex}
\input{../arithmetic/macros.tex}
\input{../equations/macros.tex}
\input{../operations/macros.tex}
\input{../identity/macros.tex}
\input{../functions/macros.tex}
\input{../objects/macros.tex}
\input{../relations/macros.tex}
\input{../ordered_pairs/macros.tex}
\input{../sets/macros.tex}
\input{./macros.tex}
\sstart
\stitle{Differential Entropy}

\ssection{Why}

We want a notion of entropy
for continuous random variables.

\ssection{Definition}

The \ct{relative entropy}{}
of a probability density function
is the expected negative log
of the density.

\ssubsection{Notation}

Let $R$ denote the set of
real numbers.
Let $f: R^n \to R$ be
a probability density function.
The differential entropy of $f$
is
\[
  - \int f \log f
\]
We denote the differential entropy
of $f$ by $h(f)$.
\strats
