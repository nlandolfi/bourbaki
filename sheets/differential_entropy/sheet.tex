%!name:differential_entropy
%!need:multivariate_real_densities
%!need:entropy

\ssection{Why}

We want a notion of \say{uncertainty} for a real-valued (continuous) random variable.

\ssection{Definition}

The \t{relative entropy} of a probability density function is the integral of the density against the negative log of the density.

\ssubsection{Notation}

Let $f: \R^n \to \R$ be a probability density function.
The differential entropy of $f$ is
\[
  - \int f \log f
\]
We denote the differential entropy of $f$ by $h(f)$.

\ssection{Example}

Let $x: \Omega \to \R$ be uniform on $[0, \nicefrac{1}{2}]$.
Then $h(x) = \log\nicefrac{1}{2} < 0$.

\ssection{Problems}

We have $h(ax) = h(x) + \log\abs{a}$.
In generaly $h(Ax) = h(x) + \log\abs{A}$.

\ssection{Differences still meaningful}

Even though the value of the differential entropy is not necessarily a good analogy to discrete entropy, differences still are.
In particular, the following holds
\[
  I(X; Y) = H(Y) - H(Y \mid X) = H(X) = H(X \mid Y)
\]

\blankpage
