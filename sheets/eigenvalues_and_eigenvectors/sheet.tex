%!name:eigenvalues_and_eigenvectors
%!need:real_matrices
%!need:real_vectors
%!need:norms
%!need:matrix_determinants
%!need:monic_polynomials

\section*{Why}

We discuss vectors for which the action of a matrix is scalar multiplication.
  \ifhmode\unskip\fi\footnote{
Future editions will expand.
  }

\section*{Definition}

Let $A \in \R ^{n \times n}$ be a square matrix.
A nonzero vector $x \in \R ^{n}$ is an \t{eigenvector} of $A$, and $\lambda  \in \R $ is its corresponding \t{eigenvalue}, if $A x = \lambda  x$.
In other words, $x \neq 0$ is an eigenvector if the action of $A$ on $x$ is to mimic scalar multiplication.

Speaking of eigenvalues is sensible only when the matrix involved is square.
In other words, when the domain and codomain are the same.
We often care about eigenvalue computations when a matrix is compounded iteratively.


%<div data-littype='paragraph'>
%  <div data-littype='run'> ‚ù≤%\ssubsection{Notation}‚ù≥ </div>
%  <div data-littype='run'> ‚ù≤%‚ù≥ </div>
%  <div data-littype='run'> ‚ù≤%The spectrum of $A ‚àà ùó•^{n √ó n}$ is sometimes denoted $\Lambda(A)$.‚ù≥ </div>
%</div>

\section*{Eigenspaces}

If $x$ is an eigenvector with eigenvalue $\lambda $, then for any $\alpha  \in \R $, $\alpha  x$ is an eigenvector with eigenvalue $\alpha \lambda $, since $A (\alpha  x) = \alpha  (Ax) = (\alpha \lambda ) x$.
In other words, if $A$ has an eigenvector then the action of $A$ on some subspace $S \subset \R ^n$ is to mimic scalar multiplication.
In this case, we call the subspace $S$ an \t{eigenspace}, and any nonzero $x \in S$ an eigenvector.

An eigenspace is an \t{invariant subspace} of $A$.
In other words, if $E$ is an eigenspace corresponding to eigenvalue $\lambda $ then $AE \subset E$.

The dimension of $E$ is the maximum number of linear independent eigenvectors which have the same eigenvalue $\lambda $.
We call this number the \t{geometric multiplicity} of $\lambda $.

\section*{Characteristic polynomial}

If $x$ is an eigenvector for $A$ associated with $\lambda $ then $Ax = \lambda  x$ so $Ax - \lambda  x = 0$ and $(A - \lambda  I)x = 0$.
In other words, $x$ is an element of the nullspace of $A - \lambda  I$. Or equivalently, $\lambda  I - A$.

The \t{characteristic polynomial} of $A \in \R ^{n \times n}$ is the polynomial $p: \R  \to \R $ in defined by
  \[
p(x) = \det (zI - A).
  \]
$p$ is monic: the coefficient of the degree $n$ term is 1.
\begin{proposition}
$\lambda $ is an eigenvalue of $A$ if and only if $p(\lambda ) = 0$.
\begin{proof}
Since $\lambda $ is an eigenvalue if and only if there is a nonzero vector $x$ such that $\lambda  x - Ax = 0$, if and only if $\lambda  I - A$ is singular, if and only if $\det(\lambda  I - A) = 0$.
\end{proof}
\label{proposition:eigenvalues_and_eigenvectors:characteristic_polynomial}
\end{proposition}
A simple consequence of Proposition~\ref{proposition:eigenvalues_and_eigenvectors:characteristic_polynomial} is that a (real) matrix may have complex eigenvalues.
