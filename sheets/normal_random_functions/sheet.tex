
%!name:normal_random_functions
%!need:random_functions
%!need:multivariate_normals

\section*{Why}

We want to discuss real-valued random functions whose family of random variables have simple densities.\footnote{Future editions will expand.}

\section*{Definition}

A \t{normal random function} is a real-valued random function whose family of real-valued random variables has the property that any subfamily is jointly normal.

For this reason, we call the family of random variables (or stochastic process) corresponding to the random function a \t{gaussian process} or \t{normal process}.
%\footnote{As usual, The choice of \say{normal} is a result of the Bourbaki project's convention to eschew historical names. Though here, as in \sheetref{multivariate_normals}{Multivariate Normals} the language of the project is nonstandard.}

  \subsection*{Notation}

Let $(\Omega , \mathcal{A} , \mathbfsf{P} )$ be a probability space and $A$ a set.
Let $x: \Omega  \to (A \to \R )$ be a random function with family $y: A \to (\Omega  \to \R )$.

The random function $x$ is a normal if, for all $a^1, \dots , a^m \in A$, $(y(a^1), \dots , y(a^m))$ is jointly normal.

  \section*{Mean and covariance function}

\begin{proposition}
Let $x: \Omega  \to (A \to \R )$ be a normal random function with family $X: A \to (\Omega  \to \R )$.
There exists unique functions $m: A \to \R $ and $k: A \times A \to \R $ so that the mean of the random variable $X_a$ is $m(a)$ for all $A$ and the covariance of the random variables $X_a$ and $X_{a'}$ is $k(a, a')$ for all $a, a' \in A$.\footnote{Future editions may include an account.}\end{proposition}
For this reason, we call $m$ the \t{mean function} and $k$ the \t{covariance function} of the random function.

Conversely, let $m: A \to \R $ and $k: A \times A \to \R $.
Then if $k$ satisfies the property that for all $a^1, \dots , a^m$, the $m \times m$ matrix
      \[
\pmat{
k(a^1, a^1) & \cdots & k(a^1, a^m) \\
\vdots & \ddots & \vdots \\
k(a^m, a^1) & \cdots & k(a^m, a^m) \\
}
      \]
is positive semidefinite, then we can construct a Gaussian process with mean function $m$ and covariance function $k$.\footnote{Some authors belabor this point because of the natural inclination to want to specify an \textit{inverse} covariance function, which need not satisfy the consistency property. The consistency property ensures that any marginal of a subfamilys density is the density of that further subfamily. Future editions may expand.}
For this reason, we call $k$ with such a property \t{positive semidefinite} or a \t{covariance function}.
Notice, of course, that $k$ is symmetric.
The matrix above is sometimes called the \t{Gram matrix} for $k$ and $a^1, \dots , a^m$.

  \subsection*{Example}

Let $A = \set{1, \dots , n}$ and let $K \in \R ^{n \times n}$ be symmetric positive semidefinite.
Define $m: A \to \R $ to be $m \equiv 0$ (the constant zero function) and $k(i, j) = K_{ij}$.
Then the normal random function $x: \Omega  \to (A \to \R )$ with mean $m$ and covariance $k$ is in one to one correspondence with the gaussian random vectors with mean zero.

%% \ssubsection{Random function interpretation}
%%
%% Many authorities discuss a normal random function as \say{putting a prior} on a \say{space} (see, for example, \sheetref{real_function_space}{Real Function Space}) of functions.
%% One samples functions by drawing an outcome $\omega \in \Omega$, and then defining the sample $f: I \to \R$ by $f(i) = x(i)(\omega)$.
%% \ssubsection{Multivariate normal special case}
%%
%% If the index set is finite, and can be ordered, then the normal random function is in one-to-one correspondence with the multivariate normal random vectors.
%%
%%\blankpage

