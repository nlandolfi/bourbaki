<!--
!name:normal_random_functions
!need:random_functions
!need:multivariate_normals
-->

§ Why ⦉
¶ ⦊
  ‖ We want to discuss real-valued random functions whose family
    of random variables have simple densities.
    † ⦊
      ‖ Future editions will expand. ⦉
    ⦉⦉
⦉

§ Definition ⦉
¶ ⦊
  ‖ A ❬normal random function❭ is a real-valued random function
    whose family of real-valued random variables has the property
    that any subfamily is jointly normal. ⦉
⦉

¶ ⦊
  ‖ For this reason, we call the family of random variables (or
    stochastic process) corresponding to the random function a
    ❬gaussian process❭ or ❬normal process❭. ⦉

  <!--
\footnote{As usual, The choice of \say{normal} is a result of the Bourbaki project's convention to eschew historical names. Though here, as in \sheetref{multivariate_normals}{Multivariate Normals} the language of the project is nonstandard.}
-->

  §§ Notation ⦉
  ¶ ⦊
    ‖ Let $(Ω, 𝒜, 𝗣)$ be a probability space and $A$ a set. ⦉

    ‖ Let $x: Ω → (A → 𝗥)$ be a random function with family
      $y: A → (Ω → 𝗥)$. ⦉
  ⦉

  ¶ ⦊
    ‖ The random function $x$ is a normal if, for all $a^1, …,
      a^m ∈ A$, $(y(a^1), …, y(a^m))$ is jointly normal. ⦉
  ⦉

  § Mean and covariance function ⦉
  <statement type='proposition'>
    ‖ Let $x: Ω → (A → 𝗥)$ be a normal random function with
      family $X: A → (Ω → 𝗥)$. ⦉

    ‖ There exists unique functions $m: A → 𝗥$ and $k: A ×A →
      𝗥$ so that the mean of the random variable $X_a$ is
      $m(a)$ for all $A$ and the covariance of the random
      variables $X_a$ and $X_{a'}$ is $k(a, a')$ for all $a, a'
      ∈ A$.
      † ⦊
        ‖ Future editions may include an account. ⦉
      ⦉⦉
  </statement>
  ¶ ⦊
    ‖ For this reason, we call $m$ the ❬mean function❭ and $k$
      the ❬covariance function❭ of the random function. ⦉
  ⦉

  ¶ ⦊
    ‖ Conversely, let $m: A → 𝗥$ and $k: A ×A → 𝗥$. ⦉

    ‖ Then if $k$ satisfies the property that for all $a^1, …,
      a^m$, the $m ×m$ matrix
      ◇ ⦊
        ‖ \pmat{ ⦉

        ‖ k(a^1, a^1) ＆ ⋯ ＆ k(a^1, a^m) ᜶ ⦉

        ‖ ⋮ ＆ ⋱ ＆ ⋮ ᜶ ⦉

        ‖ k(a^m, a^1) ＆ ⋯ ＆ k(a^m, a^m) ᜶ ⦉

        ‖ } ⦉
      ⦉
      is positive semidefinite, then we can construct a Gaussian
      process with mean function $m$ and covariance function $k$.
      † ⦊
        ‖ Some authors belabor this point because of the natural
          inclination to want to specify an ‹inverse› covariance
          function, which need not satisfy the consistency
          property. The consistency property ensures that any
          marginal of a subfamilys density is the density of that
          further subfamily. Future editions may expand. ⦉
      ⦉⦉

    ‖ For this reason, we call $k$ with such a property
      ❬positive semidefinite❭ or a ❬covariance function❭. ⦉

    ‖ Notice, of course, that $k$ is symmetric. ⦉

    ‖ The matrix above is sometimes called the ❬Gram matrix❭ for
      $k$ and $a^1, …, a^m$. ⦉
  ⦉

  §§ Example ⦉
  ¶ ⦊
    ‖ Let $A = \set{1, …, n}$ and let $K ∈ 𝗥^{n ×n}$ be
      symmetric positive semidefinite. ⦉

    ‖ Define $m: A → 𝗥$ to be $m ≡ 0$ (the constant zero
      function) and $k(i, j) = K_{ij}$. ⦉

    ‖ Then the normal random function $x: Ω → (A → 𝗥)$ with
      mean $m$ and covariance $k$ is in one to one
      correspondence with the gaussian random vectors with mean
      zero. ⦉
  ⦉

  <!--
% \ssubsection{Random function interpretation}
%
% Many authorities discuss a normal random function as \say{putting a prior} on a \say{space} (see, for example, \sheetref{real_function_space}{Real Function Space}) of functions.
% One samples functions by drawing an outcome $\omega \in \Omega$, and then defining the sample $f: I \to \R$ by $f(i) = x(i)(\omega)$.

% \ssubsection{Multivariate normal special case}
%
% If the index set is finite, and can be ordered, then the normal random function is in one-to-one correspondence with the multivariate normal random vectors.
%
%\blankpage
-->
⦉
