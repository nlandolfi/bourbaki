<!--yaml
name: norm_weighted_least_squares_linear_regressors
needs:
    - weighted_least_squares_linear_regressors
-->

§ Why ⦉
¶ ⦊
  ‖ What is the best linear predictor if we choose according to
    a particular norm. ⦉
⦉

§ Definition ⦉
¶ ⦊
  ‖ Suppose we have a paired dataset of $n$ records with inputs
    in $𝗥^d$ and outputs in $𝗥$. ⦉

  ‖ A ❬norm weighted least squares linear predictor❭ for a norm
    $g: 𝗥^n → 𝗥$ is a linear transformation $f: 𝗥^d → 𝗥$ (the
    field is $𝗥$) which minimizes
    ◇ ⦊
      ‖ g(y - Ax). ⦉
    ⦉⦉
⦉

§ Weight matrix ⦉
¶ ⦊
  ‖ Let $\norm{·}_{W}$ be the weighted norm for some positive
    semidefinite weight matrix $W$. ⦉

  ‖ We want to find $x$ to minimize
    ◇ ⦊
      ‖ \norm{y - AX}_{W}. ⦉
    ⦉⦉

  ‖ This problem is referred to by many authors as ❬weighted
    least squares❭ or the ❬weighted least squares problem❭. ⦉
⦉

§ Diagonal weight matrix ⦉
¶ ⦊
  ‖ A special case of norm weighted least squares with a
    weighted norm is the usual weighted least squares problem
    (see␣
    <a href='/sheets/weighted_least_squares_linear_predictors.html'>
      ‖ Weighted Least Squares Linear Predictors ⦉
    </a>
    ). ⦉

  ‖ Consider weighted least squares with weights $w ∈ 𝗥^n$, $w
    ≥ 0$. ⦉

  ‖ Define $W ∈ 𝗥^{n ×n}$ so that $W_{ii} = w_i$ and $W_{ij}
    = 0$ when $i ≠ j$. ⦉

  ‖ So, in particular, $W$ is a diagonal matrix and
    ◇ ⦊
      ‖ \norm{y - Ax}_{W} = ∑_{i = 1}^{n} w_i(y_i -
        \transpose{x}a_i)^2. ⦉
    ⦉⦉
⦉

§ Solution ⦉
<statement type='proposition'>
  ‖ There exists a unique weighted least squares linear predictor
    and its parameters are given by ⦉
  ◇ ⦊
    ‖ \inv{(\transpose{A}WA)}\transpose{A}Wy. ⦉
  ⦉
</statement>