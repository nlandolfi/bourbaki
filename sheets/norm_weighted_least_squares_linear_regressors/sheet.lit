<!--yaml
name: norm_weighted_least_squares_linear_regressors
needs:
    - weighted_least_squares_linear_regressors
-->

Â§ Why â¦‰
Â¶ â¦Š
  â€– What is the best linear predictor if we choose according to
    a particular norm. â¦‰
â¦‰

Â§ Definition â¦‰
Â¶ â¦Š
  â€– Suppose we have a paired dataset of $n$ records with inputs
    in $ğ—¥^d$ and outputs in $ğ—¥$. â¦‰

  â€– A â¬norm weighted least squares linear predictorâ­ for a norm
    $g: ğ—¥^n â†’ ğ—¥$ is a linear transformation $f: ğ—¥^d â†’ ğ—¥$ (the
    field is $ğ—¥$) which minimizes
    â—‡ â¦Š
      â€– g(y - Ax). â¦‰
    â¦‰â¦‰
â¦‰

Â§ Weight matrix â¦‰
Â¶ â¦Š
  â€– Let $\norm{Â·}_{W}$ be the weighted norm for some positive
    semidefinite weight matrix $W$. â¦‰

  â€– We want to find $x$ to minimize
    â—‡ â¦Š
      â€– \norm{y - AX}_{W}. â¦‰
    â¦‰â¦‰

  â€– This problem is referred to by many authors as â¬weighted
    least squaresâ­ or the â¬weighted least squares problemâ­. â¦‰
â¦‰

Â§ Diagonal weight matrix â¦‰
Â¶ â¦Š
  â€– A special case of norm weighted least squares with a
    weighted norm is the usual weighted least squares problem
    (seeâ£
    <a href='/sheets/weighted_least_squares_linear_predictors.html'>
      â€– Weighted Least Squares Linear Predictors â¦‰
    </a>
    ). â¦‰

  â€– Consider weighted least squares with weights $w âˆˆ ğ—¥^n$, $w
    â‰¥ 0$. â¦‰

  â€– Define $W âˆˆ ğ—¥^{n Ã—n}$ so that $W_{ii} = w_i$ and $W_{ij}
    = 0$ when $i â‰  j$. â¦‰

  â€– So, in particular, $W$ is a diagonal matrix and
    â—‡ â¦Š
      â€– \norm{y - Ax}_{W} = âˆ‘_{i = 1}^{n} w_i(y_i -
        \transpose{x}a_i)^2. â¦‰
    â¦‰â¦‰
â¦‰

Â§ Solution â¦‰
<statement type='proposition'>
  â€– There exists a unique weighted least squares linear predictor
    and its parameters are given by â¦‰
  â—‡ â¦Š
    â€– \inv{(\transpose{A}WA)}\transpose{A}Wy. â¦‰
  â¦‰
</statement>