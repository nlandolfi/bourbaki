%!name:feature_embeddings
%!need:least_squares_linear_predictors

\ssection{Why}

Linear predictors are simple and we know how to select the parameters,
but the main drawback is that we there may not be a linear relationship between inputs and outputs.

\ssection{Definition}

A \t{feature embedding} for postcepts $A$ is a mapping $\phi: A \to \R^d$.
In this setting, we call $a \in A$ the \t{raw input record} and we call $\phi(a)$ an \t{embedding}, \t{feature embedding} of \t{feature vector}.
A feature embedding is \t{faithful} if, whenever records $a_i$ and $a_j$ are in some sense \say{similar} in the set $A$, the embeddings $\phi(a_i)$ and $\phi(a_j)$ are close in the vector space $\R^d$.

Since it is common for raw input records $a \in A$ to consist of many fields, in is regular to have several embedding functions $\phi_i$ which operate component-wise on the fields of $a$.
We concatenate these field embeddings and commonly add a constant feature $1$.
Since $\R^d$ is a vector space, it is common to refer to it in this case as the \t{feature space}.

\blankpage
