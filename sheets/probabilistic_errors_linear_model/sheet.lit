<!--
!name:probabilistic_errors_linear_model
!need:probability_measures
!need:feature_maps
!need:data_matrix
!need:real_matrix_inverses
!need:covariance
!need:probabilistic_predictors
!need:random_variable_moments
!refs:pukelsheim2004optimal
-->

Â§ Why â¦‰
Â¶ â¦Š
  â€– We model a real-valued output as corrupted by small random
    errors. â¦‰

  â€– Thus, we can talk about a dataset which is â€œcloseâ€ to
    being consistent with a linear predictor. â¦‰
â¦‰

Â§ Definition â¦‰
Â¶ â¦Š
  â€– Let $(Î©, ğ’œ, ğ—£)$ be a probability space. â¦‰

  â€– Let $x âˆˆ ğ—¥^d$ and $e: Î© â†’ ğ—¥^n$. â¦‰

  â€– For $A âˆˆ ğ—¥^{n Ã—d}$, define $y: Î© â†’ ğ—¥^n$ by $y = Ax +
    e$. â¦‰

  â€– We call $(x, A, e)$ a â¬probabilistic errors linear modelâ­. â¦‰

  â€– We call $y$ the â¬response vectorâ­, $A$ the â¬model matrixâ­
    and $e$ the â¬error vectorâ­. â¦‰
â¦‰

Â§ Moment assumptions â¦‰
Â¶ â¦Š
  â€– The most basic distributional assumption for a probabilistic
    errors linear model pertain to the expectation and variance. â¦‰

  â€– Since $ğ—˜(y) = Ax + ğ—˜(e)$ and $\var(y) = \var(e)$, these
    assumptions can be given for $e$ or for $y$. â¦‰
â¦‰

Â¶ â¦Š
  â€– If $ğ—˜(x) = 0$ and $\var(y) = Ïƒ^2I$ then we call $(x, A,
    e)$ a â¬classical linear model with moment assumptionsâ­. â¦‰

  â€– Notice that the components of $e$ are assumed uncorrelated. â¦‰

  â€– We have $d + 1$ unknowns (the $d Ã—1$ entires of $Î¸$ and
    scalar parameter $Ïƒ^2$. â¦‰
â¦‰

Â¶ â¦Š
  â€– In this case $ğ—˜(y_i) = \transpose{a^i}Î¸$ and so $Î¸$ is
    called the â¬mean parameter vectorâ­ and $Ïƒ^2$ is called the
    â¬model varianceâ­. â¦‰

  â€– The model variance indicates the variability inherent in the
    observations. â¦‰

  â€– Neither the mean nor variance of the error depends on the
    regression vector $x$ nor on the parameter vector $Î¸$. â¦‰
â¦‰

Â§ Examples â¦‰
Â¶ â¦Š
  â€– Consider the â¬two-sample problemâ­ in which we have two
    populations with (unknown) mean responses $Î±_1, Î±_2 âˆˆ ğ—¥$. â¦‰

  â€– We observe these responses with (perhaps unknown) common
    variance $Ïƒ^2$, and assume that errors are uncorrelated. â¦‰
â¦‰

Â¶ â¦Š
  â€– We define $y^1 = Î±_1ğŸ + e^1$ and $y^2 = Î±_2ğŸ + e^2$ so
    that we can stack these and obtain
    â—‡ â¦Š
      â€– y = \bmat{y^1 áœ¶ y^2} = \bmat{Î±_1ğŸ áœ¶ Î±_2ğŸ} + \bmat{e^1
        áœ¶ e^2}. â¦‰
    â¦‰â¦‰

  â€– To cast this in our standard form we define
    â—‡ â¦Š
      â€– A = \transpose{ â¦‰

      â€– \bmat{\bmat{1áœ¶0} ï¼† â‹¯ ï¼†\bmat{1áœ¶0} ï¼† \bmat{0 áœ¶ 1} ï¼†â‹¯ ï¼†
        \bmat{0 áœ¶ 1} â¦‰

      â€– } â¦‰

      â€– }, \quad x = \bmat{Î±_1 áœ¶ Î±_2}. â¦‰
    â¦‰
    with regression vectors $a_1 = (1, 0)$ and $x_2 = (0, 1)$
    repeated $n_1$ and $n_2$ times, respectively. â¦‰

  â€– An input design for this model involves specifying a
    sequence of these two vectors, which (with the uncorrelated
    assumption) reduces to dictating how many responses should be
    collected from each population. â¦‰

  â€– The inputs here is really the set $ğ’³ = \set{1, 2}$. â¦‰

  â€– The feature function is $Ï†: ğ’³ â†’ ğ—¥^2$ defined by $Ï†(1) =
    (1, 0)$ and $Ï†(2) = (0, 1)$. â¦‰

  â€– And so the regression range is $\set{(1, 0), (0, 1)}$. â¦‰
â¦‰
