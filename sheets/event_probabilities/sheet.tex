
%!name:event_probabilities
%!need:probability_distributions

\section*{Why}

Since one and only one outcome occurs, given a distribution on outcomes, we define the probability of a set of outcomes as the sum of their probabilities.

\section*{Definition}

Suppose $p$ is a distribution on $\Omega $.
For any event $A \subset \Omega $, we call the value $\sum_{a \in A} p(a)$ the \t{event probability}.
We refer to the probability \textit{of} $A$.
The probability of $A$ is the sum of the probabilities of its outcomes.

\subsection*{Notation}

We can define a function $\mathbfsf{P} : \powerset{\Omega } \to \R $ by $\mathbfsf{P} (A) = \sum_{a \in A} p(a)$.
We call $\mathbfsf{P} $ \t{the event probability function} (or \t{the probability measure}) induced by $p$.
Since $\mathbfsf{P} $ depends on the sample space $\Omega $ and the distribution $p$, we occasionally denote this dependence by $\mathbfsf{P} _{\Omega , p}$ or $\mathbfsf{P} _p$.

Many authors associate an event $A \subset \Omega $ with a function $\pi : \Omega  \to \set{0, 1}$ so that $A = \Set*{\omega  \in \Omega }{\pi (\omega ) = 1}$.
In this context, it is common to write $P[\pi ]$ for $\mathbfsf{P} [\pi ^{-1}(1)]$ which is $\mathbfsf{P} (A)$.

\section*{Example: die}

Define $p: \set{1, \dots , 6} \to \R $ by $p(\omega ) = 1/6$ for $\omega  = 1, \dots , 6$.
Define the event $E = \set{2, 4, 6}$.
Then
    \[
\textstyle
\mathbfsf{P} (E) = \sum_{\omega  \in E} p(\omega ) = p(2) + p(4) + p(6) = 1/2.
    \]

\section*{Properties of $\mathbfsf{P} $}

As a result of the conditions on $p$, $\mathbfsf{P} $ satisfies
    \begin{enumerate}
      \item $\mathbfsf{P} (A) \geq 0$ for all $A \subset \Omega $;
      \item $\mathbfsf{P} (\Omega ) = 1$ (and $\mathbfsf{P} (\varnothing) = 0$);
      \item $\mathbfsf{P} (A) + \mathbfsf{P} (B)$ for all $A, B \subset \Omega $ and $A \cap  B = \varnothing$.
This statement follows from the more general identity
          \[
\mathbfsf{P} (A \cup B) = \mathbfsf{P} (A) + \mathbfsf{P} (B) - \mathbfsf{P} (A \cap  B)
          \]
for $A, B \subset \Omega $, by using $\mathbfsf{P} (\varnothing) = 0$ of (2) above.
    \end{enumerate}

These three conditions are sometimes called the \t{axioms of probability for finite sets}.
Do all such $\mathbfsf{P} $ satisfying (1)-(3) have a corresponding underlying probability distribution?

In other words, suppose $f: \powerset{\Omega } \to \R $ satisfies (1)-(3).
Define $q: \Omega  \to \R $ by $q(\omega ) = f(\set{\omega })$.
If $f$ satisfies the axioms, then $q$ is a probability distribution.
For this reason we call any function satisfying (i)-(iii) an \t{event probability function} (or a \t{(finite) probability measure}).

\section*{Other basic consequences}

\subsection*{Probability by cases}

Let $\mathbfsf{P} $ be a probability event function.
Suppose $A_1, \dots , A_n$ partition $\Omega $.
Then for any $B \subset \Omega $,
    \[
\textstyle
\mathbfsf{P} (B) = \sum_{i = 1}^{n} \mathbfsf{P} (A_i \cap  B).
    \]
Some authors call this the \t{law of total probability}.

\subsection*{Monotonicity}

If $A \subseteq B$, then $\mathbfsf{P} (A) \leq P(B)$.
This is easy to see by splitting $B$ into $A \cap  B$ and $B - A$, and applying (1) and (3).

\subsection*{Subadditivity}

For $A, B \subset \Omega $, $\mathbfsf{P} (A \cup B) \leq \mathbfsf{P} (A) + \mathbfsf{P} (B)$.
This is easy to see from the more general identity in (3) above.
This is sometimes referred to as a \t{union bound}, in reference to \textit{bounding} the quantity $\mathbfsf{P} (A \cup B)$.
