
%!name:event_probabilities
%!need:uncertain_events
%!need:probability_distributions
%!need:real_summation

\section*{Why}

Since one and only one outcome occurs, given a distribution on outcomes, we define the probability of a set of outcomes as the sum of their probabilities.

\section*{Definition}

Suppose $p$ is a distribution on a \textit{finite} set of outcomes $\Omega $.
Given an event $E \subset \Omega $, we define the \t{event probability} of $E$ \textit{under} $p$ as the sum of the probabilities of the outcomes in $E$.

\subsection*{Notation}

It is common to define a function $P: \powerset{\Omega } \to \R $ by
    \[
P(A) = \sum_{a \in A} p(a) \quad \text{for all } A \subset \Omega
    \]
We call this function $P$ \t{the event probability function} (or \t{the probability measure}) associated with $p$.
Since it depends on the sample space $\Omega $ and the distribution $p$, we occasionally denote this dependence by $P_{\Omega , p}$ or $P_p$.

%<div data-littype='paragraph'>
% <div data-littype='run'> Given an ‚Äπevent‚Ä∫ $A ‚äÇ Œ©$, many authors ‚Äπdefine‚Ä∫ a function
%    $œÄ: Œ© ‚Üí \set{0, 1}$ so that $œÄ(œâ) = 1$ if and only if $œâ
%    ‚àà A$. </div>
% <div data-littype='run'> In this context, it is common to write $P[œÄ]$ for
%    $ùó£[œÄ^{-1}(1)]$ which is $ùó£(A)$. </div>
%</div>

\subsection*{Example: a single six-sided die}

We model rolling a single die with the set of outcomes $\Omega  = \set{1, \dots , 6}$ as usual.
We $p: \Omega  \to \R $ by $p(\omega ) = 1/6$ for $\omega  = 1, \dots , 6$.
In other words, $p$ is the constant function at value $1/6$ on $\Omega $.
Now, we model the event that the number of pips showing is an even number by the set $E$ defined by $E = \set{2, 4, 6}$.
Given all this modeling, the probability of the event $E$ is
    \[
\textstyle
\sum_{\omega  \in E} p(\omega ) = p(2) + p(4) + p(6) = 1/2.
    \]

\section*{Properties of event probabilities}

As a result of the conditions on $p$, $\mathbfsf{P} $ satisfies
    \begin{enumerate}
      \item $\mathbfsf{P} (A) \geq 0$ for all $A \subset \Omega $;
      \item $\mathbfsf{P} (\Omega ) = 1$ (and $\mathbfsf{P} (\varnothing) = 0$);
      \item $\mathbfsf{P} (A) + \mathbfsf{P} (B)$ for all $A, B \subset \Omega $ and $A \cap  B = \varnothing$.
This statement follows from the more general identity
          \[
\mathbfsf{P} (A \cup B) = \mathbfsf{P} (A) + \mathbfsf{P} (B) - \mathbfsf{P} (A \cap  B)
          \]
for $A, B \subset \Omega $, by using $\mathbfsf{P} (\varnothing) = 0$ of (2) above.
    \end{enumerate}

These three conditions are sometimes called the \t{axioms of probability for finite sets}.
Do all such $\mathbfsf{P} $ satisfying (1)-(3) have a corresponding underlying probability distribution?

In other words, suppose $f: \powerset{\Omega } \to \R $ satisfies (1)-(3).
Define $q: \Omega  \to \R $ by $q(\omega ) = f(\set{\omega })$.
If $f$ satisfies the axioms, then $q$ is a probability distribution.
For this reason we call any function satisfying (i)-(iii) an \t{event probability function} (or a \t{(finite) probability measure}).

\section*{Other basic consequences}

\subsection*{Probability by cases}

Let $\mathbfsf{P} $ be a probability event function.
Suppose $A_1, \dots , A_n$ partition $\Omega $.
Then for any $B \subset \Omega $,
    \[
\textstyle
\mathbfsf{P} (B) = \sum_{i = 1}^{n} \mathbfsf{P} (A_i \cap  B).
    \]
Some authors call this the \t{law of total probability}.

\subsection*{Monotonicity}

If $A \subseteq B$, then $\mathbfsf{P} (A) \leq P(B)$.
This is easy to see by splitting $B$ into $A \cap  B$ and $B - A$, and applying (1) and (3).

\subsection*{Subadditivity}

For $A, B \subset \Omega $, $\mathbfsf{P} (A \cup B) \leq \mathbfsf{P} (A) + \mathbfsf{P} (B)$.
This is easy to see from the more general identity in (3) above.
This is sometimes referred to as a \t{union bound}, in reference to \textit{bounding} the quantity $\mathbfsf{P} (A \cup B)$.
