<!--yaml
name: event_probabilities
needs:
    - uncertain_events
    - outcome_probabilities
    - size_of_direct_products
wikipedia: https://en.wikipedia.org/wiki/Event_(probability_theory)
-->

§ Why ⦉
¶ ⦊
  ‖ Since one and only one outcome occurs, given a distribution
    on outcomes, we define the probability of a set of outcomes
    as the sum of their probabilities. ⦉
⦉

§ Definition ⦉
¶ ⦊
  ‖ Suppose $p$ is a distribution on a ‹finite› set of outcomes
    $Ω$. ⦉

  ‖ Given an event $E ⊂ Ω$, the ❬probability❭ (or ❬chance❭)
    ‹of› $E$ ‹under› $p$ is the sum of the probabilities of the
    outcomes in $E$. ⦉

  ‖ The frequentist interpretation is clear—the probability of an
    event is the proportion of times any of its outcomes will
    occur in the long run. ⦉
⦉

§§ Notation ⦉
¶ ⦊
  ‖ It is common to define a function $P: \powerset{Ω} → 𝗥$ by
    ◇ ⦊
      ‖ P(A) = ∑_{a ∈ A} p(a) \quad \text{for all } A ⊂ Ω ⦉
    ⦉⦉

  ‖ We call this function $P$ ❬the event probability function❭
    (or ❬the probability measure❭) associated with $p$. ⦉

  ‖ Since it depends on the sample space $Ω$ and the
    distribution $p$, we occasionally denote this dependence by
    $P_{Ω, p}$ or $P_p$. ⦉
⦉

¶ ⦊
  ‖ It is tempting, and therefore common to write $P(ω)$ when
    $ω ∈ Ω$ and one intends to denote $P(\set{ω})$, which is
    just $p(ω)$. ⦉

  ‖ It is therefore easy to see that from $P$ we can compute
    $p$, and vice versa. ⦉
⦉

§ Examples ⦉
¶ ⦊
  ‖ ‹Rolling a die.› ⦉

  ‖ We consider the usual model of rolling a fair die (see␣
    <a href='/sheets/outcome_probabilities.html'>
      ‖ Outcome Probabilities ⦉
    </a>
    ). ⦉

  ‖ So we have $Ω = \set{1, …, 6}$ and $p: Ω → [0,1]$
    defined by
    ◇ ⦊
      ‖ p(ω) = 1/6 \quad \text{for all } ω ∈ Ω ⦉
    ⦉⦉

  ‖ Given the model, the probability of the event $E = \set{2,
    4, 6}$ is
    ◇ ⦊
      ‖ \textstyle ⦉

      ‖ P(E) = ∑_{ω ∈ E} p(ω) = p(2) + p(4) + p(6) = 1/2. ⦉
    ⦉⦉
⦉

¶ ⦊
  ‖ ‹Rolling two dice.› ⦉

  ‖ We consider the usual model of rolling two die at once
    (see␣
    <a href='/sheets/outcome_probabilities.html'>
      ‖ Outcome Probabilities ⦉
    </a>
    ). ⦉

  ‖ We take $Ω = \set{(1,1), (1,2) …, (6,5), (6,6)}$ ⦉

  ‖ In other words, $Ω$ is $\set{1,2,3,4,5,6}^2$. ⦉

  ‖ Suppose we model a distribution on outcomes $p: Ω → [0,1]$
    by defining $p(ω) = 1/36$ for each $ω ∈ Ω$. ⦉

  ‖ We use the set $A = \set{(1,4), (2,3), (3, 2), (4,1)}$ for
    the event corresponding to the statement that the sum of the
    two numbers is 5. ⦉

  ‖ In other words,
    ◇ ⦊
      ‖ A = \Set{(ω_1, ω_2) ∈ Ω}{ ω_1 + ω_2 = 5} ⦉
    ⦉⦉

  ‖ The probability of $A$ is
    ◇ ⦊
      ‖ P(A) = p((1,4)) + p((2,3)) + p((3,2)) + p((4,1)) = 4/36
        = 1/9. ⦉
    ⦉⦉

  ‖ Suppose we modify the statement so that $B = \Set{(ω_1,ω_2)
    ∈ Ω}{ω_1 + ω_2 = 12}$. ⦉

  ‖ We have $P(B) = 1/36$. ⦉

  ‖ So we have modeled that the sum of the number of the pips
    on the two die being 12 as less probable than the event
    that the sum of the number of pips being 5. ⦉
⦉

¶ ⦊
  ‖ ‹Flipping a coin three times.› ⦉

  ‖ We model flipping a coin three times with the outcome space
    $Ω = \set{0,1}^3$. ⦉

  ‖ We interpret $(ω_1, ω_2, ω_3) ∈ Ω$ so that $ω_1$ is the
    outcome of the first flip—heads is 1 and tails is 0. ⦉

  ‖ Suppose we model each outcome as equally probable, and so
    put a distribution $p: Ω → [0,1]$ on $Ω$ satisfying $p(ω) =
    1/8$ for every $ω ∈ Ω$. ⦉

  ‖ We want to consider all outcomes in which we see two heads. ⦉

  ‖ Our model is the event $A ⊂ Ω$ defined by
    ◇ ⦊
      ‖ A = \set{(1,1,1), (1,1,0), (1,0,1), (0,1,1)} ⦉
    ⦉⦉

  ‖ Under our chosen distribution, $P(A) = 1/2$. ⦉
⦉

¶ ⦊
  ‖ ‹Flipping a coin $n$ times.› ⦉

  ‖ We model flipping a coin $n$ times with a sample space $Ω
    = \{0,1\}^n$. ⦉

  ‖ Here, we agree to interpret $(ω_1, …, ω_n) ∈ Ω$ so that
    $ω_i$ is 1 if the coin lands heads on the $i$th toss and
    $0$ if it lands tails; $i = 1, …, n$. ⦉

  ‖ The size of $Ω$ is $2^n$, since $\num{\{0,1\}} = 2$. ⦉

  ‖ Suppose we choose a distribution $p: Ω → [0,1]$ so that
    ◇ ⦊
      ‖ p(ω) = \frac{1}{2^n} ⦉
    ⦉⦉

  ‖ Now consider the event $H_k$ defined by
    ◇ ⦊
      ‖ H_k = \Set{ω ∈ Ω}{\num{\Set{i}{ω_i = 1} = 1} = k}. ⦉
    ⦉
    so that it contains all outcomes having a total of $k$
    heads. ⦉

  ‖ Then
    ◇ ⦊
      ‖ P(H_k) = \frac{\num{H_k}}{2^n} = \frac{{n \choose k}}{2^n} ⦉
    ⦉⦉
⦉

§ Properties of event probabilities ⦉
¶ ⦊
  ‖ The properties of $p$ ensure that $P$ satisfies
    𝍫 ⦊
      ‣ ‖ $P(A) ≥ 0$ for all $A ⊂ Ω$; ⦉⦉

      ‣ ‖ $P(Ω) = 1$ (and $P(∅) = 0$); ⦉⦉

      ‣ ‖ $P(A) + P(B)$ for all $A, B ⊂ Ω$ and $A ∩ B = ∅$. ⦉⦉
    ⦉⦉

  ‖ The last statement (3) follows from the more general
    identity—known as the ❬inclusion-exclusion formula❭—
    ◇ ⦊
      ‖ P(A ∪ B) = P(A) + P(B) - P(A ∩ B) ⦉
    ⦉
    for $A, B ⊂ Ω$, by using $𝗣(∅) = 0$ of (2) above. ⦉

  ‖ These three conditions are sometimes called the ❬axioms of
    probability for finite sets❭. ⦉
⦉

¶ ⦊
  ‖ Do all such $P$ satisfying (1)-(3) have a corresponding
    underlying probability distribution? ⦉

  ‖ The answer is easily seen to be yes. ⦉

  ‖ Suppose $f: \powerset{Ω} → 𝗥$ satisfies (1)-(3). ⦉

  ‖ Define $q: Ω → 𝗥$ by $q(ω) = f(\set{ω})$. ⦉

  ‖ If $f$ satisfies the axioms, then $q$ is a probability
    distribution. ⦉

  ‖ For this reason we call any function satisfying (i)-(iii) an
    ❬event probability function❭ (or a ❬(finite) probability
    measure❭). ⦉
⦉

§ Other basic consequences ⦉
¶ ⦊
  ‖ ‹Disjoint events.› ⦉

  ‖ Two events $A$ and $B$ are ❬disjoint❭ or ❬mutually
    exclusive❭ if $A ∩ B = ∅$. ⦉

  ‖ Likwise, a list of events $A_1, …, A_n$ are ❬disjoint❭ or
    ❬mutually exclusive❭ if $A_i ∩ A_j = ∅$ for all $i ≠ j$,
    $i,j ∈ \set{1, …, n}$. ⦉

  ‖ A direct consequence of (3) above is
    ◇ ⦊
      ‖ \textstyle ⦉

      ‖ P(∪_{i =1 }^{n} A_i) = ∑_{i = 1}^{n} P(A_i) ⦉
    ⦉⦉
⦉

¶ ⦊
  ‖ ‹Probability by cases.› ⦉

  ‖ Suppose $A_1, …, A_n$ partition $Ω$. ⦉

  ‖ Then for any $B ⊂ Ω$,
    ◇ ⦊
      ‖ \textstyle ⦉

      ‖ P(B) = ∑_{i = 1}^{n} P(A_i ∩ B). ⦉
    ⦉⦉

  ‖ Some authors call this the ❬law of total probability❭. ⦉

  ‖ This is easy to see by using the distributive laws of set
    algebra (see␣
    <a href='/sheets/set_unions_and_intersections.html'>
      ‖ Set Unions and Intersections ⦉
    </a>
    ). ⦉

  ‖ A simple consequence is that for any $A$, $B$
    ◇ ⦊
      ‖ P(B) = P(B ∩ A) + P(B ∩ (Ω - A)) ⦉
    ⦉
    since $A, Ω - A$ partition $Ω$. ⦉
⦉

¶ ⦊
  ‖ ‹Monotonicity.› ⦉

  ‖ If $A ⊆ B$, then $P(A) ≤ P(B)$. ⦉

  ‖ This is easy to see by splitting $B$ into $A ∩ B$ and $B
    - A$, and applying (1) and (3). ⦉
⦉

¶ ⦊
  ‖ ‹Subadditivity.› ⦉

  ‖ For $A, B ⊂ Ω$, $P(A ∪ B) ≤ P(A) + P(B)$. ⦉

  ‖ This is easy to see from the more general identity in (3)
    above. ⦉

  ‖ This is sometimes referred to as a ❬union bound❭, in
    reference to ‹bounding› the quantity $P(A ∪ B)$. ⦉
⦉

<!--macros.tex
\newcommand{\PE}{\mathbfsf{P}}
-->