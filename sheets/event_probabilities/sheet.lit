<!--
!name:event_probabilities
!need:probability_distributions
-->

Â§ Why â¦‰
Â¶ â¦Š
  â€– Since one and only one outcome occurs, given a distribution
    on outcomes, we define the probability of a set of outcomes
    as the sum of their probabilities. â¦‰
â¦‰

Â§ Definition â¦‰
Â¶ â¦Š
  â€– Suppose $p$ is a distribution on $Î©$. â¦‰

  â€– For any event $A âŠ‚ Î©$, we call the value $âˆ‘_{a âˆˆ A}
    p(a)$ the â¬event probabilityâ­. â¦‰

  â€– We refer to the probability â€¹ofâ€º $A$. â¦‰

  â€– The probability of $A$ is the sum of the probabilities of
    its outcomes. â¦‰
â¦‰

Â§Â§ Notation â¦‰
Â¶ â¦Š
  â€– We can define a function $ğ—£: \powerset{Î©} â†’ ğ—¥$ by $ğ—£(A) =
    âˆ‘_{a âˆˆ A} p(a)$. â¦‰

  â€– We call $ğ—£$ â¬the event probability functionâ­ (or â¬the
    probability measureâ­) induced by $p$. â¦‰

  â€– Since $ğ—£$ depends on the sample space $Î©$ and the
    distribution $p$, we ocassionally denote this dependence by
    $ğ—£_{Î©, p}$ or $ğ—£_p$. â¦‰
â¦‰

Â¶ â¦Š
  â€– Many authors associate an event $A âŠ‚ Î©$ with a function
    $Ï€: Î© â†’ \set{0, 1}$ so that $A = \Set*{Ï‰ âˆˆ Î©}{Ï€(Ï‰) = 1}$. â¦‰

  â€– In this context, it is common to write $ğ—£[Ï€(Ï‰)]$ for $ğ—£(A)$. â¦‰
â¦‰

Â§ Example: die â¦‰
Â¶ â¦Š
  â€– Define $p: \set{1, â€¦, 6} â†’ ğ—¥$ by $p(Ï‰) = 1/6$ for $Ï‰ =
    1, â€¦, 6$. â¦‰

  â€– Define the event $E = \set{2, 4, 6}$. â¦‰

  â€– Then
    â—‡ â¦Š
      â€– \textstyle â¦‰

      â€– ğ—£(E) = âˆ‘_{Ï‰ âˆˆ E} p(Ï‰) = p(2) + p(4) + p(6) = 1/2. â¦‰
    â¦‰â¦‰
â¦‰

Â§ Properties of $ğ—£$ â¦‰
Â¶ â¦Š
  â€– As a result of the conditions on $p$, $ğ—£$ satisfies
    ğ« â¦Š
      â€£ $ğ—£(A) â‰¥ 0$ for all $A âŠ‚ Î©$; â¦‰

      â€£ $ğ—£(Î©) = 1$ (and $ğ—£(âˆ…) = 0$); â¦‰

      â€£ â€– $ğ—£(A) + ğ—£(B)$ for all $A, B âŠ‚ Î©$ and $A âˆ© B = âˆ…$. â¦‰

        â€– This statement follows from the more general identity
          â—‡ â¦Š
            â€– ğ—£(A âˆª B) = ğ—£(A) + ğ—£(B) - ğ—£(A âˆ© B) â¦‰
          â¦‰
          for $A, B âŠ‚ Î©$, by using $ğ—£(âˆ…) = 0$ of (2) above. â¦‰â¦‰
    â¦‰â¦‰
â¦‰

Â¶ â¦Š
  â€– These three conditions are sometimes called the â¬axioms of
    probability for finite setsâ­. â¦‰

  â€– Do all such $ğ—£$ satisfying (1)-(3) have a corresponding
    underlying probability distribution? â¦‰
â¦‰

Â¶ â¦Š
  â€– In other words, suppose $f: \powerset{Î©} â†’ ğ—¥$ satifies
    (1)-(3). â¦‰

  â€– Define $q: Î© â†’ ğ—¥$ by $q(Ï‰) = f(\set{Ï‰})$. â¦‰

  â€– If $f$ satisfies the axioms, then $q$ is a probability
    distribution. â¦‰

  â€– For this reason we call any function satisfying (i)-(iii) an
    â¬event probability functionâ­ (or a â¬(finite) probability
    measureâ­). â¦‰
â¦‰

Â§ Other basic consequences â¦‰

Â§Â§ Probability by cases â¦‰
Â¶ â¦Š
  â€– Let $ğ—£$ be a probability event function. â¦‰

  â€– Suppose $A_1, â€¦, A_n$ partition $Î©$. â¦‰

  â€– Then for any $B âŠ‚ Î©$,
    â—‡ â¦Š
      â€– \textstyle â¦‰

      â€– ğ—£(B) = âˆ‘_{i = 1}^{n} ğ—£(A_i âˆ© B). â¦‰
    â¦‰â¦‰

  â€– Some authors call this the â¬law of total probabilityâ­. â¦‰
â¦‰

Â§Â§ Monotonicity â¦‰
Â¶ â¦Š
  â€– If $A âŠ† B$, then $ğ—£(A) â‰¤ P(B)$. â¦‰

  â€– This is easy to see by splitting $B$ into $A âˆ© B$ and $B
    \setminus A$, and applying (1) and (3). â¦‰
â¦‰

Â§Â§ Subadditivity â¦‰
Â¶ â¦Š
  â€– For $A, B âŠ‚ Î©$, $ğ—£(A âˆª B) â‰¤ ğ—£(A) + ğ—£(B)$. â¦‰

  â€– This is easy to see from the more general identity in (3)
    above. â¦‰

  â€– This is sometimes referred to as a â¬union boundâ­, in
    reference to â€¹boundingâ€º the quantity $ğ—£(A âˆª B)$. â¦‰
â¦‰
