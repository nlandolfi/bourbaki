<!--
!name:event_probabilities
!need:uncertain_events
!need:probability_distributions
!need:real_summation
-->

Â§ Why â¦‰
Â¶ â¦Š
  â€– Since one and only one outcome occurs, given a distribution
    on outcomes, we define the probability of a set of outcomes
    as the sum of their probabilities. â¦‰
â¦‰

Â§ Definition â¦‰
Â¶ â¦Š
  â€– Suppose $p$ is a distribution on a â€¹finiteâ€º set of outcomes
    $Î©$. â¦‰

  â€– Given an event $E âŠ‚ Î©$, the â¬event probabilityâ­ of $E$
    â€¹underâ€º $p$ as the sum of the probabilities of the outcomes
    in $E$. â¦‰
â¦‰

Â§Â§ Notation â¦‰
Â¶ â¦Š
  â€– It is common to define a function $P: \powerset{Î©} â†’ ğ—¥$ by
    â—‡ â¦Š
      â€– P(A) = âˆ‘_{a âˆˆ A} p(a) \quad \text{for all } A âŠ‚ Î© â¦‰
    â¦‰â¦‰

  â€– We call this function $P$ â¬the event probability functionâ­
    (or â¬the probability measureâ­) associated with $p$. â¦‰

  â€– Since it depends on the sample space $Î©$ and the
    distribution $p$, we occasionally denote this dependence by
    $P_{Î©, p}$ or $P_p$. â¦‰
â¦‰

Â§Â§ Example: a single six-sided die â¦‰
Â¶ â¦Š
  â€– We model rolling a single die with the set of outcomes $Î©
    = \set{1, â€¦, 6}$ as usual. â¦‰

  â€– We $p: Î© â†’ ğ—¥$ by $p(Ï‰) = 1/6$ for $Ï‰ = 1, â€¦, 6$. â¦‰

  â€– In other words, $p$ is the constant function at value $1/6$
    on $Î©$. â¦‰

  â€– Now, we model the event that the number of pips showing is
    an even number by the set $E$ defined by $E = \set{2, 4,
    6}$. â¦‰

  â€– Given all this modeling, the probability of the event $E$
    is
    â—‡ â¦Š
      â€– \textstyle â¦‰

      â€– âˆ‘_{Ï‰ âˆˆ E} p(Ï‰) = p(2) + p(4) + p(6) = 1/2. â¦‰
    â¦‰â¦‰
â¦‰

Â§ Properties of event probabilities â¦‰
Â¶ â¦Š
  â€– As a result of the conditions on $p$, $ğ—£$ satisfies
    ğ« â¦Š
      â€£ $ğ—£(A) â‰¥ 0$ for all $A âŠ‚ Î©$; â¦‰

      â€£ $ğ—£(Î©) = 1$ (and $ğ—£(âˆ…) = 0$); â¦‰

      â€£ â€– $ğ—£(A) + ğ—£(B)$ for all $A, B âŠ‚ Î©$ and $A âˆ© B = âˆ…$. â¦‰

        â€– This statement follows from the more general identity
          â—‡ â¦Š
            â€– ğ—£(A âˆª B) = ğ—£(A) + ğ—£(B) - ğ—£(A âˆ© B) â¦‰
          â¦‰
          for $A, B âŠ‚ Î©$, by using $ğ—£(âˆ…) = 0$ of (2) above. â¦‰â¦‰
    â¦‰â¦‰
â¦‰

Â¶ â¦Š
  â€– These three conditions are sometimes called the â¬axioms of
    probability for finite setsâ­. â¦‰

  â€– Do all such $ğ—£$ satisfying (1)-(3) have a corresponding
    underlying probability distribution? â¦‰
â¦‰

Â¶ â¦Š
  â€– In other words, suppose $f: \powerset{Î©} â†’ ğ—¥$ satisfies
    (1)-(3). â¦‰

  â€– Define $q: Î© â†’ ğ—¥$ by $q(Ï‰) = f(\set{Ï‰})$. â¦‰

  â€– If $f$ satisfies the axioms, then $q$ is a probability
    distribution. â¦‰

  â€– For this reason we call any function satisfying (i)-(iii) an
    â¬event probability functionâ­ (or a â¬(finite) probability
    measureâ­). â¦‰
â¦‰

Â§ Other basic consequences â¦‰

Â§Â§ Probability by cases â¦‰
Â¶ â¦Š
  â€– Let $ğ—£$ be a probability event function. â¦‰

  â€– Suppose $A_1, â€¦, A_n$ partition $Î©$. â¦‰

  â€– Then for any $B âŠ‚ Î©$,
    â—‡ â¦Š
      â€– \textstyle â¦‰

      â€– ğ—£(B) = âˆ‘_{i = 1}^{n} ğ—£(A_i âˆ© B). â¦‰
    â¦‰â¦‰

  â€– Some authors call this the â¬law of total probabilityâ­. â¦‰
â¦‰

Â§Â§ Monotonicity â¦‰
Â¶ â¦Š
  â€– If $A âŠ† B$, then $ğ—£(A) â‰¤ P(B)$. â¦‰

  â€– This is easy to see by splitting $B$ into $A âˆ© B$ and $B
    - A$, and applying (1) and (3). â¦‰
â¦‰

Â§Â§ Subadditivity â¦‰
Â¶ â¦Š
  â€– For $A, B âŠ‚ Î©$, $ğ—£(A âˆª B) â‰¤ ğ—£(A) + ğ—£(B)$. â¦‰

  â€– This is easy to see from the more general identity in (3)
    above. â¦‰

  â€– This is sometimes referred to as a â¬union boundâ­, in
    reference to â€¹boundingâ€º the quantity $ğ—£(A âˆª B)$. â¦‰
â¦‰
