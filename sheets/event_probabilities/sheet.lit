<!--
!name:event_probabilities
!need:uncertain_events
!need:probability_distributions
!need:real_summation
-->

§ Why ⦉
¶ ⦊
  ‖ Since one and only one outcome occurs, given a distribution
    on outcomes, we define the probability of a set of outcomes
    as the sum of their probabilities. ⦉
⦉

§ Definition ⦉
¶ ⦊
  ‖ Suppose $p$ is a distribution on a ‹finite› set of outcomes
    $Ω$. ⦉

  ‖ Given an event $E ⊂ Ω$, the ❬event probability❭ of $E$
    ‹under› $p$ as the sum of the probabilities of the outcomes
    in $E$. ⦉
⦉

§§ Notation ⦉
¶ ⦊
  ‖ It is common to define a function $P: \powerset{Ω} → 𝗥$ by
    ◇ ⦊
      ‖ P(A) = ∑_{a ∈ A} p(a) \quad \text{for all } A ⊂ Ω ⦉
    ⦉⦉

  ‖ We call this function $P$ ❬the event probability function❭
    (or ❬the probability measure❭) associated with $p$. ⦉

  ‖ Since it depends on the sample space $Ω$ and the
    distribution $p$, we occasionally denote this dependence by
    $P_{Ω, p}$ or $P_p$. ⦉
⦉

§§ Example: a single six-sided die ⦉
¶ ⦊
  ‖ We model rolling a single die with the set of outcomes $Ω
    = \set{1, …, 6}$ as usual. ⦉

  ‖ We $p: Ω → 𝗥$ by $p(ω) = 1/6$ for $ω = 1, …, 6$. ⦉

  ‖ In other words, $p$ is the constant function at value $1/6$
    on $Ω$. ⦉

  ‖ Now, we model the event that the number of pips showing is
    an even number by the set $E$ defined by $E = \set{2, 4,
    6}$. ⦉

  ‖ Given all this modeling, the probability of the event $E$
    is
    ◇ ⦊
      ‖ \textstyle ⦉

      ‖ ∑_{ω ∈ E} p(ω) = p(2) + p(4) + p(6) = 1/2. ⦉
    ⦉⦉
⦉

§ Properties of event probabilities ⦉
¶ ⦊
  ‖ As a result of the conditions on $p$, $𝗣$ satisfies
    𝍫 ⦊
      ‣ $𝗣(A) ≥ 0$ for all $A ⊂ Ω$; ⦉

      ‣ $𝗣(Ω) = 1$ (and $𝗣(∅) = 0$); ⦉

      ‣ ‖ $𝗣(A) + 𝗣(B)$ for all $A, B ⊂ Ω$ and $A ∩ B = ∅$. ⦉

        ‖ This statement follows from the more general identity
          ◇ ⦊
            ‖ 𝗣(A ∪ B) = 𝗣(A) + 𝗣(B) - 𝗣(A ∩ B) ⦉
          ⦉
          for $A, B ⊂ Ω$, by using $𝗣(∅) = 0$ of (2) above. ⦉⦉
    ⦉⦉
⦉

¶ ⦊
  ‖ These three conditions are sometimes called the ❬axioms of
    probability for finite sets❭. ⦉

  ‖ Do all such $𝗣$ satisfying (1)-(3) have a corresponding
    underlying probability distribution? ⦉
⦉

¶ ⦊
  ‖ In other words, suppose $f: \powerset{Ω} → 𝗥$ satisfies
    (1)-(3). ⦉

  ‖ Define $q: Ω → 𝗥$ by $q(ω) = f(\set{ω})$. ⦉

  ‖ If $f$ satisfies the axioms, then $q$ is a probability
    distribution. ⦉

  ‖ For this reason we call any function satisfying (i)-(iii) an
    ❬event probability function❭ (or a ❬(finite) probability
    measure❭). ⦉
⦉

§ Other basic consequences ⦉

§§ Probability by cases ⦉
¶ ⦊
  ‖ Let $𝗣$ be a probability event function. ⦉

  ‖ Suppose $A_1, …, A_n$ partition $Ω$. ⦉

  ‖ Then for any $B ⊂ Ω$,
    ◇ ⦊
      ‖ \textstyle ⦉

      ‖ 𝗣(B) = ∑_{i = 1}^{n} 𝗣(A_i ∩ B). ⦉
    ⦉⦉

  ‖ Some authors call this the ❬law of total probability❭. ⦉
⦉

§§ Monotonicity ⦉
¶ ⦊
  ‖ If $A ⊆ B$, then $𝗣(A) ≤ P(B)$. ⦉

  ‖ This is easy to see by splitting $B$ into $A ∩ B$ and $B
    - A$, and applying (1) and (3). ⦉
⦉

§§ Subadditivity ⦉
¶ ⦊
  ‖ For $A, B ⊂ Ω$, $𝗣(A ∪ B) ≤ 𝗣(A) + 𝗣(B)$. ⦉

  ‖ This is easy to see from the more general identity in (3)
    above. ⦉

  ‖ This is sometimes referred to as a ❬union bound❭, in
    reference to ‹bounding› the quantity $𝗣(A ∪ B)$. ⦉
⦉
