<!--yaml
name: event_probabilities
needs:
    - uncertain_events
    - outcome_probabilities
    - size_of_direct_products
wikipedia: https://en.wikipedia.org/wiki/Event_(probability_theory)
-->

Â§ Why â¦‰
Â¶ â¦Š
  â€– Since one and only one outcome occurs, given a distribution
    on outcomes, we define the probability of a set of outcomes
    as the sum of their probabilities. â¦‰
â¦‰

Â§ Definition â¦‰
Â¶ â¦Š
  â€– Suppose $p$ is a distribution on a â€¹finiteâ€º set of outcomes
    $Î©$. â¦‰

  â€– Given an event $E âŠ‚ Î©$, the â¬probabilityâ­ (or â¬chanceâ­)
    â€¹ofâ€º $E$ â€¹underâ€º $p$ is the sum of the probabilities of the
    outcomes in $E$. â¦‰

  â€– The frequentist interpretation is clearâ€”the probability of an
    event is the proportion of times any of its outcomes will
    occur in the long run. â¦‰
â¦‰

Â§Â§ Notation â¦‰
Â¶ â¦Š
  â€– It is common to define a function $P: \powerset{Î©} â†’ ğ—¥$ by
    â—‡ â¦Š
      â€– P(A) = âˆ‘_{a âˆˆ A} p(a) \quad \text{for all } A âŠ‚ Î© â¦‰
    â¦‰â¦‰

  â€– We call this function $P$ â¬the event probability functionâ­
    (or â¬the probability measureâ­) associated with $p$. â¦‰

  â€– Since it depends on the sample space $Î©$ and the
    distribution $p$, we occasionally denote this dependence by
    $P_{Î©, p}$ or $P_p$. â¦‰
â¦‰

Â¶ â¦Š
  â€– It is tempting, and therefore common to write $P(Ï‰)$ when
    $Ï‰ âˆˆ Î©$ and one intends to denote $P(\set{Ï‰})$, which is
    just $p(Ï‰)$. â¦‰

  â€– It is therefore easy to see that from $P$ we can compute
    $p$, and vice versa. â¦‰
â¦‰

Â§ Examples â¦‰
Â¶ â¦Š
  â€– â€¹Rolling a die.â€º â¦‰

  â€– We consider the usual model of rolling a fair die (seeâ£
    <a href='/sheets/outcome_probabilities.html'>
      â€– Outcome Probabilities â¦‰
    </a>
    ). â¦‰

  â€– So we have $Î© = \set{1, â€¦, 6}$ and $p: Î© â†’ [0,1]$
    defined by
    â—‡ â¦Š
      â€– p(Ï‰) = 1/6 \quad \text{for all } Ï‰ âˆˆ Î© â¦‰
    â¦‰â¦‰

  â€– Given the model, the probability of the event $E = \set{2,
    4, 6}$ is
    â—‡ â¦Š
      â€– \textstyle â¦‰

      â€– P(E) = âˆ‘_{Ï‰ âˆˆ E} p(Ï‰) = p(2) + p(4) + p(6) = 1/2. â¦‰
    â¦‰â¦‰
â¦‰

Â¶ â¦Š
  â€– â€¹Rolling two dice.â€º â¦‰

  â€– We consider the usual model of rolling two die at once
    (seeâ£
    <a href='/sheets/outcome_probabilities.html'>
      â€– Outcome Probabilities â¦‰
    </a>
    ). â¦‰

  â€– We take $Î© = \set{(1,1), (1,2) â€¦, (6,5), (6,6)}$ â¦‰

  â€– In other words, $Î©$ is $\set{1,2,3,4,5,6}^2$. â¦‰

  â€– Suppose we model a distribution on outcomes $p: Î© â†’ [0,1]$
    by defining $p(Ï‰) = 1/36$ for each $Ï‰ âˆˆ Î©$. â¦‰

  â€– We use the set $A = \set{(1,4), (2,3), (3, 2), (4,1)}$ for
    the event corresponding to the statement that the sum of the
    two numbers is 5. â¦‰

  â€– In other words,
    â—‡ â¦Š
      â€– A = \Set{(Ï‰_1, Ï‰_2) âˆˆ Î©}{ Ï‰_1 + Ï‰_2 = 5} â¦‰
    â¦‰â¦‰

  â€– The probability of $A$ is
    â—‡ â¦Š
      â€– P(A) = p((1,4)) + p((2,3)) + p((3,2)) + p((4,1)) = 4/36
        = 1/9. â¦‰
    â¦‰â¦‰

  â€– Suppose we modify the statement so that $B = \Set{(Ï‰_1,Ï‰_2)
    âˆˆ Î©}{Ï‰_1 + Ï‰_2 = 12}$. â¦‰

  â€– We have $P(B) = 1/36$. â¦‰

  â€– So we have modeled that the sum of the number of the pips
    on the two die being 12 as less probable than the event
    that the sum of the number of pips being 5. â¦‰
â¦‰

Â¶ â¦Š
  â€– â€¹Flipping a coin three times.â€º â¦‰

  â€– We model flipping a coin three times with the outcome space
    $Î© = \set{0,1}^3$. â¦‰

  â€– We interpret $(Ï‰_1, Ï‰_2, Ï‰_3) âˆˆ Î©$ so that $Ï‰_1$ is the
    outcome of the first flipâ€”heads is 1 and tails is 0. â¦‰

  â€– Suppose we model each outcome as equally probable, and so
    put a distribution $p: Î© â†’ [0,1]$ on $Î©$ satisfying $p(Ï‰) =
    1/8$ for every $Ï‰ âˆˆ Î©$. â¦‰

  â€– We want to consider all outcomes in which we see two heads. â¦‰

  â€– Our model is the event $A âŠ‚ Î©$ defined by
    â—‡ â¦Š
      â€– A = \set{(1,1,1), (1,1,0), (1,0,1), (0,1,1)} â¦‰
    â¦‰â¦‰

  â€– Under our chosen distribution, $P(A) = 1/2$. â¦‰
â¦‰

Â¶ â¦Š
  â€– â€¹Flipping a coin $n$ times.â€º â¦‰

  â€– We model flipping a coin $n$ times with a sample space $Î©
    = \{0,1\}^n$. â¦‰

  â€– Here, we agree to interpret $(Ï‰_1, â€¦, Ï‰_n) âˆˆ Î©$ so that
    $Ï‰_i$ is 1 if the coin lands heads on the $i$th toss and
    $0$ if it lands tails; $i = 1, â€¦, n$. â¦‰

  â€– The size of $Î©$ is $2^n$, since $\num{\{0,1\}} = 2$. â¦‰

  â€– Suppose we choose a distribution $p: Î© â†’ [0,1]$ so that
    â—‡ â¦Š
      â€– p(Ï‰) = \frac{1}{2^n} â¦‰
    â¦‰â¦‰

  â€– Now consider the event $H_k$ defined by
    â—‡ â¦Š
      â€– H_k = \Set{Ï‰ âˆˆ Î©}{\num{\Set{i}{Ï‰_i = 1} = 1} = k}. â¦‰
    â¦‰
    so that it contains all outcomes having a total of $k$
    heads. â¦‰

  â€– Then
    â—‡ â¦Š
      â€– P(H_k) = \frac{\num{H_k}}{2^n} = \frac{{n \choose k}}{2^n} â¦‰
    â¦‰â¦‰
â¦‰

Â§ Properties of event probabilities â¦‰
Â¶ â¦Š
  â€– The properties of $p$ ensure that $P$ satisfies
    ğ« â¦Š
      â€£ â€– $P(A) â‰¥ 0$ for all $A âŠ‚ Î©$; â¦‰â¦‰

      â€£ â€– $P(Î©) = 1$ (and $P(âˆ…) = 0$); â¦‰â¦‰

      â€£ â€– $P(A) + P(B)$ for all $A, B âŠ‚ Î©$ and $A âˆ© B = âˆ…$. â¦‰â¦‰
    â¦‰â¦‰

  â€– The last statement (3) follows from the more general
    identityâ€”known as the â¬inclusion-exclusion formulaâ­â€”
    â—‡ â¦Š
      â€– P(A âˆª B) = P(A) + P(B) - P(A âˆ© B) â¦‰
    â¦‰
    for $A, B âŠ‚ Î©$, by using $ğ—£(âˆ…) = 0$ of (2) above. â¦‰

  â€– These three conditions are sometimes called the â¬axioms of
    probability for finite setsâ­. â¦‰
â¦‰

Â¶ â¦Š
  â€– Do all such $P$ satisfying (1)-(3) have a corresponding
    underlying probability distribution? â¦‰

  â€– The answer is easily seen to be yes. â¦‰

  â€– Suppose $f: \powerset{Î©} â†’ ğ—¥$ satisfies (1)-(3). â¦‰

  â€– Define $q: Î© â†’ ğ—¥$ by $q(Ï‰) = f(\set{Ï‰})$. â¦‰

  â€– If $f$ satisfies the axioms, then $q$ is a probability
    distribution. â¦‰

  â€– For this reason we call any function satisfying (i)-(iii) an
    â¬event probability functionâ­ (or a â¬(finite) probability
    measureâ­). â¦‰
â¦‰

Â§ Other basic consequences â¦‰
Â¶ â¦Š
  â€– â€¹Disjoint events.â€º â¦‰

  â€– Two events $A$ and $B$ are â¬disjointâ­ or â¬mutually
    exclusiveâ­ if $A âˆ© B = âˆ…$. â¦‰

  â€– Likwise, a list of events $A_1, â€¦, A_n$ are â¬disjointâ­ or
    â¬mutually exclusiveâ­ if $A_i âˆ© A_j = âˆ…$ for all $i â‰  j$,
    $i,j âˆˆ \set{1, â€¦, n}$. â¦‰

  â€– A direct consequence of (3) above is
    â—‡ â¦Š
      â€– \textstyle â¦‰

      â€– P(âˆª_{i =1 }^{n} A_i) = âˆ‘_{i = 1}^{n} P(A_i) â¦‰
    â¦‰â¦‰
â¦‰

Â¶ â¦Š
  â€– â€¹Probability by cases.â€º â¦‰

  â€– Suppose $A_1, â€¦, A_n$ partition $Î©$. â¦‰

  â€– Then for any $B âŠ‚ Î©$,
    â—‡ â¦Š
      â€– \textstyle â¦‰

      â€– P(B) = âˆ‘_{i = 1}^{n} P(A_i âˆ© B). â¦‰
    â¦‰â¦‰

  â€– Some authors call this the â¬law of total probabilityâ­. â¦‰

  â€– This is easy to see by using the distributive laws of set
    algebra (seeâ£
    <a href='/sheets/set_unions_and_intersections.html'>
      â€– Set Unions and Intersections â¦‰
    </a>
    ). â¦‰

  â€– A simple consequence is that for any $A$, $B$
    â—‡ â¦Š
      â€– P(B) = P(B âˆ© A) + P(B âˆ© (Î© - A)) â¦‰
    â¦‰
    since $A, Î© - A$ partition $Î©$. â¦‰
â¦‰

Â¶ â¦Š
  â€– â€¹Monotonicity.â€º â¦‰

  â€– If $A âŠ† B$, then $P(A) â‰¤ P(B)$. â¦‰

  â€– This is easy to see by splitting $B$ into $A âˆ© B$ and $B
    - A$, and applying (1) and (3). â¦‰
â¦‰

Â¶ â¦Š
  â€– â€¹Subadditivity.â€º â¦‰

  â€– For $A, B âŠ‚ Î©$, $P(A âˆª B) â‰¤ P(A) + P(B)$. â¦‰

  â€– This is easy to see from the more general identity in (3)
    above. â¦‰

  â€– This is sometimes referred to as a â¬union boundâ­, in
    reference to â€¹boundingâ€º the quantity $P(A âˆª B)$. â¦‰
â¦‰

<!--macros.tex
\newcommand{\PE}{\mathbfsf{P}}
-->