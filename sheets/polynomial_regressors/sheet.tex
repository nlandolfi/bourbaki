%!name:polynomial_regressors
%!need:polynomials
%!need:feature_maps

\ssection{Why}

A simple example of an embedding.\footnote{Future editions will expand, or perhaps collapse this sheet.}

\ssection{Definition}

Fix $d \in \N$.
A \t{polynomial feature map} of degree $d$ is a function $\phi: \R \to \R^d$ with
\[
  \phi(x) = \transpose{\pmat{1 & x^2 & \cdots & x^d}}.
\]
For $x \in \R$, we call $\phi(x)$ the \t{polynomial embedding} of $x$.

A \t{polynomial regressor} is a least squares linear predictor using a polynomial feature embedding (of any degree, but to be precise one must specify the degree).
The task of consructing a linear predictor is often referred to as \t{polynomial regression}.

Given a dataset of paired records $(x^1, y^1), \dots, (x^2, y^2) \in \R^2$, one can construct a predictor $g: \R \to \R$ for $y$ by embedding the dataset $(\phi(x^1), \dots, \phi(x^n))$ and finding the least squares linear regressor $f: \R^d \to \R$ for $y$.
One defines the predictor $g: \R \to \R$ by $g(\phi(x))$.


\blankpage
