<!--yaml
name: polynomial_regressors
needs:
    - polynomials
    - feature_maps
-->

Â§ Why â¦‰
Â¶ â¦Š
  â€– A simple example of an embedding.
    â€  â¦Š
      â€– Future editions will expand, or perhaps collapse this
        sheet. â¦‰
    â¦‰â¦‰
â¦‰

Â§ Definition â¦‰
Â¶ â¦Š
  â€– Fix $d âˆˆ ğ—¡$. â¦‰

  â€– A â¬polynomial feature mapâ­ of degree $d$ is a function $Ï†:
    ğ—¥ â†’ ğ—¥^d$ with
    â—‡ â¦Š
      â€– Ï†(x) = \pmat{1 ï¼† x^2 ï¼† â‹¯ ï¼† x^d}^âŠ¤. â¦‰
    â¦‰â¦‰

  â€– For $x âˆˆ ğ—¥$, we call $Ï†(x)$ the â¬polynomial embeddingâ­ of
    $x$. â¦‰
â¦‰

Â¶ â¦Š
  â€– A â¬polynomial regressorâ­ is a least squares linear predictor
    using a polynomial feature embedding (of any degree, but to
    be precise one must specify the degree). â¦‰

  â€– The task of consructing a linear predictor is often referred
    to as â¬polynomial regressionâ­. â¦‰
â¦‰

Â¶ â¦Š
  â€– Given a dataset of paired records $(x^1, y^1), â€¦, (x^n,
    y^n) âˆˆ ğ—¥^2$, one can construct a predictor $g: ğ—¥ â†’ ğ—¥$ for
    $y$ by embedding the dataset $(Ï†(x^1), â€¦, Ï†(x^n))$ and
    finding the least squares linear regressor $f: ğ—¥^d â†’ ğ—¥$ for
    $y$. â¦‰

  â€– One defines the predictor $g: ğ—¥ â†’ ğ—¥$ by $g(Ï†(x))$. â¦‰
â¦‰

<tex>
  â€– \blankpage â¦‰
</tex>