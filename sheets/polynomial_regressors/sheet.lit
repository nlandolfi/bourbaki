<!--yaml
name: polynomial_regressors
needs:
    - polynomials
    - feature_maps
-->

§ Why ⦉
¶ ⦊
  ‖ A simple example of an embedding.
    † ⦊
      ‖ Future editions will expand, or perhaps collapse this
        sheet. ⦉
    ⦉⦉
⦉

§ Definition ⦉
¶ ⦊
  ‖ Fix $d ∈ 𝗡$. ⦉

  ‖ A ❬polynomial feature map❭ of degree $d$ is a function $φ:
    𝗥 → 𝗥^d$ with
    ◇ ⦊
      ‖ φ(x) = \pmat{1 ＆ x^2 ＆ ⋯ ＆ x^d}^⊤. ⦉
    ⦉⦉

  ‖ For $x ∈ 𝗥$, we call $φ(x)$ the ❬polynomial embedding❭ of
    $x$. ⦉
⦉

¶ ⦊
  ‖ A ❬polynomial regressor❭ is a least squares linear predictor
    using a polynomial feature embedding (of any degree, but to
    be precise one must specify the degree). ⦉

  ‖ The task of consructing a linear predictor is often referred
    to as ❬polynomial regression❭. ⦉
⦉

¶ ⦊
  ‖ Given a dataset of paired records $(x^1, y^1), …, (x^n,
    y^n) ∈ 𝗥^2$, one can construct a predictor $g: 𝗥 → 𝗥$ for
    $y$ by embedding the dataset $(φ(x^1), …, φ(x^n))$ and
    finding the least squares linear regressor $f: 𝗥^d → 𝗥$ for
    $y$. ⦉

  ‖ One defines the predictor $g: 𝗥 → 𝗥$ by $g(φ(x))$. ⦉
⦉

<tex>
  ‖ \blankpage ⦉
</tex>