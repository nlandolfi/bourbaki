<!--yaml
name: probably_approximately_correct_inductors
needs:
    - supervised_probabilistic_data_models
refs:
    - shai_shalev-schwartz2014understanding/2
-->

Â§ Why â¦‰
Â¶ â¦Š
  â€– We want to talk about an inductor's error under a
    probabilistic supervised data model. â¦‰
â¦‰

Â§ Definition â¦‰
Â¶ â¦Š
  â€– Let $(X, ğ’³, Î¼)$ and $f: X â†’ Y$ be a probabilistic
    supervised data model. â¦‰

  â€– Let $â„‹ âŠ‚ (X â†’ Y)$ be a hypothesis class. â¦‰
â¦‰

Â¶ â¦Š
  â€– Let $Ïµ,Î´ âˆˆ (0, 1)$. â¦‰

  â€– An inductor $A: (X Ã— Y)^n â†’ â„‹$ is â¬$(Ïµ,Î´)$-probably
    approximately correctâ­ for $Î¼$ and $f$ if
    â—‡ â¦Š
      â€– Î¼^{n}\left[ â¦‰

      â€– \underset{Î¼, f}{\mathword{error}}\left( â¦‰

      â€– A((x_i, f(x_i))_{i = 1}^{n}) â¦‰

      â€– \right) â‰¤ Ïµ â¦‰

      â€– \right] â‰¥ 1 - Î´. â¦‰
    â¦‰â¦‰
â¦‰

Â¶ â¦Š
  â€– To show that an algorithm is probably approximately correct,
    we bound by $Î´$ the probability of the inductor producing a
    predictor which exceeds $Ïµ$ error. â¦‰
â¦‰

Â¶ â¦Š
  â€– A hypothesis class $â„‹$ is â¬probably approximately correct
    learnableâ­ (or â¬PAC learnableâ­) if (a) for every underlying
    measure $Î¼$, labeling function $f: X â†’ Y$, error $1-Ïµ$ and
    confidence $1-Î´$, (b) there exists $m_0 âˆˆ ğ—¡$ and a sequence
    of inductors $(A^m: (X Ã—Y)^{m} â†’ â„‹)_{m = m_0}^{âˆ}$ so that
    (c) $A^m$ is $(Ïµ, Î´)$-probably approximately correct for all
    $m â‰¥ m_0$ . â¦‰

  â€– If the inductors are subfamilies of a family of inductors
    $(A^n)_{n âˆˆ ğ—¡}$ then we say that this family of inductors
    â¬PAC learnsâ­ $â„‹$. â¦‰
â¦‰

Â¶ â¦Š
  â€– We interpret these conditions as follows. â¦‰

  â€– No matter the underlying distribution and labeling function,
    given an accuracy and confidence, we can specify the number
    of samples required to make the algorithm â€œsucceed.â€ â¦‰

  â€– By which we mean that (â€œwith high probabilityâ€) the
    predictor's error will be â€œsmall.â€ â¦‰
â¦‰

Â§Â§ Sample complexity â¦‰
Â¶ â¦Š
  â€– If a family of inductors $ğ’œ = (A_n)_n$ PAC learns $â„‹$,
    then condition (b) above is equivalent to the existence
    function mapping $(Ïµ, Î´)$ to $ğ—¡$. â¦‰

  â€– Let $m_0^â˜…$ be the smallest integer such that condition (b)
    holds. â¦‰

  â€– We call the function $(Ïµ, Î´) â†¦ m_0^â˜…(Ïµ,Î´)$ the â¬sample
    complexityâ­ of the family $ğ’œ$ on the hypothesis class $â„‹$. â¦‰
â¦‰

Â§Â§ Other terminology â¦‰
Â¶ â¦Š
  â€– For (what seem to be) historical reasons, some authors refer
    to the definition above as â¬agnostic PAC learningâ­ since it
    drops a common condition (developed further in these sheets)
    relating the hypothesis class to the correct labeling function
    $f$. â¦‰
â¦‰

<tex>
  â€– \blankpage â¦‰
</tex>
