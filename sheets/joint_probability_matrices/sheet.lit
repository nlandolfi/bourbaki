<!--
!name:joint_probability_matrices
!need:real_matrix_rank
!need:dependent_events
-->

§ Why ⦉
¶ ⦊
  ‖ We can characterize the dependence of two events in terms
    of the rank of a particular matrix. ⦉
⦉

§ Definition ⦉
¶ ⦊
  ‖ Given a probability measure $𝗣: \powerset{Ω} → 𝗥$ on the
    finite set $Ω$ and two events $A, B ⊂ Ω$, the ❬joint
    probability matrix❭ of $A$ and $B$ is the matrix
    ◇ ⦊
      ‖ M = \bmat{ ⦉

      ‖ 𝗣(A ∩ B) ❲&❳ 𝗣(A ∩ \relcomplement{B}{Ω}) ᜶ ⦉

      ‖ 𝗣(\relcomplement{A}{Ω} ∩ B) ❲&❳ 𝗣(\relcomplement{A}{Ω} ∩
        \relcomplement{B}{Ω}) ᜶ ⦉

      ‖ }. ⦉
    ⦉⦉
⦉

§§ Characterization of independence ⦉
¶ ⦊
  ‖ If $A$ and $B$ are independent, then so are $A$ and
    $\relcomplement{B}{Ω}$, $B$ and $\relcomplement{A}{Ω}$, and
    $\relcomplement{A}{Ω}$ and$\relcomplement{B}{Ω}$. ⦉

  ‖ In other words,
    ◇ ⦊
      ‖ M = \bmat{𝗣(A) ᜶ 𝗣(\relcomplement{A}{Ω}) }\bmat{𝗣(B) ❲&❳
        𝗣(\relcomplement{B}{Ω}}. ⦉
    ⦉⦉

  ‖ In this case, we see that $\rank(M) = 1$. ⦉
⦉

¶ ⦊
  ‖ Conversely, suppose $\rank(M) = 1$. ⦉

  ‖ Then, using the law of total probabiliy, each row is a
    multiple of
    ◇ ⦊
      ‖ M1 = \bmat{𝗣(A) ᜶ 𝗣(\relcomplement{A}{Ω})} . ⦉
    ⦉⦉

  ‖ In particular, we have $𝗣(A ∩ B) = α 𝗣(A)$ and
    $𝗣(\relcomplement{A}{Ω} ∩ B) = αP(\relcomplement{A}{Ω})$. ⦉

  ‖ So
    ◇ ⦊
      ‖ 𝗣(A ∩ B) + 𝗣(\relcomplement{A}{Ω} ∩ B) = α(𝗣(B) +
        𝗣(\relcomplement{A}{Ω})), ⦉
    ⦉
    from which we deduce $α = 𝗣(B)$ ⦉

  ‖ Likewise, the multiplier for the second column of $M$ is
    $𝗣(\relcomplement{B}{Ω})$. ⦉

  ‖ In other words, $A$ and $B$ are independent. ⦉

  ‖ We conclude that $A$ and $B$ are independent if and only
    if $\rank(M) = 1$. ⦉
⦉

<tex>
  ‖ \blankpage ⦉
</tex>
