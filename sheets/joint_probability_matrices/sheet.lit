<!--
!name:joint_probability_matrices
!need:real_matrix_rank
!need:dependent_events
-->

Â§ Why â¦‰
Â¶ â¦Š
  â€– We can characterize the dependence of two events in terms
    of the rank of a particular matrix. â¦‰
â¦‰

Â§ Definition â¦‰
Â¶ â¦Š
  â€– Given a probability measure $ğ—£: \powerset{Î©} â†’ ğ—¥$ on the
    finite set $Î©$ and two events $A, B âŠ‚ Î©$, the â¬joint
    probability matrixâ­ of $A$ and $B$ is the matrix
    â—‡ â¦Š
      â€– M = \bmat{ â¦‰

      â€– ğ—£(A âˆ© B) â²&â³ ğ—£(A âˆ© \relcomplement{B}{Î©}) áœ¶ â¦‰

      â€– ğ—£(\relcomplement{A}{Î©} âˆ© B) â²&â³ ğ—£(\relcomplement{A}{Î©} âˆ©
        \relcomplement{B}{Î©}) áœ¶ â¦‰

      â€– }. â¦‰
    â¦‰â¦‰
â¦‰

Â§Â§ Characterization of independence â¦‰
Â¶ â¦Š
  â€– If $A$ and $B$ are independent, then so are $A$ and
    $\relcomplement{B}{Î©}$, $B$ and $\relcomplement{A}{Î©}$, and
    $\relcomplement{A}{Î©}$ and$\relcomplement{B}{Î©}$. â¦‰

  â€– In other words,
    â—‡ â¦Š
      â€– M = \bmat{ğ—£(A) áœ¶ ğ—£(\relcomplement{A}{Î©}) }\bmat{ğ—£(B) â²&â³
        ğ—£(\relcomplement{B}{Î©}}. â¦‰
    â¦‰â¦‰

  â€– In this case, we see that $\rank(M) = 1$. â¦‰
â¦‰

Â¶ â¦Š
  â€– Conversely, suppose $\rank(M) = 1$. â¦‰

  â€– Then, using the law of total probabiliy, each row is a
    multiple of
    â—‡ â¦Š
      â€– M1 = \bmat{ğ—£(A) áœ¶ ğ—£(\relcomplement{A}{Î©})} . â¦‰
    â¦‰â¦‰

  â€– In particular, we have $ğ—£(A âˆ© B) = Î± ğ—£(A)$ and
    $ğ—£(\relcomplement{A}{Î©} âˆ© B) = Î±P(\relcomplement{A}{Î©})$. â¦‰

  â€– So
    â—‡ â¦Š
      â€– ğ—£(A âˆ© B) + ğ—£(\relcomplement{A}{Î©} âˆ© B) = Î±(ğ—£(B) +
        ğ—£(\relcomplement{A}{Î©})), â¦‰
    â¦‰
    from which we deduce $Î± = ğ—£(B)$ â¦‰

  â€– Likewise, the multiplier for the second column of $M$ is
    $ğ—£(\relcomplement{B}{Î©})$. â¦‰

  â€– In other words, $A$ and $B$ are independent. â¦‰

  â€– We conclude that $A$ and $B$ are independent if and only
    if $\rank(M) = 1$. â¦‰
â¦‰

<tex>
  â€– \blankpage â¦‰
</tex>
