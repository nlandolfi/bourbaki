<!--
!name:affine_mMSE_estimators
!need:norms
!need:matrix_trace
!need:matrix_transpose
!need:covariance
!need:real_matrix_inverses
!need:estimators
-->

Â§ Definition â¦‰
Â¶ â¦Š
  â€– We want to estimate a random variable $x: Î© â†’ ğ—¥^n$ from a
    random variable $y: Î© â†’ ğ—¥^n$ using an estimator $Ï†: ğ—¥^m â†’
    ğ—¥^n$ which is affine.
    â€  â¦Š
      â€– Actually, the development flips this. Future editions will
        correct. â¦‰
    â¦‰â¦‰

  â€– In other words, $Ï†(Î¾) = AÎ¾ + b$ for some $A âˆˆ ğ—¥^{n Ã—m}$
    and $b âˆˆ ğ—¥^n$. â¦‰

  â€– We will use the mean squared error cost. â¦‰
â¦‰

Â¶ â¦Š
  â€– We want to find $A$ and $b$ to minimize
    â—‡ â¦Š
      â€– ğ—˜{\norm{Ax + b - y}^2}. â¦‰
    â¦‰â¦‰
â¦‰

<proof>
  â€– Express $ğ—˜(\norm{Ax + b - y}^2)$ as $ğ—˜((Ax + b - y)^âŠ¤(Ax
    + b - y))$
    â—‡ â¦Š
      â€– \mat{ â¦‰

      â€– ï¼† + ï¼† \tr(A ğ—˜(xx^âŠ¤) A^âŠ¤) ï¼† + ï¼† ğ—˜(x)^âŠ¤ A^âŠ¤ b ï¼† - ï¼†
        \tr(A^âŠ¤ ğ—˜(yx^âŠ¤)) áœ¶ â¦‰

      â€– ï¼† + ï¼† b^âŠ¤ A ğ—˜(x) ï¼† + ï¼† b^âŠ¤ b ï¼† - ï¼† b^âŠ¤ğ—˜(y) áœ¶ â¦‰

      â€– ï¼† - ï¼† \tr(A ğ—˜(xy^âŠ¤) ) ï¼† - ï¼† ğ—˜(y)^âŠ¤ b ï¼† + ï¼† ğ—˜(yy^âŠ¤) â¦‰

      â€– } â¦‰
    â¦‰â¦‰

  â€– The gradients with respect to $b$ are
    â—‡ â¦Š
      â€– \mat{ â¦‰

      â€– ï¼† + ï¼† 0 ï¼† + ï¼† A ğ—˜(x) ï¼† - ï¼† 0 áœ¶ â¦‰

      â€– ï¼† + ï¼† A ğ—˜(x) ï¼† + ï¼† 2 b ï¼† - ï¼† ğ—˜(y) áœ¶ â¦‰

      â€– ï¼† - ï¼† 0 ï¼† - ï¼† ğ—˜(y) ï¼† + ï¼† 0 â¦‰

      â€– } â¦‰
    â¦‰
    so $2Ağ—˜(x) + 2b - 2ğ—˜(y)$. â¦‰

  â€– The gradients with respect to $A$ are
    â—‡ â¦Š
      â€– \mat{ â¦‰

      â€– ï¼†ï¼† + ï¼† ğ—˜(xx^âŠ¤)A^âŠ¤ + ğ—˜(xx^âŠ¤)^âŠ¤ A^âŠ¤ ï¼† + ï¼† ğ—˜(x)b^âŠ¤ ï¼† -
        ï¼† ğ—˜(yx^âŠ¤)^âŠ¤ áœ¶ â¦‰

      â€– ï¼† + ï¼† ğ—˜(x)b^âŠ¤ ï¼† + ï¼† 0 ï¼† - ï¼† 0 áœ¶ â¦‰

      â€– ï¼† - ï¼† ğ—˜(xy^âŠ¤) ï¼† - ï¼† 0 ï¼† + ï¼† 0 â¦‰

      â€– } â¦‰
    â¦‰
    so $2ğ—˜(xx^âŠ¤)A^âŠ¤ + 2ğ—˜(x)b^âŠ¤ - 2ğ—˜(xy^âŠ¤)$. â¦‰

  â€– We want $A$ and $b$ solutions to
    â—‡ â¦Š
      â€– \begin{aligned} â¦‰

      â€– Ağ—˜(x) + b - ğ—˜(y) ï¼†= 0 áœ¶ â¦‰

      â€– ğ—˜(xx^âŠ¤)A^âŠ¤ + ğ—˜(x)b^âŠ¤ - ğ—˜(xy^âŠ¤) ï¼†= 0 â¦‰

      â€– \end{aligned} â¦‰
    â¦‰
    so first get $b = ğ—˜(y) - Ağ—˜(x)$. â¦‰

  â€– Then express
    â—‡ â¦Š
      â€– \begin{aligned} â¦‰

      â€– ğ—˜(xx^âŠ¤)A^âŠ¤ + ğ—˜(x)(ğ—˜(y) - A ğ—˜(x))^âŠ¤ - ğ—˜(xy^âŠ¤) = 0.áœ¶ â¦‰

      â€– ğ—˜(xx^âŠ¤)A^âŠ¤ + ğ—˜(x)ğ—˜(y)^âŠ¤ - ğ—˜(x)ğ—˜(x)^âŠ¤ A^âŠ¤ - ğ—˜(xy^âŠ¤) = 0.
        áœ¶ â¦‰

      â€– (ğ—˜(xx^âŠ¤) - ğ—˜(x)ğ—˜(x)^âŠ¤)A^âŠ¤ = ğ—˜(xy^âŠ¤) - ğ—˜(x)ğ—˜(y)^âŠ¤. áœ¶ â¦‰

      â€– \cov(x, x)A^âŠ¤ = \cov(x, y). â¦‰

      â€– \end{aligned} â¦‰
    â¦‰â¦‰

  â€– So $A^âŠ¤ = (\cov(x,x))^{-1}\cov(x, y)$ means $A = \cov(y,
    x)(\cov(x, x))^{-1}$ is a solution. â¦‰

  â€– Then $b = ğ—˜(y) - \cov(y, x)\cov(x,x)^{-1}ğ—˜(x)$. â¦‰

  â€– So to summarize, the estimator $Ï†(x) = Ax + b$ is â¦‰
  â—‡ â¦Š
    â€– \cov(y, x)(\cov{x, x})^{-1} x + ğ—˜(y) - \cov(y,
      x)(\cov(x,x))^{-1}ğ—˜(x) â¦‰
  â¦‰

  â€– or â¦‰
  â—‡ â¦Š
    â€– ğ—˜(y) + \cov(y, x) (\cov{x, x})^{-1} (x - ğ—˜(x)) â¦‰
  â¦‰
</proof>
