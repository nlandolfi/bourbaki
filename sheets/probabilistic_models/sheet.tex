
%!name:probabilistic_models
%!need:probability_measures
%!need:expectation
%!refs:diaconis/1988/sufficiency as symmetry

\section*{Why}

We have a space $X$ and a family of probability measures $\mathcal{P} $ on this space.
Assume $x \in X$ drawn from a fixed, unknown measure $P \in \mathcal{P} $.
Given $x$, how should we guess $P$?

\section*{Definition}

A \t{probabilistic model} (or \t{statistical model}, \t{parametric statistical model}, \t{statistical experiment}) is a family of probability measures over the same measurable space $(X, \mathcal{F} )$.
Call the index set the \t{parameter set} or \t{set of parameters}.
The set $X$ is called the \t{sample space}.
A \t{statistic} is any function on the sample space.

\subsection*{Notation}

Let $(X, \mathcal{F} )$ denote a measurable space.
We usually denote the parameter by $\Theta $, and denote the family
\[
\mathcal{P}  = \Set{\mathbfsf{P} _\theta : \mathcal{F}  \to [0,1]}{\mathbfsf{P} _\theta  \text{ a measure}, \theta  \in \Theta }.
\]
Often $\Theta  \subset \R ^d$.
% montanari has an additional stipulation on ùí´ 


\subsection*{Example: coin flips}

The usual model for $n$ flips of a coin takes $X = \set{0,1}^n$, the set of binary $n$-tuples.
For $\theta  \in [0, 1]$, a distribution $p_\theta (x) = \theta ^t(1-\theta )^{n-t}$ where $t = t(x) = x_1 + \cdots + x_n$ is defined on $X$.
A probability measure $\mathbfsf{P} _\theta $ is defined on $\powerset{X}$ in the \sheetref{event_probabilities}{the usual way}.
Thus, the probabilistic model is $\Set{\mathbfsf{P} _\theta }{\theta  \in [0,1]}$.
Given $x$, we are asked to guess $\theta $.

%<div data-littype='section' data-litsectionlevel='2' data-litsectionnumbered='false'> Example: gaussian </div>
%<div data-littype='paragraph'>
% <div data-littype='run'> A common model for quantities takes $X = ùó•^d$, the‚ê£
%    <a href='/sheets/n-dimensional_space.html'>
%     <div data-littype='run'> $d$-dimensional real space </div>
%    </a>
%    . </div>
%</div>


\section*{Decisions}

A \t{decision procedure} (\t{estimator}, \t{statistical procedure}) is a measurable function $A: \mathcal{X}  \to \mathcal{A} $ where $\mathcal{A} $ is a a set, called the \t{actions} or \t{decisions}.
Often $\mathcal{A}  = \Theta $, in which case $A(x)$ givens an \t{estimate} of $\theta $, which we denote $\hat{\theta }(x)$.

\section*{Judging decisions}

Given a \t{loss function} $L: \mathcal{A}  \times  \Theta  \to \Rbar$, the \t{risk} of $A$ is
\[
R(A, \theta ) = \E  L(A(x), \theta ).
\]

\blankpage
