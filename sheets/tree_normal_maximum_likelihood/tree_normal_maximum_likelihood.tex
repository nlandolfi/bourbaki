\input{../../sheet.tex}
\sbasic
\input{../ordered_pairs/macros.tex}
\input{../probability_events/macros.tex}
\input{../length_common_notions/macros.tex}
\input{../objects/macros.tex}
\input{../set_equality/macros.tex}
\input{../sets/macros.tex}
\input{../families/macros.tex}
\input{../relations/macros.tex}
\input{../set_operations/macros.tex}
\input{../graphs/macros.tex}
\input{../event_probabilities/macros.tex}
\input{../distance/macros.tex}
\input{../identity/macros.tex}
\input{../set_inclusion/macros.tex}
\input{../sentences/macros.tex}
\input{../relation_properties/macros.tex}
\input{../natural_order/macros.tex}
\input{../natural_families/macros.tex}
\input{../family_operations/macros.tex}
\input{../common_sense/macros.tex}
\input{../natural_numbers/macros.tex}
\input{../injective_functions/macros.tex}
\input{../functions/macros.tex}
\input{../trees/macros.tex}
\input{../marginal_distributions/macros.tex}
\input{../conditional_event_probability/macros.tex}
\input{../metrics/macros.tex}
\input{../logarithm/macros.tex}
\input{../equations/macros.tex}
\input{../arithmetic/macros.tex}
\input{../set_specification/macros.tex}
\input{../partial_orders/macros.tex}
\input{../direct_products/macros.tex}
\input{../natural_summation/macros.tex}
\input{../probability/macros.tex}
\input{../cardinality/macros.tex}
\input{../operations/macros.tex}
\input{../rooted_trees/macros.tex}
\input{../conditional_distributions/macros.tex}
\input{../approximation/macros.tex}
\input{../similarity_functions/macros.tex}
\input{../entropy/macros.tex}
\input{../cross_entropy/macros.tex}
\input{../differential_entropy/macros.tex}
\input{../solving_equations/macros.tex}
\input{../total_orders/macros.tex}
\input{../datasets/macros.tex}
\input{../real_summation/macros.tex}
\input{../probability_outcomes/macros.tex}
\input{../intervals/macros.tex}
\input{../algebras/macros.tex}
\input{../rooted_tree_distributions/macros.tex}
\input{../tree_distributions/macros.tex}
\input{../distribution_approximation/macros.tex}
\input{../relative_entropy/macros.tex}
\input{../differential_cross_entropy/macros.tex}
\input{../integer_numbers/macros.tex}
\input{../optimization/macros.tex}
\input{../distribution_selection/macros.tex}
\input{../identity_matrices/macros.tex}
\input{../probability_distributions/macros.tex}
\input{../element_functions/macros.tex}
\input{../groups/macros.tex}
\input{../space/macros.tex}
\input{../rooted_tree_densities/macros.tex}
\input{../marginal_densities/macros.tex}
\input{../tree_distribution_approximators/macros.tex}
\input{../differential_relative_entropy/macros.tex}
\input{../rational_numbers/macros.tex}
\input{../maximum_likelihood/macros.tex}
\input{../density_selection/macros.tex}
\input{../symmetric_matrices/macros.tex}
\input{../quadratic_forms/macros.tex}
\input{../probability_densities/macros.tex}
\input{../matrix_multiplication/macros.tex}
\input{../inverse_elements/macros.tex}
\input{../fields/macros.tex}
\input{../n-dimensional_space/macros.tex}
\input{../tree_densities/macros.tex}
\input{../conditional_densities/macros.tex}
\input{../tree_density_approximators/macros.tex}
\input{../real_numbers/macros.tex}
\input{../matrices/macros.tex}
\input{../vectors/macros.tex}
\input{../density_maximum_likelihood/macros.tex}
\input{../positive_definite_matrices/macros.tex}
\input{../normal_densities/macros.tex}
\input{../multivariate_real_densities/macros.tex}
\input{../matrix_inverse/macros.tex}
\input{../matrix_determinants/macros.tex}
\input{../tree_normals/macros.tex}
\input{../normal_conditionals/macros.tex}
\input{../best_tree_density_approximators/macros.tex}
\input{../real_matrices/macros.tex}
\input{../transpose/macros.tex}
\input{../normal_maximum_likelihood/macros.tex}
\input{../multivariate_normals/macros.tex}
\input{../tree_normal_approximators/macros.tex}
\input{../trace/macros.tex}
\input{../multivariate_normal_maximum_likelihood/macros.tex}
\input{./macros.tex}
\sstart
\stitle{Tree Normal Maximum Likelihood}
% bring back if drop tree_normal_approximators
%%!need:best_tree_density_approximators
%%!need:tree_normals

\ssection{Why}

What if we use the principle of maximum likelihood to select the maximum likelihood normal density which factors according to a tree?

\ssection{Definition}

A \t{maximum likelihood tree normal} of a dataset in $\R^d$ is a multivariate normal density that factors according to a tree and maximizes the likelihood of the dataset.

\ssection{Results}

\begin{prop}

Let $D = (x^1, \dots, x^n)$ be a dataset in $\R^d$.
A normal density is a maximum likelihood tree normal of $D$ if and only if it is an optimal tree approximator of the empirical normal density of $D$.

\begin{proof}

First, let $f: \R^d \to \R$ be a normal density.

First, express the log likelihood of $f$ on a record $x^k$ by
$$
  \log f(x^k) = - \frac{1}{2} (x^k - \mu)^{\tp} \inv{\Sigma} (x^k - \mu) - \frac{1}{2} \log \det \Sigma - \frac{d}{2}\log 2\pi.
$$

Second, use the trace to rewrite the quadratic form
$$
  -\frac{1}{2}\trp{\Sigma^{-1} (x^k - \mu) (x^k - \mu)^{\tp}}.
$$

Third, use these two, and the linearity of trace to express the average negative log likelihood by

$$
  \begin{aligned}
    - \frac{1}{n} \sum_{k = 1}^{n} \log f(x^k) &= \frac{1}{2} \trp{\Sigma^{-1}\parens*{\frac{1}{n}\sum_{i = 1}^{n} (x^k - \mu)(x^k - \mu)^{\tp}}} + \frac{1}{2}\log\det\Sigma + \frac{d}{2}\log 2\pi.
  \end{aligned}
$$

  Fourth, use matrix calculus (or the derivation in Proposition 1 of Multivariate Normal Maximum Likelihood) to see that, for a minimizer of the negative average log likelihood, the mean must be $\frac{1}{n} \sum_{i = 1}^{n} x^k$.

  Fifth, recognize the empirical covariance matrix $\frac{1}{n}\sum_{k = 1}^{n} (x^k - \mu)(x^k - \mu)^{\tp}$; denote it by $S$.

Sixth, change variables with $P = \Sigma^{-1}$ and express
$$
  \logp{\detp{\Sigma}} = \logp{\detp{\inv{P}}} = \logp{\invp{\det P}} = -\logp{\detp{P}}
$$

  Seventh, write the likelihood in simplified form (using circulant property of trace)
$$
  \frac{1}{2} \trp{SP} - \frac{1}{2} \log \det P - \frac{d}{2}\log 2\pi
$$

Eighth, drop the constant and prefactors:
$$
  \trp{SP} - \log \det P
$$

Ninth, if $g$ is a normal with then the tree density approximation objective is the same equivalent to
$$
  d(g, f) = h(g, f) - h(f) \sim h(g, f) = - \int_{\R^d} g\log f.
$$

TODO:
Extra, the let $g$ be normal and $f$ be normal. The tree normal approximation problem
$$
  d(g, f) \sim h(g, f) = - \int_{R^d} g \log f.
$$
The log of $f$ is
$$
  - \frac{1}{2} \trp{ \Sigma_f \int_{\R^d} (x - \mu_f)(x - \mu_f)^{\tp} dx } - \frac{1}{2} \log \det \Sigma_f^{-1} - \frac{d}{2} \log 2\pi
$$

Since the set of optimal solutions for both optimization is contained in the set of normal densities which match on the mean we can assume that $\mu_f = \mu_g$.
So the approximation objective is equivalent to
$$
  \tr{P_f \Sigma_g} - \log \det P_f,
$$
which is exactly the maximum likelihood objective.

Thus, a solution is a maximum likelihood tree normal of a dataset if and only if it is an optimal tree approximator of the empirical normal density of the dataset.

\end{proof}

\end{prop}
\strats
