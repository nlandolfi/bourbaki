
\section*{Why}

What is the best estimate for a random variable if we consider the square error?

\section*{Definition}

Let $(\Omega , \mathcal{A} , \mathbfsf{P} )$ be a probability space and $x: \Omega  \to \R $ a random variable.
A \t{minimum mean squared error estimate} or \t{MMSE estimate} or \t{least square estimate} is a value $\xi  \in \R $ which minimizes $\E  (x - \xi )^2$.

\begin{proposition}
There is a unique MMSE estimate and it is given by $\E (x)$.
\end{proposition}

\section*{Vector case}

Let $(\Omega , \mathcal{A} , \mathbfsf{P} )$ be a probability space and $y: \Omega  \to \R ^n$ a random variable.\footnote{Future editions might collapse this into the previous case.}

A \t{minimum mean squared error estimator} or \t{MMSE estimator} or \t{least square estimator} is a value $\xi  \in \R $ which minimizes $\E  \norm{x - \xi }^2$.

\begin{proposition}
There is a unique MMSE estimator and it is given by $\E (y)$.
\end{proposition}

\blankpage