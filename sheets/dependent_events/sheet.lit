¶ ⦊
  ‖ ❲%!name:dependent_events❳ ⦉

  ‖ ❲%!need:conditional_event_probabilities❳ ⦉
⦉

¶ ⦊
  ‖ \ssection{Why} ⦉

  ‖ We want to talk about how knowledge of one aspect of an
    outcome can give us knowledge about another aspect. ⦉
⦉

¶ ⦊
  ‖ \ssection{Definition} ⦉
⦉

¶ ⦊
  ‖ Two events $A, B ⊂ Ω$ are ❬independent❭ under a probability
    measure $𝗣: \powerset{Ω} → 𝗥$ if
    ◇ ⦊
      ‖ 𝗣(A ∩ B) = 𝗣(A)𝗣(B). ⦉
    ⦉⦉

  ‖ In other words, they are independent if the probability of
    their intersection is the product of their respective
    probabilities. ⦉

  ‖ Otherwise, we call $A$ and $B$ ❬dependent❭. ⦉
⦉

¶ ⦊
  ‖ In the case that $𝗣(B) ≠ 0$, then $𝗣(A ∩ B) = 𝗣(A)𝗣(B)$
    is equivalent to $𝗣(A \mid B) = 𝗣(A)$. ⦉

  ‖ We interpret this second expression as encoding the fact
    that the occurence of event $B$ does not change the
    “credibility” of the event $A$. ⦉
⦉

¶ ⦊
  ‖ \ssubsection{Example: two dice} ⦉
⦉

¶ ⦊
  ‖ Define $Ω = \Set{(ω_1, ω_2)}{ ω_i ∈ \set{1, …, 6}}$, and
    interpret $ω ∈ Ω$ as corresponding to pips face up after
    rolling two dice. ⦉

  ‖ Define $p: Ω → 𝗥$ by $p(ω) = \nicefrac{1}{36}$. ⦉
⦉

¶ ⦊
  ‖ Two events are $A = \Set{ω ∈ Ω}{ ω_1 + ω_2 > 5}$, “the
    sum is greater than 5”, and $B = \Set{ω ∈ Ω}{ω_1 > 3}$,
    “the number of pips on the first die is greater than 3”. ⦉

  ‖ Then $𝗣(A) = \nicefrac{26}{36}$. ⦉

  ‖ Also, $𝗣(A | B) = \nicefrac{17}{16}$. ⦉

  ‖ So, these events are dependent. ⦉

  ‖ Roughly speaking, we say that knowing $B$ tells us something
    about $A$. ⦉

  ‖ In this case, we say that it “makes $A$ more probable.” ⦉
⦉

¶ ⦊
  ‖ In the language used to describe the events, we say that
    knowledge that the number of pips on the first die is
    greater than three makes its more probable that the sum of
    the number of pips on each die is greater than 5. ⦉
⦉
