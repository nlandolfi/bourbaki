¶ ⦊
  ‖ ❲%!name:dependent_events❳ ⦉

  ‖ ❲%!need:conditional_event_probabilities❳ ⦉
⦉

¶ ⦊
  ‖ \ssection{Why} ⦉

  ‖ We want to talk about how knowledge of one aspect of an
    outcome can give us knowledge about another aspect. ⦉
⦉

¶ ⦊
  ‖ \ssection{Definition} ⦉
⦉

¶ ⦊
  ‖ Let $𝗣: \powerset{Ω} → 𝗥$ be a probability measure. ⦉
⦉

¶ ⦊
  ‖ Two events $A, B ⊂ Ω$ are ❬independent❭ if
    ◇ ⦊
      ‖ 𝗣(A ∩ B) = 𝗣(A)𝗣(B). ⦉
    ⦉⦉

  ‖ In other words, they are independent if the probability of
    their intersection is the product of their respective
    probabilities. ⦉

  ‖ Otherwise, we call $A$ and $B$ ❬dependent❭. ⦉
⦉

¶ ⦊
  ‖ In the case that $𝗣(B) ≠ 0$, then $𝗣(A ∩ B) = 𝗣(A)𝗣(B)$
    is equivalent to $𝗣(A \mid B) = 𝗣(A)$. ⦉

  ‖ We interpret this second expression as encoding the fact
    that the occurence of event $B$ does not change the
    “credibility” of the event $A$. ⦉
⦉

¶ ⦊
  ‖ \ssubsection{Example: two dice} ⦉
⦉

¶ ⦊
  ‖ Define $Ω = \Set{(ω_1, ω_2)}{ ω_i ∈ \set{1, …, 6}}$, and
    interpret $ω ∈ Ω$ as corresponding to pips face up after
    rolling two dice. ⦉

  ‖ Define $p: Ω → 𝗥$ by $p(ω) = \nicefrac{1}{36}$. ⦉
⦉

¶ ⦊
  ‖ Two events are $A = \Set{ω ∈ Ω}{ ω_1 + ω_2 > 5}$, “the
    sum is greater than 5”, and $B = \Set{ω ∈ Ω}{ω_1 > 3}$,
    “the number of pips on the first die in greater than 3”. ⦉

  ‖ Then $𝗣(A) = \nicefrac{26}{36}$. ⦉

  ‖ Also, $𝗣(A | B) = \nicefrac{17}{16}$. ⦉

  ‖ So, these events are dependent. ⦉

  ‖ Roughly speaking, we say that knowing $B$ tells us something
    about $A$. ⦉

  ‖ In this case, we say that it “makes $A$ more probable.” ⦉
⦉

¶ ⦊
  ‖ \blankpage ⦉
⦉
