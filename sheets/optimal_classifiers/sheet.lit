<!--
!name:optimal_classifiers
!need:classifier_errors
!need:combined_orders
!need:multiobjective_optimization_problems
-->

Â§ Why â¦‰
â€  â¦Š
  â€– Future editions will include. â¦‰
â¦‰

Â§ Discussion â¦‰
Â¶ â¦Š
  â€– Given two classifiers $G_1$ and $G_2$ and a dataset, we can
    associate to each its false positve and false negative rate
    on the dataset. â¦‰

  â€– For a finite dataset, these are two rational numbers. â¦‰

  â€– It is natural to prefer $G_1$ to $G_2$ if the former has
    a smaller false positive rate. â¦‰

  â€– Conversely, it is natural to prefer $G_2$ to $G_1$ if the
    former has a smaller false negative rate. â¦‰

  â€– Unfortunately, one may need to trade-off these two desiderata
    (seeâ£
    <a href='/sheets/combined_orders.html'>
      â€– Combined Orders â¦‰
    </a>
    ), since there is no total order. â¦‰

  â€– In other words, choosing betwen $G_1$ and $G_2$ is a
    multiobjective optimization problem. â¦‰
â¦‰

Â§Â§ Scalarization â¦‰
Â¶ â¦Š
  â€– Let $ğ’¢$ be a set of classifiers and let $f: ğ’¢ â†’ ğ—¥^2$ be
    defined so that $f_1(G)$ is the false negative rate of $G$
    on some dataset and $f_2(G)$ is the false positive rate of
    $G$ on the same dataset. â¦‰

  â€– The â¬$Îº$-scalarized error metricâ­ (or â¬Neyman-Pearson metricâ­
    associated with $G âˆˆ ğ’¢$ is $Îº f_1(G) + f_2(G)$. â¦‰

  â€– In the case that $Îº > 1$, false negatives are given higher
    cost than false positives, and vice versa whtn $Îº < 1$. â¦‰

  â€– For $Îº = 1$, the scalarized error metric is the same as
    the overall error rate. â¦‰
â¦‰

<tex>
  â€– \blankpage â¦‰
</tex>
