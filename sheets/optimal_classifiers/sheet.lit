<!--
!name:optimal_classifiers
!need:classifier_errors
!need:combined_orders
!need:multiobjective_optimization_problems
-->

§ Why ⦉
† ⦊
  ‖ Future editions will include. ⦉
⦉

§ Discussion ⦉
¶ ⦊
  ‖ Given two classifiers $G_1$ and $G_2$ and a dataset, we can
    associate to each its false positve and false negative rate
    on the dataset. ⦉

  ‖ For a finite dataset, these are two rational numbers. ⦉

  ‖ It is natural to prefer $G_1$ to $G_2$ if the former has
    a smaller false positive rate. ⦉

  ‖ Conversely, it is natural to prefer $G_2$ to $G_1$ if the
    former has a smaller false negative rate. ⦉

  ‖ Unfortunately, one may need to trade-off these two desiderata
    (see␣
    <a href='/sheets/combined_orders.html'>
      ‖ Combined Orders ⦉
    </a>
    ), since there is no total order. ⦉

  ‖ In other words, choosing betwen $G_1$ and $G_2$ is a
    multiobjective optimization problem. ⦉
⦉

§§ Scalarization ⦉
¶ ⦊
  ‖ Let $𝒢$ be a set of classifiers and let $f: 𝒢 → 𝗥^2$ be
    defined so that $f_1(G)$ is the false negative rate of $G$
    on some dataset and $f_2(G)$ is the false positive rate of
    $G$ on the same dataset. ⦉

  ‖ The ❬$κ$-scalarized error metric❭ (or ❬Neyman-Pearson metric❭
    associated with $G ∈ 𝒢$ is $κ f_1(G) + f_2(G)$. ⦉

  ‖ In the case that $κ > 1$, false negatives are given higher
    cost than false positives, and vice versa whtn $κ < 1$. ⦉

  ‖ For $κ = 1$, the scalarized error metric is the same as
    the overall error rate. ⦉
⦉

<tex>
  ‖ \blankpage ⦉
</tex>
