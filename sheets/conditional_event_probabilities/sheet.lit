<!--
!name:conditional_event_probabilities
!need:event_probabilities
-->

Â§ Why â¦‰
Â¶ â¦Š
  â€– How should we modify probabilities, given that we know some
    aspect of the outcome (i.e., that some event has occurred). â¦‰
â¦‰

Â§ Definition â¦‰
Â¶ â¦Š
  â€– Suppose $Î©$ is a set of outcomes and $ğ—£: \powerset{Î©} â†’ ğ—¥$
    is a finite probability measure. â¦‰

  â€– Suppose $A, B âŠ‚ Î©$ with $ğ—£(B) â‰  0$. â¦‰

  â€– The â¬conditional probabilityâ­ of $A$ â¬givenâ­ $B$ is fraction
    of the probability of $A âˆ© B$ over the probability of $B$. â¦‰
â¦‰

Â§Â§ Notation â¦‰
Â¶ â¦Š
  â€– In a slightly slippery but universally standard notation, we
    denote the conditional probability of $A$ given $B$ by $ğ—£(A
    | B)$. â¦‰

  â€– In other words, we define
    â—‡ â¦Š
      â€– ğ—£(A | B) = \frac{ğ—£(A âˆ© B)}{ğ—£(B)}, â¦‰
    â¦‰â¦‰

  â€– for all $A, B âŠ‚ Î©$, whenever $ğ—£(B) â‰  0$. â¦‰
â¦‰

Â¶ â¦Š
  â€– For example, we can express the law of total probability
    (seeâ£
    <a href='/sheets/event_probabilities.html'>
      â€– Event Probabilities â¦‰
    </a>
    ) as
    â—‡ â¦Š
      â€– \textstyle â¦‰

      â€– ğ—£(B) = âˆ‘_{i = 1}^{n} ğ—£(A_i)ğ—£(B | A_i), â¦‰
    â¦‰
    where $A_1, â€¦, A_n$ partition $Î©$ and $B âŠ‚ Î©$ with $ğ—£(B)
    > 0$. â¦‰
â¦‰

Â§ Conditional probability measure â¦‰
Â¶ â¦Š
  â€– Suppose $B âŠ‚ Î©$ and $ğ—£(B) > 0$. â¦‰

  â€– Then (i) $ğ—£(A | B) â‰¥ 0$ for all $A âŠ‚ Î©$, since $ğ—£(A âˆ©
    B) â‰¥ 0$. â¦‰

  â€– Moreover, (ii)
    â—‡ â¦Š
      â€– ğ—£(Î© | B) = ğ—£(Î© âˆ© B)/ğ—£(B) = ğ—£(B)/ğ—£(B) = 1 â¦‰
    â¦‰â¦‰

  â€– Similarly, $ğ—£(âˆ… | B) = ğ—£(âˆ… âˆ© B)/ğ—£(B) = 0/ğ—£(B) = 0$. â¦‰
â¦‰

Â¶ â¦Š
  â€– Finally, (iii) if $A âˆ© C = âˆ…$, then
    â—‡ â¦Š
      â€– \begin{aligned} â¦‰

      â€– ğ—£(A âˆ© C | B) â¦‰

      â€– â²&â³= â¦‰

      â€– ğ—£((A âˆ© C) âˆ© B)/ğ—£(B) áœ¶ â¦‰

      â€– â²&â³= â¦‰

      â€– ğ—£((A âˆ© B) âˆ© (C âˆ© B)/ğ—£(B) áœ¶ â¦‰

      â€– â²&â³\overset{(a)}{=} â¦‰

      â€– (ğ—£(A âˆ© B) + ğ—£(C âˆ© B))/ğ—£(B) áœ¶ â¦‰

      â€– â²&â³= â¦‰

      â€– ğ—£(A âˆ© B)/ğ—£(B) + P(C âˆ© B)/ğ—£(B) áœ¶ â¦‰

      â€– â²&â³= â¦‰

      â€– ğ—£(A | B) + P(C | B). â¦‰

      â€– \end{aligned} â¦‰
    â¦‰
    where (a) follows since $A âˆ© B$ and $C âˆ© B$ are disjoint. â¦‰
â¦‰

Â¶ â¦Š
  â€– Together, (i)-(iii) mean that $ğ—£(\cdot | B)$ is itself a
    probability measure on $Î©$. â¦‰

  â€– We therefore refer to $ğ—£(\cdot | B)$ as a â¬conditional
    probability measureâ­. â¦‰
â¦‰

Â§Â§ Induced conditional distribution â¦‰
Â¶ â¦Š
  â€– Therefore, we expect there to also correspond a new
    distribution on the set of outcomes. â¦‰

  â€– For $ğ—£_p$, define $q: Î© â†’ ğ—¥$ by
    â—‡ â¦Š
      â€– q(Ï‰) = \begin{cases} â¦‰

      â€– \frac{p(Ï‰)}{ğ—£(B)} â²&â³ \text{ if } Ï‰ âˆˆ B áœ¶ â¦‰

      â€– 0 â²&â³ \text{ otherwise. } áœ¶ â¦‰

      â€– \end{cases} â¦‰
    â¦‰â¦‰

  â€– In this case $ğ—£_q(A) = ğ—£_p(A | B)$. â¦‰

  â€– We call $q$ the â¬conditional distributionâ­ induced by
    â¬conditioning onâ­ the event $B$. â¦‰
â¦‰

Â§Â§ Finite intersections â¦‰
Â¶ â¦Š
  â€– As a simple repeated application of our definition, suppose
    $A_1, â€¦, A_n âŠ‚ Î©$. â¦‰

  â€– Then
    â—‡ â¦Š
      â€– ğ—£(A_1 âˆ© A_2 âˆ© â‹¯ âˆ© A_n) = ğ—£(A_1) ğ—£(A_2 | A_1) â‹¯ ğ—£(A_n
        | A_1 âˆ© â‹¯ âˆ© A_{n-1}) â¦‰
    â¦‰â¦‰

  â€– Many authors call this the â¬chain ruleâ­. â¦‰
â¦‰
