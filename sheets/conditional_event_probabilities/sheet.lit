<!--
!name:conditional_event_probabilities
!need:event_probabilities
-->

§ Why ⦉
¶ ⦊
  ‖ How should we modify probabilities, given that we know some
    aspect of the outcome (i.e., that some event has occurred). ⦉
⦉

§ Definition ⦉
¶ ⦊
  ‖ Suppose $Ω$ is a set of outcomes and $𝗣: \powerset{Ω} → 𝗥$
    is a finite probability measure. ⦉

  ‖ Suppose $A, B ⊂ Ω$ with $𝗣(B) ≠ 0$. ⦉

  ‖ The ❬conditional probability❭ of $A$ ❬given❭ $B$ is fraction
    of the probability of $A ∩ B$ over the probability of $B$. ⦉
⦉

§§ Notation ⦉
¶ ⦊
  ‖ In a slightly slippery but universally standard notation, we
    denote the conditional probability of $A$ given $B$ by $𝗣(A
    | B)$. ⦉

  ‖ In other words, we define
    ◇ ⦊
      ‖ 𝗣(A | B) = \frac{𝗣(A ∩ B)}{𝗣(B)}, ⦉
    ⦉⦉

  ‖ for all $A, B ⊂ Ω$, whenever $𝗣(B) ≠ 0$. ⦉
⦉

¶ ⦊
  ‖ For example, we can express the law of total probability
    (see␣
    <a href='/sheets/event_probabilities.html'>
      ‖ Event Probabilities ⦉
    </a>
    ) as
    ◇ ⦊
      ‖ \textstyle ⦉

      ‖ 𝗣(B) = ∑_{i = 1}^{n} 𝗣(A_i)𝗣(B | A_i), ⦉
    ⦉
    where $A_1, …, A_n$ partition $Ω$ and $B ⊂ Ω$ with $𝗣(B)
    > 0$. ⦉
⦉

§ Conditional probability measure ⦉
¶ ⦊
  ‖ Suppose $B ⊂ Ω$ and $𝗣(B) > 0$. ⦉

  ‖ Then (i) $𝗣(A | B) ≥ 0$ for all $A ⊂ Ω$, since $𝗣(A ∩
    B) ≥ 0$. ⦉

  ‖ Moreover, (ii)
    ◇ ⦊
      ‖ 𝗣(Ω | B) = 𝗣(Ω ∩ B)/𝗣(B) = 𝗣(B)/𝗣(B) = 1 ⦉
    ⦉⦉

  ‖ Similarly, $𝗣(∅ | B) = 𝗣(∅ ∩ B)/𝗣(B) = 0/𝗣(B) = 0$. ⦉
⦉

¶ ⦊
  ‖ Finally, (iii) if $A ∩ C = ∅$, then
    ◇ ⦊
      ‖ \begin{aligned} ⦉

      ‖ 𝗣(A ∩ C | B) ⦉

      ‖ ❲&❳= ⦉

      ‖ 𝗣((A ∩ C) ∩ B)/𝗣(B) ᜶ ⦉

      ‖ ❲&❳= ⦉

      ‖ 𝗣((A ∩ B) ∩ (C ∩ B)/𝗣(B) ᜶ ⦉

      ‖ ❲&❳\overset{(a)}{=} ⦉

      ‖ (𝗣(A ∩ B) + 𝗣(C ∩ B))/𝗣(B) ᜶ ⦉

      ‖ ❲&❳= ⦉

      ‖ 𝗣(A ∩ B)/𝗣(B) + P(C ∩ B)/𝗣(B) ᜶ ⦉

      ‖ ❲&❳= ⦉

      ‖ 𝗣(A | B) + P(C | B). ⦉

      ‖ \end{aligned} ⦉
    ⦉
    where (a) follows since $A ∩ B$ and $C ∩ B$ are disjoint. ⦉
⦉

¶ ⦊
  ‖ Together, (i)-(iii) mean that $𝗣(\cdot | B)$ is itself a
    probability measure on $Ω$. ⦉

  ‖ We therefore refer to $𝗣(\cdot | B)$ as a ❬conditional
    probability measure❭. ⦉
⦉

§§ Induced conditional distribution ⦉
¶ ⦊
  ‖ Therefore, we expect there to also correspond a new
    distribution on the set of outcomes. ⦉

  ‖ For $𝗣_p$, define $q: Ω → 𝗥$ by
    ◇ ⦊
      ‖ q(ω) = \begin{cases} ⦉

      ‖ \frac{p(ω)}{𝗣(B)} ❲&❳ \text{ if } ω ∈ B ᜶ ⦉

      ‖ 0 ❲&❳ \text{ otherwise. } ᜶ ⦉

      ‖ \end{cases} ⦉
    ⦉⦉

  ‖ In this case $𝗣_q(A) = 𝗣_p(A | B)$. ⦉

  ‖ We call $q$ the ❬conditional distribution❭ induced by
    ❬conditioning on❭ the event $B$. ⦉
⦉

§§ Finite intersections ⦉
¶ ⦊
  ‖ As a simple repeated application of our definition, suppose
    $A_1, …, A_n ⊂ Ω$. ⦉

  ‖ Then
    ◇ ⦊
      ‖ 𝗣(A_1 ∩ A_2 ∩ ⋯ ∩ A_n) = 𝗣(A_1) 𝗣(A_2 | A_1) ⋯ 𝗣(A_n
        | A_1 ∩ ⋯ ∩ A_{n-1}) ⦉
    ⦉⦉

  ‖ Many authors call this the ❬chain rule❭. ⦉
⦉
