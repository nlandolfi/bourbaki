<!--yaml
name: conditional_event_probabilities
needs:
    - event_probabilities
wikipedia: https://en.wikipedia.org/wiki/Conditional_probability
-->

Â§ Why â¦‰
Â¶ â¦Š
  â€– How should we modify probabilities, given that we know some
    aspect of the outcome (i.e., that some event has occurred)? â¦‰

  â€– In other words, and roughly speaking, how does knowledge
    abot one aspect of an outcome give us knowledge about
    another aspect. â¦‰
â¦‰

Â§ Definition â¦‰
Â¶ â¦Š
  â€– Suppose $Î©$ is a finite set of outcomes with a probability
    even function $P$. â¦‰

  â€– Suppose $A, B âŠ‚ Î©$ are two events and $P(B) â‰  0$. â¦‰

  â€– The â¬conditional probabilityâ­ of $A$ â¬givenâ­ $B$ is the
    ratio of the probability of $A âˆ© B$ to the probability of
    $B$. â¦‰

  â€– Other language includes the â€¹conditional probabilityâ€º of $A$
    â¬given thatâ­ $B$ â¬has happenedâ­. â¦‰
â¦‰

Â¶ â¦Š
  â€– The frequentist interpretation is straightforward. â¦‰

  â€– We collect many outcomes, and $P(B)$ is the fraction of
    times that the event $B$ occurs. â¦‰

  â€– $P(A âˆ© B)$ is the fraction of times both $A$ and $B$
    occurs. â¦‰

  â€– The conditional probability of $A$ given $B$ is the fraction
    of times that $A$ happened â€¹among the outcomes in which $B$
    happenedâ€º. â¦‰
â¦‰

Â§Â§ Notation â¦‰
Â¶ â¦Š
  â€– In a slightly slippery but universally standard notation, we
    denote the conditional probability of $A$ given $B$ by $P(A
    | B)$. â¦‰

  â€– In other words, we define
    â—‡ â¦Š
      â€– P(A | B) = \frac{P(A âˆ© B)}{P(B)}, â¦‰
    â¦‰â¦‰

  â€– for all $A, B âŠ‚ Î©$, whenever $P(B) â‰  0$. â¦‰
â¦‰

Â¶ â¦Š
  â€– Notice that for any two events, $P(A âˆ© B) = P(A) P(B|A)$. â¦‰

  â€– Pleasantly, this equation makes sense even if $P(A) = 0$,
    since then $P(A âˆ© B) â‰¤ P(A) = 0$ means $P(A âˆ© B)$ is
    also 0â€”irrespecitve of our definition of $P(B | A)$ in this
    case. â¦‰

  â€– (Here we have swapped the order of $A$ and $B$). â¦‰
â¦‰

Â¶ â¦Š
  â€– For example, we can express the law of total probability
    (seeâ£
    <a href='/sheets/event_probabilities.html'>
      â€– Event Probabilities â¦‰
    </a>
    ) as
    â—‡ â¦Š
      â€– \textstyle â¦‰

      â€– P(B) = âˆ‘_{i = 1}^{n} P(A_i)P(B | A_i), â¦‰
    â¦‰
    where $A_1, â€¦, A_n$ partition $Î©$ and $B âŠ‚ Î©$ with $P(B)
    > 0$. â¦‰

  â€– A particular nice example,
    â—‡ â¦Š
      â€– P(B) = P(B | A)P(A) + P(B | Î© - A)P(Î© - A) â¦‰
    â¦‰â¦‰
â¦‰

Â§ Examples â¦‰
Â¶ â¦Š
  â€– â€¹Rolling a die.â€º â¦‰

  â€– As usual, model rolling a die with the sample space $Î© =
    \set{1, â€¦, 6}$ and distribution $p: Î© â†’ [0,1]$ defined by
    $p(Ï‰) = 1/6$ for all $Ï‰ âˆˆ Î©$. â¦‰

  â€– Take the two events, $A = \set{6}$ and $B = \set{4, 5,
    6}$, which we interpret as â€œthe number of pips is 6â€ and
    â€œthe number of pips is at least 4â€, respectively. â¦‰

  â€– Then
    â—‡ â¦Š
      â€– P(A | B) = \frac{1/6}{1/2} = \frac{1}{3} â¦‰
    â¦‰â¦‰

  â€– This has a nice interpretation, given that we know the
    number of pips face up is either 4 or 5 or 6, the chance
    that it is 6 is $1$ in $3$. â¦‰
â¦‰

Â§ Conditional probability measure â¦‰
Â¶ â¦Š
  â€– It happens that $P(\cdot | B)$ is itself a probability
    event function on $Î©$. â¦‰

  â€– We therefore refer to $P(\cdot | B)$ as a â¬conditional
    probability functionâ­ or â¬conditional probability measureâ­. â¦‰
â¦‰

Â¶ â¦Š
  â€– To see this, suppose $B âŠ‚ Î©$ and $P(B) > 0$. â¦‰

  â€– Then (i) $P(A | B) â‰¥ 0$ for all $A âŠ‚ Î©$, since $P(A âˆ©
    B) â‰¥ 0$. â¦‰

  â€– Moreover, (ii)
    â—‡ â¦Š
      â€– P(Î© | B) = P(Î© âˆ© B)/P(B) = P(B)/P(B) = 1 â¦‰
    â¦‰â¦‰

  â€– Similarly, $P(âˆ… | B) = P(âˆ… âˆ© B)/P(B) = 0/P(B) = 0$. â¦‰
â¦‰

Â¶ â¦Š
  â€– Finally, (iii) if $A âˆ© C = âˆ…$, then
    â—‡ â¦Š
      â€– \begin{aligned} â¦‰

      â€– P(A âˆ© C | B) â¦‰

      â€– &= \frac{P((A âˆ© C) âˆ© B)}{P(B)} áœ¶ â¦‰

      â€– &\overset{(a)}{=} \frac{P(A âˆ© B) + P(C âˆ© B)}{P(B)} áœ¶ â¦‰

      â€– &= P(A | B) + P(C | B). â¦‰

      â€– \end{aligned} â¦‰
    â¦‰
    where (a) follows since $(A âˆ© C) âˆ© B = (A âˆ© B) âˆª (C âˆ©
    B)$ and the sets $A âˆ© B$, $C âˆ© B$ are disjoint. â¦‰
â¦‰

Â§Â§ Induced conditional distribution â¦‰
Â¶ â¦Š
  â€– Since $P(Â·|B)$ is a probability event function, we expect it
    to have a corresponding distribution. â¦‰

  â€– Denote the distribution of $P$ by $p: Î© â†’ [0,1]$. â¦‰

  â€– In other words, $p$ satisfies $p(Ï‰) = P(\set{Ï‰})$ as usual. â¦‰

  â€– Now define $q: Î© â†’ ğ—¥$ by
    â—‡ â¦Š
      â€– q(Ï‰) = \begin{cases} â¦‰

      â€– \frac{p(Ï‰)}{P(B)} â²&â³ \text{ if } Ï‰ âˆˆ B áœ¶ â¦‰

      â€– 0 â²&â³ \text{ otherwise. } áœ¶ â¦‰

      â€– \end{cases} â¦‰
    â¦‰â¦‰

  â€– In this case the event probability function induced by $A$
    is $P(Â·|B)$. â¦‰

  â€– We call $q$ the â¬conditional distributionâ­ induced by
    â¬conditioning onâ­ the event $B$. â¦‰
â¦‰

Â§Â§ Finite intersections â¦‰
Â¶ â¦Š
  â€– As a simple repeated application of our definition, suppose
    $A_1, â€¦, A_n âŠ‚ Î©$. â¦‰

  â€– Then
    â—‡ â¦Š
      â€– P(A_1 âˆ© A_2 âˆ© â‹¯ âˆ© A_n) = P(A_1) P(A_2 | A_1) â‹¯ P(A_n
        | A_1 âˆ© â‹¯ âˆ© A_{n-1}) â¦‰
    â¦‰â¦‰

  â€– Many authors call this the â¬chain ruleâ­. The order of the
    $A_i$ is inconsequential. â¦‰
â¦‰
