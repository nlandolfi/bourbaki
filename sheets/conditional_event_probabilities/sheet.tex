
\section*{Why}

How should we modify probabilities, given that we know some aspect of the outcome (i.e., that some event has occurred)?

\section*{Definition}

Suppose $\Omega $ is a set of outcomes and $\mathbfsf{P} : \powerset{\Omega } \to \R $ is a finite probability measure.
Suppose $A, B \subset \Omega $ with $\mathbfsf{P} (B) \neq 0$.
The \t{conditional probability} of $A$ \t{given} $B$ is the ratio of the probability of $A \cap  B$ to the probability of $B$.

\subsection*{Notation}

In a slightly slippery but universally standard notation, we denote the conditional probability of $A$ given $B$ by $\mathbfsf{P} (A \mid  B)$.
In other words, we define
\[
\mathbfsf{P} (A \mid  B) = \frac{\mathbfsf{P} (A \cap B)}{\mathbfsf{P} (B)},
\]
for all $A, B \subset \Omega $, whenever $\mathbfsf{P} (B) \neq 0$.

For example, we can express the law of total probability (see \sheetref{event_probabilities}{Event Probabilities}) as
\[
\textstyle
\mathbfsf{P} (B) = \sum_{i = 1}^{n} \mathbfsf{P} (A_i)\mathbfsf{P} (B \mid  A_i),
\]
where $A_1, \dots , A_n$ partition $\Omega $ and $B \subset \Omega $ with $\mathbfsf{P} (B) > 0$.

\section*{Conditional probability measure}

It happens that $\mathbfsf{P} (\cdot \mid  B)$ is itself a probability measure on $\Omega $.
We therefore refer to $\mathbfsf{P} (\cdot \mid  B)$ as a \t{conditional probability measure}.

To see this, suppose $B \subset \Omega $ and $\mathbfsf{P} (B) > 0$.
Then (i) $\mathbfsf{P} (A \mid  B) \geq 0$ for all $A \subset \Omega $, since $\mathbfsf{P} (A \cap  B) \geq 0$.
Moreover, (ii)
\[
\mathbfsf{P} (\Omega  \mid  B) = \mathbfsf{P} (\Omega  \cap B)/\mathbfsf{P} (B) = \mathbfsf{P} (B)/\mathbfsf{P} (B) = 1
\]
Similarly, $\mathbfsf{P} (\varnothing \mid  B) = \mathbfsf{P} (\varnothing \cap  B)/\mathbfsf{P} (B) = 0/\mathbfsf{P} (B) = 0$.

Finally, (iii) if $A \cap  C = \varnothing$, then
\[
\begin{aligned}
\mathbfsf{P} (A \cap  C \mid  B)
&=
\mathbfsf{P} ((A \cap  C) \cap  B)/\mathbfsf{P} (B) \\
&=
\mathbfsf{P} ((A \cap  B) \cap  (C \cap  B)/\mathbfsf{P} (B) \\
&\overset{(a)}{=}
(\mathbfsf{P} (A \cap  B) + \mathbfsf{P} (C \cap B))/\mathbfsf{P} (B) \\
&=
\mathbfsf{P} (A \cap  B)/\mathbfsf{P} (B) + P(C \cap B)/\mathbfsf{P} (B) \\
&=
\mathbfsf{P} (A \mid  B) + P(C \mid  B).
\end{aligned}
\]
where (a) follows since $A \cap  B$ and $C \cap  B$ are disjoint.

\subsection*{Induced conditional distribution}

Therefore, we expect there to also correspond a new distribution on the set of outcomes.
For $\mathbfsf{P} _p$, define $q: \Omega  \to \R $ by
\[
q(\omega ) = \begin{cases}
\frac{p(\omega )}{\mathbfsf{P} (B)} & \text{ if } \omega  \in B \\
0 & \text{ otherwise. } \\
\end{cases}
\]
In this case $\mathbfsf{P} _q(A) = \mathbfsf{P} _p(A \mid  B)$.
We call $q$ the \t{conditional distribution} induced by \t{conditioning on} the event $B$.

\subsection*{Finite intersections}

As a simple repeated application of our definition, suppose $A_1, \dots , A_n \subset \Omega $.
Then
\[
\mathbfsf{P} (A_1 \cap  A_2 \cap  \cdots \cap  A_n) = \mathbfsf{P} (A_1) \mathbfsf{P} (A_2 \mid  A_1) \cdots \mathbfsf{P} (A_n \mid  A_1 \cap  \cdots \cap  A_{n-1})
\]
Many authors call this the \t{chain rule}. The order of the $A_i$ is inconsequential.
