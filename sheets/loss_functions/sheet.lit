<!--
!name:loss_functions
!need:predictor_performance_metrics
!need:similarity_functions
-->

Â§ Why â¦‰
Â¶ â¦Š
  â€– We compare inductors by comparing the relations they produce. â¦‰

  â€– We can compare relations by similarity functions. â¦‰
â¦‰

Â§ Definition â¦‰
Â¶ â¦Š
  â€– Let $X$ and $Y$ be sets. â¦‰

  â€– Let $â„›$ be the set of relations between $X$ and $Y$. â¦‰

  â€– Let $â„‹ âŠ‚ â„›$ be a set of hypothesis relations. â¦‰
â¦‰

Â¶ â¦Š
  â€– Given a similarity function $d: â„› Ã— â„› â†’ ğ—¥$ we can consider â¦‰
â¦‰

Â¶ â¦Š
  â€– A â¬lossâ­ function is a nonnegative real-valued function on
    pairs which is zero only on repeated pairs. â¦‰

  â€– It need not be symmetric. â¦‰

  â€– A loss function is often also called a â¬costâ­ or â¬riskâ­
    function. â¦‰
â¦‰

Â¶ â¦Š
  â€– We use loss functions to judge a prediction in comparison
    to the recorded postcept. â¦‰

  â€– The first argument of the loss function is the predicted
    postcept and the second is the recorded (observed, true,
    recorded) postcept. â¦‰
â¦‰

Â¶ â¦Š
  â€– The â¬loss of a predictor on a pairâ­ is the result of the
    loss function on the pair. â¦‰

  â€– Similarly, the â¬loss of a predictor on a datasetâ­ of record
    pairs is the sum of the losses on the pairs of the dataset. â¦‰

  â€– Likewise, the â¬average lossâ­ of a predictor is the loss of
    the predictor on the dataset divided by the size of the
    dataset. â¦‰

  â€– The average loss is also known as the â¬empirical riskâ­ of
    the predictor on the dataset. â¦‰
â¦‰

Â§Â§ Notation â¦‰
Â¶ â¦Š
  â€– Let $(a, b) âˆˆ A Ã—B$; $A, B â‰  âˆ…$. â¦‰

  â€– For a lost function $\loss: B \cross B â†’ ğ—¥$ and predictor
    $f: A â†’ B$, the loss of $f$ on $(a, b)$ is $\loss(f(a),b)$. â¦‰

  â€– Let $s = ((a^1, b^1), â€¦, (a^n, b^n))$ â¦‰

  â€– be a record sequence. â¦‰

  â€– The loss of $f$ on $s$ is $âˆ‘_{k = 1}^{n} \loss(f(a^k),
    b^k)$. â¦‰

  â€– The average loss of $f$ on $s$ is $(1/n)âˆ‘_{k =
    1}^{n} \loss(f(a^k), b^k)$. â¦‰
â¦‰

Â§ Training and test loss â¦‰
Â¶ â¦Š
  â€– Recall that $s$ is a dataset of records pairs in $A Ã—B$. â¦‰

  â€– We call a predictor $f: A â†’ B$ an â¬interpolatorâ­ of the
    dataset $s$ if, for each pair $(a^i, b^i)$ in the dataset,
    $f(a^i) = b^i$. â¦‰

  â€– An interpolator achieves zero loss on the dataset it
    interpolates. â¦‰
â¦‰

Â¶ â¦Š
  â€– The rub is that an interpolator may have nonzero loss a
    record pair which is not contained in the dataset used to
    construct it. â¦‰

  â€– For this reason, it is common to consider two datasets. â¦‰

  â€– First, the one used to construct the predictor (the
    â¬training datasetâ­) and second, one used to evaluate the
    predictor (the â¬test datasetâ­, â¬validation datasetâ­ or
    â¬evaluation datasetâ­). â¦‰
â¦‰

Â¶ â¦Š
  â€– We judge a predictor by its average loss on the test
    dataset. â¦‰

  â€– We call this the â¬test lossâ­, in contrast with the â¬train
    lossâ­ obtained on the dataset used to construct the predictor. â¦‰

  â€– A predictor whose average test loss is much larger than its
    average train lost is said to be â¬overfitâ­ to the train
    dataset.
    â€  â¦Š
      â€– Many authorities will refer to this as the â€œproblemâ€ or
        â€œdangerâ€ of overfitting. â¦‰
    â¦‰â¦‰
â¦‰

Â¶ â¦Š
  â€– Roughly speaking, we judge an inductor by some aggregation
    metric of the loss of predictors it produces on datasets. â¦‰

â¦‰
