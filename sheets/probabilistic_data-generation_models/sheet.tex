%!name:probabilistic_data-generation_models
%!need:inductors
%!need:independent_identically_distributed_random_variables
%!refs:shai_shalev-schwartz2014understanding

\ssection{Why}

We study inductors by defining a probabilistic model of the data generation.

\ssection{Definition}

Let $\CX$ be a set of inputs and $\CY$ be a set of outputs.
Let $f: \CX \to \CY$.
Let $x_i: \Omega \to \CX$ be independent and identically distributed random variables defined on some probability space $(\Omega, \CA, \PM)$ and define $y_i: \Omega \to \CY$ by $y_i \equiv f(x_i)$ for $i = 1, \dots, n$.

We call $((\Omega, \CA, \PM), \set{x_i: \Omega \to \CX}_{i=1}^{n}, f: \CX \to \CY)$ a \t{probabilistic data-generation model}
It is natural to call the law $\mu$ of $x_i$ (which is the same for each $i$) the \t{data-generating distribution} or \t{underlying distribution} and to call $f$ the \t{correct labeling function}.

Many authors refer to a probabilistic data-generation model as a \t{statistical learning (theory) framework}.
It is also common to define $S: \Omega \to (\CX \times \CY)^n$ by $S = ((x_1, y_1), \dots, (x_n, y_n))$ and refer to this as the \t{training dataset} of the probabilistic model.
Note well that it is a random variable.

\ssection{Juding predictors under the model}

The upshot of this framework is that we can theoretically analyze the performance of a candidate predictor, without recourse to some test dataset.

Let $h: \CX \to \CY$ be a hypothesis predictor.
The \t{error} of this predictor under the probabilistic data-generation model is
\[
  \mu(\Set*{x \in \CX}{h(x) \neq f(x)})
\]
In other words, the error is the probability (w.r.t. the underling distribution) that the classifier $h$ mislabels a point.

Many authors associate an event $A \in \CA$ with a function $\pi: \CX \to \set{0, 1}$ so that $A = \Set*{x \in \CX}{\pi(x) = 1}$ and it is common to write $\mu[\pi(x)]$ for $\mu(A)$.
In this case the error of $h$ is $\mu[h(x) \neq f(x)]$.

The error is measured with respect to the underling distribution $\mu$ and correct labeling function $f$.
Other names for the error of a classifier include the \t{generalization error}, the \t{risk} or the \t{true error} or \t{loss} of $h$.
