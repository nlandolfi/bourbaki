%!name:supervised_probabilistic_data_models
%!need:inductors
%!need:independent_identically_distributed_random_variables
%!need:approximators
%!refs:shai_shalev-schwartz2014understanding

\ssection{Why}

We want to discuss an inductor's performance on consistent (but possibly incomplete) datasets.

We take two steps. First, put a measure on the set of training sets and only consider high-measure subsets.
Second, consider predictors performing well in some tolerance.

\ssection{Definition}

Let $(X, \CX, \mu)$ be a probability space and $(Y, \CY)$ a measurable space.
Let $f: X \to Y$ measurable.
We call the pair $((X, \CX, \mu), f)$ a \t{(supervised) probabilistic data model}.

We interpret $\mu$ as the \t{data-generating distribution} or \t{underlying distribution} and $f$ as the \t{correct labeling function}.
Many authors refer to a supervised probabilistic data model as the \t{statistical learning (theory) framework}.

\ssection{Probable datasets}

We put a measure on the set of datasets by using the product measure $(X^n, \CX^n, \mu^n)$.
We interpret this as a model for a training set of independent and identically distributed inputs.


For $\delta \in (0, 1)$, $\CS \subset X^n$ is \t{$1-\delta$-representative} if $\mu^n(\CS) \geq 1 - \delta$.
If $\CS$ is $1-\delta$-representative for small $\delta$, we think of $\CS$ as a set of \say{probable} or \say{reasonable} datasets.
We call $\delta$ the \t{confidence parameter}.

\ssection{Predictor error}

The \t{error} of (measurable) $h: X \to Y$ (under $\mu$ and $f$) is
\[
  \underset{\mu, f}{\mathword{error}}(h) = \mu(\Set*{x \in \CX}{h(x) \neq f(x)}).
\]
We interpret this as the probability that the predictor mislabels a point.
The \t{accuracy} of $h$ is $1 - \mathword{error}_{\mu, f}(h)$.


Since $(f, g) \mapsto \mu[f(x) \neq g(x)]$ is a metric on $L^2(X, \CX, \mu, Y)$ we can talk about the error as the \say{distance} from the correct label classifier.
Thus we will say that $\epsilon \in (0, 1)$, (measurable) $h: \CX \to \CY$ \t{$\epsilon$-approximates} the correct labeling function $f$ if $\mathword{error}(h) \leq \epsilon$.
Roughly speaking, if $\epsilon \ll 1$, the the error of the hypothesis is \say{fairly small.}
We call $\epsilon$ the \t{accuracy parameter}, since the accuracy of such a predictor is $1 - \epsilon$.

\ssection{Other terminology}

A \t{hypothesis class} is a subset of the measurable functions from $X \to Y$.
Other names for the error of a classifier include the \t{generalization error}, the \t{risk} or the \t{true error} or \t{loss}.
