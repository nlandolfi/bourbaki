%!name:autoencoders
%!need:neural_networks
%!need:similarity_functions

\ssection{Why}\footnote{Future editions will include. Future editions may also change the name of this sheet.}

\ssection{Definition}

A neural network $\nu$ commutes with a neural network $\mu$ if their associated predictors commute as functions.

An \t{autoencoder} (or \t{feedforward autoencoder}) is a pair of neural networks $((\phi_1, \dots, \phi_k), (\psi_1, \dots, \psi_\ell))$.
If the networks commute and $\dom \phi_1 = \dom \psi_\ell$, we call the autoencoder \t{regular}.
We call the predictor of the first network the \t{encoder} and the predictor the second network the \t{decoder}.
We call the image of an input to the encoder an \t{embedding} (or \t{feature vector}, \t{representation}, \t{code}).

\ssection{Compressive autoencoders}

Let $(\phi, \psi)$ be regular and let $f: \R^d \to \R^k$ be the encoder and $g: \R^k \to \R^d$ be the decoder.
If $k < d$, we call the autoencoder \t{compressive}.
Otherwise, we call the autoencoder \t{noncompressive}.
An autoencoder is \t{perfect} if $g \composed f$ is the identity function.
Clearly, a compressive autoencoder can not be perfect.

Let us relax our notion of perfect by introducing a similarity function $\ell: \R^d \times \R^d \to \R$ (see \sheetref{similarity_functions}{Similarity Functions}).
An autoencoder is optimal with respect to $\ell$ if it minimizes $\int_{\R^d} \ell(g(f(z)), z) dz$.
This integral may diverge.
Even if it converges for some autoencoders, there may not be an optimal autoencoder, or a unique one.

If we parameterize a family of autoencoders $\set{x_{\theta}}_{\theta \in \Theta}$ by a compact set $\Theta$, ... \footnote{Future editions will continue.}

It is natural to be interested in compressive autoencoders.


\blankpage
