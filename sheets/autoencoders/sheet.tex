
\section*{Why}
\footnote{Future editions will include. Future editions may also change the name of this sheet.}
\section*{Definition}

A neural network $\nu $ commutes with a neural network $\mu $ if their associated predictors commute as functions.

An \t{autoencoder} (or \t{feedforward autoencoder}) is a pair of neural networks $((\phi _1, \dots , \phi _k), (\psi _1, \dots , \psi _\ell ))$.
If the networks commute and $\dom \phi _1 = \dom \psi _\ell $, we call the autoencoder \t{regular}.
We call the predictor of the first network the \t{encoder} and the predictor the second network the \t{decoder}.
We call the image of an input to the encoder an \t{embedding} (or \t{feature vector}, \t{representation}, \t{code}).

\section*{Compressive autoencoders}

Let $(\phi , \psi )$ be regular and let $f: \R ^d \to \R ^k$ be the encoder and $g: \R ^k \to \R ^d$ be the decoder.
If $k < d$, we call the autoencoder \t{compressive}.
Otherwise, we call the autoencoder \t{noncompressive}.
An autoencoder is \t{perfect} if $g \circ f$ is the identity function.
Clearly, a compressive autoencoder can not be perfect.

Let us relax our notion of perfect by introducing a similarity function $\ell : \R ^d \times \R ^d \to \R $ (see \sheetref{similarity_functions}{Similarity Functions}).
An autoencoder is optimal with respect to $\ell $ if it minimizes $\int_{\R ^d} \ell (g(f(z)), z) dz$.
This integral may diverge.
Even if it converges for some autoencoders, there may not be an optimal autoencoder, or a unique one.

If we parameterize a family of autoencoders $\set{x_{\theta }}_{\theta  \in \Theta }$ by a compact set $\Theta $, ...\footnote{Future editions will continue.}

It is natural to be interested in compressive autoencoders.

\blankpage