<!--
!name:autoencoders
!need:neural_networks
!need:similarity_functions
-->

§ Why ⦉
† ⦊
  ‖ Future editions will include. Future editions may also change
    the name of this sheet. ⦉
⦉

§ Definition ⦉
¶ ⦊
  ‖ A neural network $ν$ commutes with a neural network $μ$ if
    their associated predictors commute as functions. ⦉
⦉

¶ ⦊
  ‖ An ❬autoencoder❭ (or ❬feedforward autoencoder❭) is a pair of
    neural networks $((φ_1, …, φ_k), (ψ_1, …, ψ_ℓ))$. ⦉

  ‖ If the networks commute and $\dom φ_1 = \dom ψ_ℓ$, we call
    the autoencoder ❬regular❭. ⦉

  ‖ We call the predictor of the first network the ❬encoder❭
    and the predictor the second network the ❬decoder❭. ⦉

  ‖ We call the image of an input to the encoder an
    ❬embedding❭ (or ❬feature vector❭, ❬representation❭, ❬code❭). ⦉
⦉

§ Compressive autoencoders ⦉
¶ ⦊
  ‖ Let $(φ, ψ)$ be regular and let $f: 𝗥^d → 𝗥^k$ be the
    encoder and $g: 𝗥^k → 𝗥^d$ be the decoder. ⦉

  ‖ If $k < d$, we call the autoencoder ❬compressive❭. ⦉

  ‖ Otherwise, we call the autoencoder ❬noncompressive❭. ⦉

  ‖ An autoencoder is ❬perfect❭ if $g ∘ f$ is the identity
    function. ⦉

  ‖ Clearly, a compressive autoencoder can not be perfect. ⦉
⦉

¶ ⦊
  ‖ Let us relax our notion of perfect by introducing a
    similarity function $ℓ: 𝗥^d ×𝗥^d → 𝗥$ (see
    \sheetref{similarity_functions}{Similarity Functions}). ⦉

  ‖ An autoencoder is optimal with respect to $ℓ$ if it
    minimizes $∈t_{𝗥^d} ℓ(g(f(z)), z) dz$. ⦉

  ‖ This integral may diverge. ⦉

  ‖ Even if it converges for some autoencoders, there may not
    be an optimal autoencoder, or a unique one. ⦉
⦉

¶ ⦊
  ‖ If we parameterize a family of autoencoders $\set{x_{θ}}_{θ ∈
    Θ}$ by a compact set $Θ$, ...
    † ⦊
      ‖ Future editions will continue. ⦉
    ⦉⦉
⦉

¶ ⦊
  ‖ It is natural to be interested in compressive autoencoders. ⦉
⦉

<tex>
  ‖ \blankpage ⦉
</tex>
