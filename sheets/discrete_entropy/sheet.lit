<!--yaml
name: discrete_entropy
needs:
    - probability_distributions
    - logarithm
-->

§ Definition ⦉
¶ ⦊
  ‖ The ❬entropy❭ of a distribution is the expectation of the
    negative logarithm of the distribution under the distribution. ⦉

  ‖ It is sometimes called the ❬discrete entropy❭ to distinguish
    it with another related topic.
    † ⦊
      ‖ Future editions may not forward reference differential
        entropy. ⦉
    ⦉⦉
⦉

§§ Notation ⦉
¶ ⦊
  ‖ Let $A$ be a finite set. ⦉

  ‖ Let $p:A → 𝗥$ be a distribution. ⦉

  ‖ The entropy of $p$ is
    ◇ ⦊
      ‖ -∑_{a ∈ A} p(a) \log(p(a)). ⦉
    ⦉⦉

  ‖ We denote the entropy of $p$ by ⦉

  ‖ $H(p)$. ⦉
⦉

§§ Properties ⦉
¶ ⦊
  ‖ Let $x: Ω → V$ be a discrete random variable.
    𝍫 ⦊
      ‣ $H(x) ≥ 0$ ⦉

      ‣ $H(f(x)) ≤ H(x)$ ⦉

      ‣ For invertible $g$, $H(g(x)) ≤ H(x)$ ⦉

      ‖ ⦉
    ⦉⦉
⦉

<tex>
  ‖ \blankpage ⦉
</tex>