<!--yaml
name: discrete_entropy
needs:
    - probability_distributions
    - logarithm
-->

Â§ Definition â¦‰
Â¶ â¦Š
  â€– The â¬entropyâ­ of a distribution is the expectation of the
    negative logarithm of the distribution under the distribution. â¦‰

  â€– It is sometimes called the â¬discrete entropyâ­ to distinguish
    it with another related topic.
    â€  â¦Š
      â€– Future editions may not forward reference differential
        entropy. â¦‰
    â¦‰â¦‰
â¦‰

Â§Â§ Notation â¦‰
Â¶ â¦Š
  â€– Let $A$ be a finite set. â¦‰

  â€– Let $p:A â†’ ğ—¥$ be a distribution. â¦‰

  â€– The entropy of $p$ is
    â—‡ â¦Š
      â€– -âˆ‘_{a âˆˆ A} p(a) \log(p(a)). â¦‰
    â¦‰â¦‰

  â€– We denote the entropy of $p$ by â¦‰

  â€– $H(p)$. â¦‰
â¦‰

Â§Â§ Properties â¦‰
Â¶ â¦Š
  â€– Let $x: Î© â†’ V$ be a discrete random variable.
    ğ« â¦Š
      â€£ $H(x) â‰¥ 0$ â¦‰

      â€£ $H(f(x)) â‰¤ H(x)$ â¦‰

      â€£ For invertible $g$, $H(g(x)) â‰¤ H(x)$ â¦‰

      â€– â¦‰
    â¦‰â¦‰
â¦‰

<tex>
  â€– \blankpage â¦‰
</tex>