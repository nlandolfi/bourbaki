
\section*{Definition}

The \t{entropy} of a distribution is the expectation of the negative logarithm of the distribution under the distribution.
It is sometimes called the \t{discrete entropy} to distinguish it with another related topic.\footnote{Future editions may not forward reference differential entropy.}

\subsection*{Notation}

Let $A$ be a finite set.
Let $p:A \to \R $ be a distribution.
The entropy of $p$ is
\[
-\sum_{a \in A} p(a) \log(p(a)).
\]
We denote the entropy of $p$ by
$H(p)$.

\subsection*{Properties}

Let $x: \Omega  \to V$ be a discrete random variable.
    \begin{enumerate}
      \item $H(x) \geq 0$
      \item $H(f(x)) \leq H(x)$
      \item For invertible $g$, $H(g(x)) \leq H(x)$

    \end{enumerate}

\blankpage