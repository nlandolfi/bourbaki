%!name:distribution_expectation
%!need:induced_probability_distributions

\section*{Why}

If we model some measured value as a random variable with induced distribution $p: V \to \R $, then one interpretation of $p(v)$ for $v \in V$ is the \textit{proportion} of times in a large number of trials that we \textit{expect} to measure the value $v$.

\section*{Definition}

Given a distribution $p: \Omega  \to \R $ and a \textit{real-valued} outcome variable $x: \Omega  \to \R $, the \t{expectation} of $x$ under $p$ is $\sum_{\omega  \in \Omega } p(\omega )x(\omega )$.

\subsection*{Notation}

We denote the expectation of $x$ under $p$ by $\E (x)$.
When there is no chance of ambiguity, we write $\E (x)$.

\section*{Properties}

Let $x, y : \Omega  \to \R $ be two outcome variables and $p: \Omega  \to \R $ a distribution.
Let $\alpha , \beta  \in \R $.
Define $z = \alpha x + \beta y$ by $z(\omega ) = \alpha x(\omega ) + \beta y(\omega )$.
Then $\E (z) = \alpha \E (x) + \beta \E (z)$.

\blankpage