%!name:independent_set_of_real_vectors
%!need:real_linear_combinations

\section*{Why}

We want to capture the useful properties of the standard basis vectors.

\section*{Definition}

A set of vectors $\set{v_1, \dots , v_k}$ is \t{independent} if
  \[
\alpha _1v_1 + \alpha _2v_2 + \cdots + \alpha _kv_k = 0 \Rightarrow \alpha _1 = \alpha _2 = \cdots = \alpha _k = 0.
  \]
Notice that independence is a property of a set of vectors, not of any vector in particular.
Another way of saying this is that no vector can be represented as a linear combination of another.

\subsection*{Unique representation}

Suppose $v_1, \dots , v_k$ are independent and we have
  \[
x = \sum_{i = 1}^{k}\alpha _i v_i \quad \text{ and } \quad x = \sum_{i = 1}^{k} \beta _i v_i.
  \]
Then
  \[
0 = x - x = \sum_{i = 1}^{n} (\alpha _i - \beta _i)v_k.
  \]
Using the definition of independence, we conclude $\alpha _i - \beta _i = 0$ for $i = 1, \dots  k$.
Consequently, $\alpha _i = \beta _i$.
In other words, if $x$ can be represented as a linear combination of the vectors $v_1, \dots , v_k$, that representation is \textit{unique}.
We have shown that independence implies uniqueness?
What of the converse?

We show that lack of independence gives a lack of uniqueness.
Suppose there exists $\alpha _1, \dots , \alpha _k$, not all zero, so that
  \[
\alpha _1v_1 + + \alpha _2 v_2 + \cdots + \alpha _kv_k = 0.
  \]
In particular, suppose $\alpha _i \neq 0$.
Then we have
  \[
v_i = (1/\alpha _i) \sum_{j \neq i}\alpha _j v_j.
  \]
Suppose $x$ can be written as a linear combination of $v_1, \dots , v_k$.
In other words, there are $\beta _1, \dots , \beta _k$ so that
  \[
x = \beta _1 v_1 + \beta _2 v_2 + \cdots + \beta _k v_k
  \]

\blankpage