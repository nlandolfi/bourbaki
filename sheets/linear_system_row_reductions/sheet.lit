<!--yaml
name: linear_system_row_reductions
needs:
    - linear_equation_solutions
-->

¬ß Why ‚¶â
¬∂ ‚¶ä
  ‚Äñ We want to solve linear equations. ‚¶â

  ‚Äñ Our approach is to ‚Äúeliminate‚Äù variables from equations in
    our system. ‚¶â

  ‚Äñ Once we reach an equation in one variable, we will
    back-substitute to solve. ‚¶â
‚¶â

¬ß Two-variable example ‚¶â
¬∂ ‚¶ä
  ‚Äñ Suppose we want to find $x_1, x_2 ‚àà ùó•$ to satisfy
    ‚óá ‚¶ä
      ‚Äñ \begin{aligned} ‚¶â

      ‚Äñ 3x_1 + 2x_2 &= 10, \text{ and} ·ú∂ ‚¶â

      ‚Äñ 6x_1 + 5x_2 &= 20. ·ú∂ ‚¶â

      ‚Äñ \end{aligned} ‚¶â
    ‚¶â‚¶â

  ‚Äñ We can list the coefficients in a two-dimensional array $A
    = (3, 2; 6, 5)$ and $b = (10,20)$. ‚¶â

  ‚Äñ We can eliminate $x_1$ from the second equation by
    subtracting twice the first equation from the second. ‚¶â

  ‚Äñ In doing so we obtain the system of equations
    ‚óá ‚¶ä
      ‚Äñ \begin{aligned} ‚¶â

      ‚Äñ 3x_1 + 2x_2 &= 10 \text{ and } ·ú∂ ‚¶â

      ‚Äñ x_2 &= 0. ‚¶â

      ‚Äñ \end{aligned} ‚¶â
    ‚¶â‚¶â

  ‚Äñ The key insight is that this system has the ‚Äπsame solution
    set‚Ä∫. ‚¶â

  ‚Äñ We call the process of moving between these two systems a
    ‚ù¨row reduction‚ù≠. ‚¶â
‚¶â

¬ß Four-variable example ‚¶â
¬∂ ‚¶ä
  ‚Äñ What if instead we have four unknowns? ‚¶â

  ‚Äñ Suppose
    ‚óá ‚¶ä
      ‚Äñ A = \barray{2 & 1 & 1 & 0 ·ú∂ 4 & 3 & 3 & 1 ·ú∂ 8
        & 7 & 9 & 5 ·ú∂ 6 & 7 & 9 & 8} \text{ and } b =
        \barray{ 1 ·ú∂ 2 ·ú∂ 3 ·ú∂ 4}. ‚¶â
    ‚¶â‚¶â

  ‚Äñ We might first eliminate $x_1$ (the variable associated with
    the first column of coefficients) from the remaining three
    equations to obtain the linear system $S_1 = (A^1, b^1)$ in
    which
    ‚óá ‚¶ä
      ‚Äñ A^1 = \barray{ ‚¶â

      ‚Äñ 2 & 1 & 1 & 0 ·ú∂ ‚¶â

      ‚Äñ 0 & 1 & 1 & 1 ·ú∂ ‚¶â

      ‚Äñ 0 & 3 & 5 & 5 ·ú∂ ‚¶â

      ‚Äñ 0 & 4 & 6 & 8 ·ú∂ ‚¶â

      ‚Äñ } \text{ and } b^1 = \barray{ ‚¶â

      ‚Äñ 1 ·ú∂ ‚¶â

      ‚Äñ 0 ·ú∂ ‚¶â

      ‚Äñ -1 ·ú∂ ‚¶â

      ‚Äñ 1 ·ú∂ ‚¶â

      ‚Äñ } ‚¶â
    ‚¶â‚¶â

  ‚Äñ The trick is that, since $A_{22}' ‚â† 0$, we can take the
    same route to eliminate $x_2$, to obtain the system $S_2 =
    (A^2, b^2)$ in which
    ‚óá ‚¶ä
      ‚Äñ A_2 = \barray{ ‚¶â

      ‚Äñ 2 & 1 & 1 & 0 ·ú∂ ‚¶â

      ‚Äñ 0 & 1 & 1 & 1 ·ú∂ ‚¶â

      ‚Äñ 0 & 0 & 2 & 2 ·ú∂ ‚¶â

      ‚Äñ 0 & 0 & 2 & 4 ·ú∂ ‚¶â

      ‚Äñ } \text{ and } b^2 = \barray{ ‚¶â

      ‚Äñ 1 ·ú∂ ‚¶â

      ‚Äñ 0 ·ú∂ ‚¶â

      ‚Äñ -1 ·ú∂ ‚¶â

      ‚Äñ 1 ·ú∂ ‚¶â

      ‚Äñ } ‚¶â
    ‚¶â

    ‚Äñ Likewise for $x_3$, we obtain $S_3 = (A^3, b^3)$ in which
      ‚óá ‚¶ä
        ‚Äñ A^3 = \barray{ ‚¶â

        ‚Äñ 2 & 1 & 1 & 0 ·ú∂ ‚¶â

        ‚Äñ 0 & 1 & 1 & 1 ·ú∂ ‚¶â

        ‚Äñ 0 & 0 & 2 & 2 ·ú∂ ‚¶â

        ‚Äñ 0 & 0 & 0 & 2 ·ú∂ ‚¶â

        ‚Äñ } \text{ and } b^3 = \barray{ ‚¶â

        ‚Äñ 1 ·ú∂ ‚¶â

        ‚Äñ 0 ·ú∂ ‚¶â

        ‚Äñ -1 ·ú∂ ‚¶â

        ‚Äñ 3 ·ú∂ ‚¶â

        ‚Äñ }. ‚¶â
      ‚¶â‚¶â

    ‚Äñ Here, as in the two-variable case, the key insight is
      that all these systems have the same solution set and the
      last one, $(A^3, b^3)$, is easy to solve. ‚¶â

    ‚Äñ We solve it by ‚ù¨back substitution‚ù≠. ‚¶â

    ‚Äñ First, since $2x_4 = 3$, we find $x_4 = 3/2$. ‚¶â

    ‚Äñ Second, since $2x_3 + 2x_4 = -1$, we find $x_3 = -2$. ‚¶â

    ‚Äñ Similarly we find $x_2 = 1/2$ and $x_3 = 5/4$. ‚¶â‚¶â
‚¶â

¬ß Definition ‚¶â
¬∂ ‚¶ä
  ‚Äñ Let $S = (A ‚àà ùó•^{m √ón}, b ‚àà ùó•^{n})$ be a linear system. ‚¶â

  ‚Äñ The lower ‚ù¨row reduction‚ù≠ of $S$ for index $i$ with $A_{ii}
    ‚â† 0$ (or the $i$-row reduction) is the linear system
    $\tilde{A}_{st} = A_{st} - (A_{sj}/A_{ij})A_{it}$ if $i < s ‚â§
    m$ and $A_{st}$ otherwise. ‚¶â

  ‚Äñ We say that the system $(A, b)$ is ‚ù¨ordinarily reducible‚ù≠. ‚¶â
‚¶â

¬∂ ‚¶ä
  ‚Äñ Let $a^k, \tilde{a}^k ‚àà ùó•^{n}$ denote the $k$th row of $A$
    and $\tilde{A}$, respectively. ‚¶â

  ‚Äñ Then if $k ‚â† i$, $\tilde{a}^k = a^k - Œ±_k a^i$ where $Œ±_k
    = A_{kj}/A_{ij}$. ‚¶â

  ‚Äñ In other words, a row $k$ of the matrix $\tilde{A}$ is
    obtained by subtracting a multiple of the $i$th row of
    matrix $A$ from row $k$ of matrix $A$. ‚¶â

  ‚Äñ We are ‚Äúreducing‚Äù the rows of $A$. ‚¶â
‚¶â

<statement id='propostion:linear_system_reductions:solution_equivalence' type='proposition'>
  ‚Äñ Let $(A ‚àà ùó•^{m √ón}, b ‚àà ùó•^{n})$ be a linear system which
    row reduces to $(C, d)$. ‚¶â

  ‚Äñ Then $x ‚àà ùó•^{n}$ is a solution of $(A, b)$ if and only
    if it is a solution of $(C, d)$.
    ‚Ä† ‚¶ä
      ‚Äñ Future editions will include an account. ‚¶â
    ‚¶â‚¶â
</statement>
¬∂ ‚¶ä
  ‚Äñ First we reduce by subtracting twice row 1 from row 2,
    four times row 1 from row 3, and three times row 1 from
    row 4.
    ‚óá ‚¶ä
      ‚Äñ S_1 = \parens{\barray{ ‚¶â

      ‚Äñ 2 & 1 & 1 & 0 ·ú∂ ‚¶â

      ‚Äñ 0 & 1 & 1 & 1 ·ú∂ ‚¶â

      ‚Äñ 0 & 3 & 5 & 5 ·ú∂ ‚¶â

      ‚Äñ 0 & 4 & 6 & 8 ·ú∂ ‚¶â

      ‚Äñ }, \barray{ ‚¶â

      ‚Äñ 1 ·ú∂ ‚¶â

      ‚Äñ 0 ·ú∂ ‚¶â

      ‚Äñ -1 ·ú∂ ‚¶â

      ‚Äñ 1 ·ú∂ ‚¶â

      ‚Äñ }}. ‚¶â
    ‚¶â‚¶â

  ‚Äñ We then subtract three times row 2 from row 3 and four
    times row 2 from row 4 to obtain
    ‚óá ‚¶ä
      ‚Äñ S_2 = \left(\barray{ ‚¶â

      ‚Äñ 2 & 1 & 1 & 0 ·ú∂ ‚¶â

      ‚Äñ 0 & 1 & 1 & 1 ·ú∂ ‚¶â

      ‚Äñ 0 & 0 & 2 & 2 ·ú∂ ‚¶â

      ‚Äñ 0 & 0 & 2 & 4 ·ú∂ ‚¶â

      ‚Äñ }, \barray{ ‚¶â

      ‚Äñ 1 ·ú∂ ‚¶â

      ‚Äñ 0 ·ú∂ ‚¶â

      ‚Äñ -1 ·ú∂ ‚¶â

      ‚Äñ 1 ·ú∂ ‚¶â

      ‚Äñ }\right). ‚¶â
    ‚¶â‚¶â

  ‚Äñ Finally, we subtract two times row 3 from row 4 to obtain
    $S_4$, which we can write as
    ‚óá ‚¶ä
      ‚Äñ \begin{aligned} ‚¶â

      ‚Äñ 2x_1 + x_2 + x_3 &= 1,& ·ú∂ ‚¶â

      ‚Äñ x_2 + x_3 + x_4 &= 0,& ·ú∂ ‚¶â

      ‚Äñ 2x_3 + 2x_4 &= -1,& \text{ and } ·ú∂ ‚¶â

      ‚Äñ 2x_4 &= 3.& ‚¶â

      ‚Äñ \end{aligned} ‚¶â
    ‚¶â‚¶â

  ‚Äñ We can now back-substitute to find $x_4 = 3/2$, $x_3 =
    -2$, $x_2 = 1/2$ and $x_1 = 5/4$. ‚¶â

  ‚Äñ The above proposition says that this is the only solution
    of $S$, as well. ‚¶â
‚¶â