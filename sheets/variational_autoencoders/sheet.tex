%!name:variational_autoencoders
%!need:autoencoders
% %!need:probabilistic_models
%!need:neural_distribution_families
% get this from the above, may change in future
% %!need:parameterized_distribution_graphs

\ssection{Why}\footnote{Future editions will include. Future editions may also change the name of this sheet. It is also likely that there will be added prerequisite sheets on variational inference.}

\ssection{Definition}

A \t{variational autoencoder} (\t{VAE}) from \t{latent set} $Z$ to \t{observation set} $X$ is an ordered pair
$((p_{z}^{(\theta)}, p^{(\theta)}_{x \mid z}),  q^{\phi}_{z \mid x})$
whose first coordinate is a deep latent generation pair from $Z$ to $X$ (with parameters $\theta$) and whose second coordinate is deep conditional distribution from $X$ to $Z$ (with parameters $\phi$).

A VAE inherits its \t{joint function} from its deep latent generation pair. 
$p_z^{(\theta)}$ is called the \t{latent distribution} (or \t{prior distribution}, \t{latent model}).
$p_{x \mid z}^{(\theta)}$ is called the \t{decoder distribution}. 
$q_{z \mid x}^{(\theta)}$ is called the \t{encoder distribution} (or \t{inference distribution}, \t{recognition distribution}).

A \t{variational autoencoder family}, from $Z$ to $X$, is a family of autoencoders  $\set{ ((p_{z}^{(\theta)}, p^{(\theta)}_{x \mid z}) ,  q_{z \mid x}^{(\phi)}}_{(\theta, \phi) \in \Theta \times \Phi}$.

\blankpage
