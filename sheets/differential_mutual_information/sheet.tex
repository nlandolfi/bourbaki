%!name:differential_mutual_information
%!need:mutual_information
%!need:differential_relative_entropy
%!need:marginal_densities

\ssection{Why}

\ssection{Definition}

\ssubsection{Notation}

Let $f: \R^d \to \R$
Let $d$ denote the differential relative
entropy.
The mutual information between
$i$ and $j$ for $i,j = 1, \dots, d$
and $i \neq j$ is
\[
  d(f_{ij}, f_{i}f_{j})
\]
