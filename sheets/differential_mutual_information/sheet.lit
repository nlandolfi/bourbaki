<!--yaml
name: differential_mutual_information
needs:
    - mutual_information
    - differential_relative_entropy
    - marginal_densities
-->

§ Definition ⦉
¶ ⦊
  ‖ The differential mutual information between ⦉

  ‖ $i$ and $j$th components of a multivariate ⦉

  ‖ density is the differential relative entropy ⦉

  ‖ of the $i,j$th marginal density with the ⦉

  ‖ product of the $i$th and $j$th marginal densities. ⦉
⦉

§§ Notation ⦉
¶ ⦊
  ‖ Let $f: 𝗥^d → 𝗥$. ⦉

  ‖ Let $d$ denote the differential relative ⦉

  ‖ entropy. ⦉

  ‖ The mutual information between ⦉

  ‖ $i$ and $j$ for $i,j = 1, …, d$ ⦉

  ‖ and $i ≠ j$ is
    ◇ ⦊
      ‖ d(f_{ij}, f_{i}f_{j}) ⦉
    ⦉⦉
⦉

<tex>
  ‖ \blankpage ⦉
</tex>