\input{../../sheet.tex}
\sbasic
\input{../set_equality/macros.tex}
\input{../sets/macros.tex}
\input{../objects/macros.tex}
\input{../sentences/macros.tex}
\input{../set_inclusion/macros.tex}
\input{../ordered_pairs/macros.tex}
\input{../identity/macros.tex}
\input{../set_specification/macros.tex}
\input{../relations/macros.tex}
\input{../equations/macros.tex}
\input{../families/macros.tex}
\input{../algebras/macros.tex}
\input{../operations/macros.tex}
\input{../functions/macros.tex}
\input{../solving_equations/macros.tex}
\input{../family_operations/macros.tex}
\input{../arithmetic/macros.tex}
\input{../common_sense/macros.tex}
\input{../natural_numbers/macros.tex}
\input{../injective_functions/macros.tex}
\input{../integer_numbers/macros.tex}
\input{../natural_summation/macros.tex}
\input{../probability/macros.tex}
\input{../cardinality/macros.tex}
\input{../rational_numbers/macros.tex}
\input{../real_summation/macros.tex}
\input{../probability_outcomes/macros.tex}
\input{../intervals/macros.tex}
\input{../space/macros.tex}
\input{../real_numbers/macros.tex}
\input{../probability_distributions/macros.tex}
\input{../logarithm/macros.tex}
\input{../probability_densities/macros.tex}
\input{../n-dimensional_space/macros.tex}
\input{../entropy/macros.tex}
\input{../cross_entropy/macros.tex}
\input{../differential_entropy/macros.tex}
\input{../multivariate_real_densities/macros.tex}
\input{../relative_entropy/macros.tex}
\input{../differential_cross_entropy/macros.tex}
\input{../mutual_information/macros.tex}
\input{../marginal_densities/macros.tex}
\input{../differential_relative_entropy/macros.tex}
\input{./macros.tex}
\sstart
\stitle{Differential Mutual Information}

\ssection{Why}

\ssection{Definition}

\ssubsection{Notation}

The differential mutual information between
$i$ and $j$th components of a multivariate
density is the differential relative entropy
of the $i,j$th marginal density with the
product of the $i$th and $j$th marginal densities.

\ssubsection{Notation}

Let $f: \R^d \to \R$.
Let $d$ denote the differential relative
entropy.
The mutual information between
$i$ and $j$ for $i,j = 1, \dots, d$
and $i \neq j$ is
\[
  d(f_{ij}, f_{i}f_{j})
\]
\strats
