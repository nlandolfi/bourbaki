<!--yaml
name: cross_entropy
needs:
    - logarithm
    - probability_distributions
-->

§ Definition ⦉
¶ ⦊
  ‖ Consider two distributions ⦉

  ‖ on the same finite set. ⦉

  ‖ The ❬cross entropy❭ of the first distribution ❬relative❭ to
    the second distribution is the expectation of the negative
    logarithm of the first distribution under the second
    distribution. ⦉
⦉

§§ Notation ⦉
¶ ⦊
  ‖ Let $A$ be a finite set. ⦉

  ‖ Let $p: A → 𝗥$ and $q: A → 𝗥$ be distributions. ⦉

  ‖ The cross entropy of $p$ relative to $q$ is
    ◇ ⦊
      ‖ -∑_{a ∈ A} q(a) \log(p(a)). ⦉
    ⦉⦉

  ‖ We denote the cross entropy of $p$ relative to $q$ by
    $H(q, p)$. ⦉
⦉

<tex>
  ‖ \blankpage ⦉
</tex>