<!--yaml
name: cross_entropy
needs:
    - logarithm
    - probability_distributions
-->

Â§ Definition â¦‰
Â¶ â¦Š
  â€– Consider two distributions â¦‰

  â€– on the same finite set. â¦‰

  â€– The â¬cross entropyâ­ of the first distribution â¬relativeâ­ to
    the second distribution is the expectation of the negative
    logarithm of the first distribution under the second
    distribution. â¦‰
â¦‰

Â§Â§ Notation â¦‰
Â¶ â¦Š
  â€– Let $A$ be a finite set. â¦‰

  â€– Let $p: A â†’ ğ—¥$ and $q: A â†’ ğ—¥$ be distributions. â¦‰

  â€– The cross entropy of $p$ relative to $q$ is
    â—‡ â¦Š
      â€– -âˆ‘_{a âˆˆ A} q(a) \log(p(a)). â¦‰
    â¦‰â¦‰

  â€– We denote the cross entropy of $p$ relative to $q$ by
    $H(q, p)$. â¦‰
â¦‰

<tex>
  â€– \blankpage â¦‰
</tex>