%!name:cross_entropy
%!need:logarithm
%!need:probability_distributions

\ssection{Why}

\ssection{Definition}

Consider two distributions
on the same finite set.
The \ct{cross entropy}{}
of the first distribution
\ct{relative}{} to the
second distribution
is the expectation of the
negative logarithm of the first distribution
under the second distribution.

\ssubsection{Notation}

Let $R$ denote the set of
real numbers. Let
$A$ be a finite set.
Let $p: A \to R$ and
$q: A \to R$ be distributions.
The cross entropy of $p$ relative to $q$
is
\[
  -\sum_{a \in A} q(a) \log(p(a)).
\]
We denote the cross entropy
of $p$ relative to $q$ by
$H(q, p)$.

\blankpage
