
\section*{Definition}

Consider two distributions
on the same finite set.
The \t{cross entropy} of the first distribution \t{relative} to the second distribution is the expectation of the negative logarithm of the first distribution under the second distribution.

\subsection*{Notation}

Let $A$ be a finite set.
Let $p: A \to \R $ and $q: A \to \R $ be distributions.
The cross entropy of $p$ relative to $q$ is
\[
-\sum_{a \in A} q(a) \log(p(a)).
\]
We denote the cross entropy of $p$ relative to $q$ by $H(q, p)$.

\blankpage