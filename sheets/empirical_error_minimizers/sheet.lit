<!--yaml
name: empirical_error_minimizers
needs:
    - supervised_probabilistic_data_models
refs:
    - shai_shalev-schwartz2014understanding
-->

Â§ Why â¦‰
Â¶ â¦Š
  â€– A natural and (conceptually) simple inductor is to select a
    predictor which achieves zero error on the training dataset. â¦‰
â¦‰

Â§ Definition â¦‰
Â¶ â¦Š
  â€– Let $(X, ğ’³, Î¼)$ and $f: X â†’ Y$ be a probabilistic
    supervised data model. â¦‰

  â€– For a dataset $(x_1, y_1), â€¦, (x_n, y_n)$ in $X Ã—Y$, the
    â¬empirical errorâ­ of a predictor $h: X â†’ Y$ is
    â—‡ â¦Š
      â€– (1/n)\num{\Set*{i âˆˆ \upto{n}}{h(x_i) â‰  y_i}}, â¦‰
    â¦‰â¦‰

  â€– An â¬empirical error minimizerâ­ is a predictor whose empirical
    error is minimal. â¦‰

  â€– Since we assume that the dataset is consistent, the correct
    labeling function is always an empirical error minimizer, and
    achieves zero. â¦‰

  â€– If the dataset is incomplete, however, there may be other
    empirical error minimizers. â¦‰
â¦‰

Â¶ â¦Š
  â€– A family of inductors $(A^n)_n$ is an â¬empirical error
    minimizerâ­ if, for each $n âˆˆ N$, and each dataset $D =
    ((x_i, y_i))_{i = 1}^{n}$, $A^n(D)$ is an empirical risk
    minimizer of $D$. â¦‰
â¦‰

Â§ Overfitting and incutive bias â¦‰
Â¶ â¦Š
  â€– Since a dataset may be incomplete, an empirical risk
    minimizer may be a â€œpoorâ€ predictor. â¦‰

  â€– For example, let $A âŠ‚ X âŠ‚ ğ—¥^2$ and $Y = \set{0, 1}$. â¦‰

  â€– Define $f: X â†’ Y$ by $f(x) = 1$ if $x âˆˆ A$ and $f(x) =
    0$ otherwise. â¦‰

  â€– Suppose $A âˆˆ ğ’³$ and $Î¼(A) = 1/2$. â¦‰
â¦‰

Â¶ â¦Š
  â€– For any training set $(x_1, y_1), â€¦, (x_n, y_n)$ in $X
    Ã—Y$, the hypothesis $h: X â†’ Y$ defined by $h(x) = y_i$ if
    $x = x_i$ and $h(x) = 0$ otherwise achieves zero empirical
    error but has â€œactualâ€ error $1/2$. â¦‰

  â€– The predictor is said to be â¬overfitâ­ or to exhibit
    â¬overfittingâ­. â¦‰

  â€– It is said to fit the training dataset â€œtoo well.â€ â¦‰
â¦‰

Â¶ â¦Š
  â€– By adjusting the hypothesis class one may â€œguard againstâ€
    overfitting. â¦‰

  â€– In this context, the constrained hypothesis class is called
    an â¬inductive biasâ­. â¦‰
â¦‰

Â§Â§ Other terminology â¦‰
Â¶ â¦Š
  â€– Other terminology for the empirical error includes â¬empirical
    riskâ­. â¦‰

  â€– For these reasons, the learning paradigm of selecting a
    predictor $h$ to minimizer the empirical risk is called
    â¬empirical risk minimizationâ­ or â¬ERMâ­. â¦‰
â¦‰